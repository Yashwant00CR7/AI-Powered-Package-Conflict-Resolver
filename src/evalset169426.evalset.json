{
  "eval_set_id": "evalset169426",
  "name": "evalset169426",
  "eval_cases": [
    {
      "eval_id": "case32f959",
      "conversation": [
        {
          "invocation_id": "e-c18c3a23-118f-41eb-b8b4-19c24a9c2e2b",
          "user_content": {
            "parts": [
              {
                "text": " File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\agents\\sequential_agent.py\", line 77, in _run_async_impl\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\agents\\base_agent.py\", line 291, in run_async\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\agents\\sequential_agent.py\", line 77, in _run_async_impl\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\agents\\base_agent.py\", line 291, in run_async\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\agents\\llm_agent.py\", line 460, in _run_async_impl\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py\", line 346, in run_async\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py\", line 423, in _run_one_step_async\n    async for llm_response in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py\", line 801, in _call_llm_async\n    async for event in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py\", line 785, in _call_llm_with_tracing\n    async for llm_response in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py\", line 1038, in _run_and_handle_error\n    raise model_error\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py\", line 1024, in _run_and_handle_error\n    async for response in agen:\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\models\\google_llm.py\", line 235, in generate_content_async\n    raise ce\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\adk\\models\\google_llm.py\", line 214, in generate_content_async\n    response = await self.api_client.aio.models.generate_content(\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\models.py\", line 7021, in generate_content\n    return await self._generate_content(\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\models.py\", line 5839, in _generate_content\n    response = await self._api_client.async_request(\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\_api_client.py\", line 1434, in async_request\n    result = await self._async_request(\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\_api_client.py\", line 1367, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\_api_client.py\", line 1312, in _async_request_once\n    await errors.APIError.raise_for_async_response(response)\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\errors.py\", line 188, in raise_for_async_response\n    await cls.raise_error_async(status_code, response_json, response)\n  File \"D:\\anaconda3\\envs\\legacy\\lib\\site-packages\\google\\genai\\errors.py\", line 210, in raise_error_async\n    raise ClientError(status_code, response_json, response)\ngoogle.genai.errors.ClientError: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}}"
              }
            ],
            "role": "user"
          },
          "final_response": {
            "parts": [
              {
                "text": "### Clear Explanation of the Issue\nThe error (`google.genai.errors.ClientError: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported'}}`) is **not a Python package dependency conflict**. It occurs in Google ADK (Agent Development Kit) when agents mix **function calling** (e.g., `transfer_to_agent` for sub-agents) with **built-in Gemini tools** (e.g., `google_search`, `vertex_ai_search`, `BuiltInCodeExecutor`). \n\n- **Root Cause**: Gemini API backend enforces strict separation: one request can use *either* custom function declarations *or* built-in tools, but not both. ADK auto-generates function calls for sub-agents/tools, conflicting with built-ins in child agents (see ADK issues [#969](https://github.com/google/adk-python/issues/969), [#134](https://github.com/google/adk-python/issues/134)).\n- **Affected Models**: Gemini 2.0 Flash-Lite (used here) has partial/no support. Works better on Gemini 2.5+ (Pro/Flash).\n- **Stack Trace Path**: Error propagates from `llm_agent.py` → `base_llm_flow.py` → `google_llm.py` → `genai.models.generate_content` during agent transfer (ResearchAssistant → WebSearch).\n- **Why Not Deps?**: No version mismatches (e.g., tenacity, concurrent.futures are fine). It's API misuse/architecture.\n\n**Fixes Applied**:\n- Pin stable SDK versions supporting Gemini 2.5+ (no breaking changes in recent releases).\n- No requirements.txt changes needed for deps; restructure code (see Migration Notes).\n\n### Updated requirements.txt Content\n```\n# Core Google ADK and GenAI SDKs (pinned to stable versions supporting Gemini 2.5+ tools/function calling)\ngoogle-adk==0.1.0  # Latest stable; supports agent restructuring workarounds\ngoogle-generativeai==0.8.3  # Supports Gemini 2.5 models, function calling separation\n\n# Retries and async (already in stack trace; pinned for stability)\ntenacity==9.0.0\n\n# Optional: For advanced agent flows (if using sub-agents)\npydantic==2.9.2  # For input/output schemas in LlmAgent\nopentelemetry-api==1.34.0  # Tracing (fixes detach errors in stack trace)\n```\n\nInstall with: `pip install -r requirements.txt`\n\n### Migration Notes\n**Breaking Changes**:\n- **Gemini 2.0 → 2.5**: 2.0 Flash-Lite lacks full function calling (use `gemini-2.5-flash` or `gemini-2.5-pro`). Deprecations: Some 2.0 Live models retire Dec 2025.\n- **ADK Agent Structure**: Cannot mix built-ins + sub-agents directly. **Workaround** (100% effective per issues):\n  ```python\n  # Wrap built-in tool agents as AgentTool (no direct built-ins in root/chain)\n  search_agent = LlmAgent(model=\"gemini-2.5-flash\", tools=[google_search], ...)  # Child: built-in only\n  search_tool = AgentTool(agent=search_agent)  # Wrapper: function calling OK\n\n  root_agent = LlmAgent(model=\"gemini-2.5-flash\", tools=[search_tool], ...)  # Root: wrappers only\n  ```\n  - Use `Runner` for sequential flows: Avoid deep sub-agent chains.\n  - Custom `AgentTool` override for grounding refs (e.g., URLs): See GitHub comment in #969.\n\n**Steps**:\n1. Update model: `model=\"gemini-2.5-flash\"` everywhere.\n2. Rerun: Restructure → Test in AI Studio first.\n3. Monitor: `finish_reason != \"RECITATION\"`; temp=0.0-0.8; context <1M tokens.\n4. Track: ADK #969 for native fix (open, Feature Roadmap).\n\nThis resolves the error without deps changes. Test & report if persists!"
              }
            ],
            "role": "model"
          },
          "intermediate_data": {
            "invocation_events": [
              {
                "author": "Web_Crawl_Agent",
                "content": {
                  "parts": [
                    {
                      "function_call": {
                        "id": "call_99590692",
                        "args": {
                          "urls": [
                            "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
                            "https://ai.google.dev/gemini-api/docs/function-calling",
                            "https://github.com/google/adk-python/issues/969",
                            "https://github.com/google/adk-python/issues/134",
                            "https://community.home-assistant.io/t/2024-4-0-google-generative-ai-with-assist-error/713606",
                            "https://ai.google.dev/gemini-api/docs/troubleshooting",
                            "https://ai.google.dev/gemini-api/docs/models/gemini"
                          ]
                        },
                        "name": "batch_crawl_tool"
                      }
                    }
                  ],
                  "role": "model"
                }
              },
              {
                "author": "Web_Crawl_Agent",
                "content": {
                  "parts": [
                    {
                      "function_response": {
                        "id": "call_99590692",
                        "name": "batch_crawl_tool",
                        "response": {
                          "result": "# Content from https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling\n\n[ Skip to main content ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#main-content)\n[ ![Google Cloud Documentation](https://www.gstatic.com/devrel-devsite/prod/v210625d4186b230b6e4f2892d2ebde056c890c9488f9b443a741ca79ae70171d/clouddocs/images/lockup.svg) ](https://docs.cloud.google.com/)\n[ Technology areas ](https://docs.cloud.google.com/docs)\n  * [ AI and ML  ](https://docs.cloud.google.com/docs/ai-ml)\n  * [ Application development  ](https://docs.cloud.google.com/docs/application-development)\n  * [ Application hosting  ](https://docs.cloud.google.com/docs/application-hosting)\n  * [ Compute  ](https://docs.cloud.google.com/docs/compute-area)\n  * [ Data analytics and pipelines  ](https://docs.cloud.google.com/docs/data)\n  * [ Databases  ](https://docs.cloud.google.com/docs/databases)\n  * [ Distributed, hybrid, and multicloud  ](https://docs.cloud.google.com/docs/dhm-cloud)\n  * [ Generative AI  ](https://docs.cloud.google.com/docs/generative-ai)\n  * [ Industry solutions  ](https://docs.cloud.google.com/docs/industry)\n  * [ Networking  ](https://docs.cloud.google.com/docs/networking)\n  * [ Observability and monitoring  ](https://docs.cloud.google.com/docs/observability)\n  * [ Security  ](https://docs.cloud.google.com/docs/security)\n  * [ Storage  ](https://docs.cloud.google.com/docs/storage)\n\n\n[ Cross-product tools ](https://docs.cloud.google.com/docs/cross-product-overviews)\n  * [ Access and resources management  ](https://docs.cloud.google.com/docs/access-resources)\n  * [ Costs and usage management  ](https://docs.cloud.google.com/docs/costs-usage)\n  * [ Infrastructure as code  ](https://docs.cloud.google.com/docs/iac)\n  * [ Migration  ](https://docs.cloud.google.com/docs/migration)\n  * [ SDK, languages, frameworks, and tools  ](https://docs.cloud.google.com/docs/devtools)\n\n\n`/`\n[ Console ](https://console.cloud.google.com/)\n  * English\n  * Deutsch\n  * Español\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Português\n  * Português – Brasil\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\nSign in\n[ ![](https://docs.cloud.google.com/_static/clouddocs/images/icons/products/vertex-ai-color.svg) ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n  * [ Vertex AI ](https://docs.cloud.google.com/vertex-ai/docs)\n  * [ Generative AI on Vertex AI ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n\n\n[Start free](https://console.cloud.google.com/freetrial)\n[ Guides ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs) [ API reference ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest) [ Vertex AI Cookbook ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/cookbook) [ Prompt gallery ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery) [ Resources ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/getting-help) [ FAQ ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/faq) [ Pricing ](https://cloud.google.com/vertex-ai/generative-ai/pricing)\n[ ![Google Cloud Documentation](https://www.gstatic.com/devrel-devsite/prod/v210625d4186b230b6e4f2892d2ebde056c890c9488f9b443a741ca79ae70171d/clouddocs/images/lockup.svg) ](https://docs.cloud.google.com/)\n  * [ Technology areas  ](https://docs.cloud.google.com/docs)\n    * More \n    * [ Guides  ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)\n    * [ API reference  ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest)\n    * [ Vertex AI Cookbook  ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/cookbook)\n    * [ Prompt gallery  ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery)\n    * [ Resources  ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/getting-help)\n    * [ FAQ  ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/faq)\n    * [ Pricing  ](https://cloud.google.com/vertex-ai/generative-ai/pricing)\n  * [ Cross-product tools  ](https://docs.cloud.google.com/docs/cross-product-overviews)\n    * More \n  * [ Console  ](https://console.cloud.google.com/)\n\n\n  * Discover\n    * [Overview of Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)\n    * [Generative AI beginner's guide](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n    * [Glossary](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/glossary-genai)\n  * Get started\n    * [Get an API key](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys)\n    * [Configure application default credentials](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/gcp-auth)\n    * [API quickstart](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstart)\n    * [Vertex AI Studio quickstart](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart)\n    * [Migrate from Google AI Studio to Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate/migrate-google-ai)\n    * [Deploy your Vertex AI Studio prompt as a web application](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/deploy-vais-prompt)\n    * [Vertex AI Studio capabilities](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/vertex-ai-studio-capabilities)\n    * [Get started with Gemini 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/get-started-with-gemini-3)\n    * [Generate an image and verify its watermark using Imagen](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/quickstart-image-generate-console)\n    * [Google GenAI libraries](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/libraries)\n    * [Compatibility with OpenAI library](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/openai)\n    * Vertex AI in express mode\n    * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n    * [Console tutorial](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/vertex-ai-studio-express-mode-quickstart)\n    * [API tutorial](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/vertex-ai-express-mode-api-quickstart)\n  * Select models\n    * Model Garden\n    * [Overview of Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)\n    * [Use models in Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/use-models)\n    * [Test model capabilities](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/quickstart)\n    * [Supported models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/available-models)\n    * Google Models\n    * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models)\n    * Gemini\n      * [Migrate to the latest Gemini models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate)\n      * Pro\n      * [Gemini 3 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro)\n      * [Gemini 3 Pro Image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro-image)\n      * [Gemini 2.5 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro)\n      * Flash\n      * [Gemini 2.5 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash)\n      * [Gemini 2.5 Flash Image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-image)\n      * [Gemini 2.5 Flash Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-live-api)\n      * [Gemini 2.0 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)\n      * Flash-Lite\n      * [Gemini 2.5 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite)\n      * [Gemini 2.0 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash-lite)\n      * Other Gemini models\n      * [Vertex AI Model Optimizer](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/vertex-ai-model-optimizer)\n    * Imagen\n      * [Imagen 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate)\n      * [Imagen 4](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/4-0-generate)\n      * [Imagen 4.0 upscale Preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/4-0-upscale)\n      * [Virtual Try-On Preview 08-04](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/virtual-try-on-preview-08-04)\n      * [Imagen product recontext preview 06-30](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/product-recontext-preview-06-30)\n    * Veo\n      * [Veo 2](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate)\n      * [Veo 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate)\n      * [Veo 3.1](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-1-generate)\n    * Lyria\n      * [Lyria 2](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/lyria/lyria-002)\n    * [Model versions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions)\n    * Managed models\n    * [Model as a Service (MaaS) overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/overview)\n    * Partner models\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models)\n      * Claude\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude)\n        * [Request predictions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/use-claude)\n        * [Batch predictions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/batch)\n        * [Prompt caching](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/prompt-caching)\n        * [Count tokens](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/count-tokens)\n        * [Web search](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/web-search)\n        * [Safety classifiers](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/safety)\n        * Model details\n        * [Claude Opus 4.5](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/opus-4-5)\n        * [Claude Sonnet 4.5](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/sonnet-4-5)\n        * [Claude Opus 4.1](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/opus-4-1)\n        * [Claude Haiku 4.5](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/haiku-4-5)\n        * [Claude Opus 4](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/opus-4)\n        * [Claude Sonnet 4](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/sonnet-4)\n        * [Claude 3.5 Haiku](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/haiku-3-5)\n        * [Claude 3 Haiku](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/haiku-3)\n      * Mistral AI\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral)\n        * Model details\n        * [Mistral Medium 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral/mistral-medium-3)\n        * [Mistral OCR (25.05)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral/mistral-ocr)\n        * [Mistral Small 3.1 (25.03)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral/mistral-small-3-1)\n        * [Codestral 2](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral/codestral-2)\n    * Open models\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/use-open-models)\n      * [Use open models via Model as a Service (MaaS)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-maas)\n      * [Grant access to open models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/grant-access-open-models)\n      * Models\n      * DeepSeek\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/deepseek)\n        * [DeepSeek-R1-0528](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/deepseek/r1-0528)\n        * [DeepSeek-V3.1](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/deepseek/deepseek-v31)\n        * [DeepSeek-OCR](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/deepseek/deepseek-ocr)\n      * OpenAI\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/openai)\n        * [OpenAI gpt-oss-120b](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/openai/gpt-oss-120b)\n        * [OpenAI gpt-oss-20b](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/openai/gpt-oss-20b)\n      * Qwen\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/qwen)\n        * [Qwen 3 Next Instruct 80B](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/qwen/qwen3-next-instruct)\n        * [Qwen 3 Next Thinking 80B](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/qwen/qwen3-next-thinking)\n        * [Qwen 3 Coder](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/qwen/qwen3-coder)\n        * [Qwen 3 235B](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/qwen/qwen3-235b)\n      * MiniMax\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/minimax)\n        * [MiniMax M2](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/minimax/minimax-m2)\n      * Kimi\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/kimi)\n        * [Kimi K2 Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/kimi/kimi-k2-thinking)\n      * Embedding (e5)\n        * [Multilingual E5 Small](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/e5/multilingual-e5-small)\n        * [Multilingual E5 Large](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/e5/multilingual-e5-large)\n      * Llama\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama)\n        * [Request predictions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/use-llama)\n        * Model details\n        * [Llama 4 Maverick](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-maverick)\n        * [Llama 4 Scout](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout)\n        * [Llama 3.3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-3)\n        * [Llama 3.2](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-2)\n        * [Llama 3.1 405b](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-1-405)\n        * [Llama 3.1 70b](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-1-70)\n        * [Llama 3.1 8b](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-1-8)\n      * API\n      * [Call MaaS APIs for open models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/call-open-model-apis)\n      * [Function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/capabilities/function-calling)\n      * [Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/capabilities/thinking)\n      * [Structured output](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/capabilities/structured-output)\n      * [Batch prediction](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/capabilities/batch-prediction)\n    * [Model deprecations (MaaS)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/deprecations/partner-models)\n    * Self-deployed models\n    * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/self-deployed-models)\n    * [Choose an open model serving option](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/choose-serving-option)\n    * Deploy open models\n      * [Deploy open models from Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/deploy-model-garden)\n      * [Deploy open models with prebuilt containers](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-prebuilt-containers)\n      * [Deploy open models with a custom vLLM container](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/deploy-custom-vllm)\n      * [Deploy models with custom weights](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/deploy-models-with-custom-weights)\n    * [Deploy partner models from Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/deploy-partner-models)\n    * Google Gemma\n      * [Use Gemma](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)\n      * [Tutorial: Deploy and inference Gemma (GPU)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/deploy-and-inference-tutorial)\n      * [Tutorial: Deploy and inference Gemma (TPU)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/deploy-and-inference-tutorial-tpu)\n    * [Llama](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-llama)\n    * [Use Hugging Face Models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models)\n    * [Hex-LLM](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hex-llm)\n    * [Comprehensive guide to vLLM for Text and Multimodal LLM Serving (GPU)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm)\n    * [vLLM TPU](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm-tpu)\n    * [xDiT](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/xdit)\n    * [Tutorial: Deploy Llamma 3 models with SpotVM and Reservations](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/spotvm-reservations/use-spotvm-reservations)\n    * Model Garden notebooks\n      * [Tutorial: Optimize model performance with advanced features in Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/open-models/model-garden-published-notebooks/model_garden_advanced_features)\n  * Build\n    * Agents\n    * [Vertex AI Agent Builder documentation](https://docs.cloud.google.com/agent-builder/overview)\n    * Prompt design\n    * [Introduction to prompting](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design)\n    * Prompting strategies\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies)\n      * [Give clear and specific instructions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/clear-instructions)\n      * [Use system instructions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions)\n      * [Include few-shot examples](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/few-shot-examples)\n      * [Add contextual information](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/contextual-information)\n      * [Structure prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/structure-prompts)\n      * [Compare prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/compare-prompts)\n      * [Instruct the model to explain its reasoning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/explain-reasoning)\n      * [Break down complex tasks](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/break-down-prompts)\n      * [Experiment with parameter values](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)\n      * [Prompt iteration strategies](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-iteration)\n    * Task-specific prompt guidance\n      * [Design multimodal prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts)\n      * [Design chat prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/chat/chat-prompts)\n      * [Design medical text prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/medlm/medlm-prompts)\n    * Capabilities\n    * Safety\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/safety-overview)\n      * [Responsible AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/responsible-ai)\n      * [System instructions for safety](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/safety-system-instructions)\n      * [Configure content filters](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters)\n      * [Gemini for safety filtering and content moderation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-for-filtering-and-moderation)\n      * [Abuse monitoring](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/abuse-monitoring)\n      * [Process blocked responses](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/process-blocked-responses)\n      * [Content Credentials](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/content-credentials)\n    * Text and code generation\n      * [Text generation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini)\n      * [System instructions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction)\n      * [Function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)\n      * [Structured output](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output)\n      * [Content generation parameters](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters)\n      * [Code execution](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution)\n      * [Medical text](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/medlm/overview)\n    * Image generation\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/overview)\n      * [Generate and edit images with Gemini](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-generation)\n      * [Generate images using text prompts with Imagen](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/generate-images)\n      * [Edit images with Imagen](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/edit-images-overview)\n      * [Verify an image watermark](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/verify-watermark)\n      * Configure Imagen parameters\n        * [Configure Responsible AI safety settings](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/configure-responsible-ai-safety-settings)\n        * [Use prompt rewriter](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/use-prompt-rewriter)\n        * [Set text prompt language](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/set-text-prompt-language)\n        * [Configure aspect ratio](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/configure-aspect-ratio)\n        * [Set output resolution](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/set-output-resolution)\n        * [Omit content using a negative prompt](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/omit-content-using-a-negative-prompt)\n        * [Generate deterministic images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/generate-deterministic-images)\n      * Generate images for retail and e-commerce\n        * [Generate Virtual Try-On images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/generate-virtual-try-on-images)\n        * [Recontextualize product images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/recontextualize-product-images)\n      * Edit images\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/edit-images-overview)\n        * [Insert objects into an image using inpaint](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/edit-insert-objects)\n        * [Remove objects from an image using inpaint](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/edit-remove-objects)\n        * [Expand the content of an image using outpaint](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/edit-outpainting)\n        * [Replace the background of an image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/replace-image-background)\n      * Customize images\n        * [Subject customization](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/subject-customization)\n        * [Style customization](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/style-customization)\n        * [Controlled Customization](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/edit-controlled)\n        * [Instruct Customization](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/instruct-customization)\n      * [Upscale images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/upscale-image)\n      * [Prompt and image attribute guide](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/img-gen-prompt-guide)\n      * [Base64 encode and decode files](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/base64-encode)\n      * [Responsible AI and usage guidelines for Imagen](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/responsible-ai-imagen)\n    * Video generation\n      * [Introduction to Veo](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/overview)\n      * [Generate Veo videos from text prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/generate-videos-from-text)\n      * [Generate Veo videos from an image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/generate-videos-from-an-image)\n      * [Generate Veo videos using first and last video frames](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/generate-videos-from-first-and-last-frames)\n      * [Extend Veo videos](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/extend-a-veo-video)\n      * [Direct Veo video generation using a reference image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/use-reference-images-to-guide-video-generation)\n      * [Insert objects into Veo videos](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/insert-objects-into-videos)\n      * [Remove objects from Veo videos](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/remove-objects-from-videos)\n      * [Veo prompt guide](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/video-gen-prompt-guide)\n      * [Veo best practices](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/best-practice)\n      * [Turn off Veo's prompt rewriter](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/turn-the-prompt-rewriter-off)\n      * [Responsible AI for Veo](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/responsible-ai-and-usage-guidelines)\n    * Music generation\n      * [Generate music using Lyria](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/music/generate-music)\n      * [Lyria prompt guide](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/music/music-gen-prompt-guide)\n    * Media analysis\n      * [Image understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding)\n      * [Video understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding)\n      * [Audio understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/audio-understanding)\n      * [Document understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/document-understanding)\n      * [Bounding box detection](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/bounding-box-detection)\n    * Grounding\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview)\n      * [Grounding with Google Search](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search)\n      * [Grounding with Google Maps](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-maps)\n      * [Grounding with Vertex AI Search](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-vertex-ai-search)\n      * [Grounding with your search API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-your-search-api)\n      * [Grounding responses using RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/ground-responses-using-rag)\n      * [Grounding with Elasticsearch](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-elasticsearch)\n      * [Web Grounding for Enterprise](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/web-grounding-enterprise)\n    * [URL context](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/url-context)\n    * Thinking\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking)\n      * [Thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures)\n    * [Computer Use](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/computer-use)\n    * Live API\n      * [Live API overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api)\n      * [Best practices with Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/best-practices)\n      * [Interactive conversations](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/streamed-conversations)\n      * [Built-in tools](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/tools)\n      * [Proactive audio](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/proactive-audio)\n      * [Speech-to-speech translation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/speech-to-speech-translation)\n    * Embeddings\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings)\n      * Text embeddings\n        * [Get text embeddings](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings)\n        * [Choose an embeddings task type](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types)\n      * [Get multimodal embeddings](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)\n      * [Get batch embeddings inferences](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings)\n    * [Translation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/translate/translate-text)\n    * [Generate speech from text](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/speech/text-to-speech)\n    * [Transcribe speech](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/speech/speech-to-text)\n    * Development tools\n    * Use AI-powered prompt writing tools\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/ai-powered-prompt-writing)\n      * Optimize prompts\n        * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer)\n        * [Zero-shot optimizer](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/zero-shot-optimizer)\n        * [Data-driven optimizer](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/data-driven-optimizer)\n      * [Use prompt templates](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-templates)\n    * RAG Engine\n      * [RAG overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview)\n      * [RAG quickstart](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-quickstart)\n      * [RAG Engine billing](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-engine-billing)\n      * [Understanding RagManagedDb](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/understanding-ragmanageddb)\n      * [Data ingestion](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-data-ingestion)\n      * Supported models\n        * [Generative models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/supported-rag-models)\n        * [Embedding models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-embedding-models)\n      * Document parsing\n        * [Supported documents](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/supported-documents)\n        * [Fine-tune RAG transformations](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/fine-tune-rag-transformations)\n        * [Use Document AI layout parser](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/layout-parser-integration)\n        * [Use the LLM parser](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/llm-parser)\n      * Vector database choices in RAG\n        * [Overview of vector database choices](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/vector-db-choices)\n        * [Use RagManagedDb with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-ragmanageddb-with-rag)\n        * [Use Vertex AI Vector Search with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-vertexai-vector-search)\n        * [Use Feature Store with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-feature-store-with-rag)\n        * [Use Weaviate with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-weaviate-db)\n        * [Use Pinecone with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-pinecone)\n      * [Use Vertex AI Search with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-vertexai-search)\n      * [Reranking for RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/retrieval-and-ranking)\n      * [Manage your RAG corpus](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/manage-your-rag-corpus)\n      * [Use CMEK with RAG](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-supports-cmek)\n      * [RAG quotas](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-quotas)\n      * [Use RAG in Gemini Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-rag-in-multimodal-live)\n    * Tokenizer\n      * [List and count tokens](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n      * [Use the Count Tokens API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/get-token-count)\n    * [Multimodal datasets](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/datasets)\n    * [Use Vertex AI Search](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/vertex-ai-search)\n    * Model tuning\n    * [Introduction to tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models)\n    * Tuning Gemini models\n      * Supervised fine-tuning\n        * [About supervised fine-tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning)\n        * [Prepare your data](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare)\n        * [Use supervised fine-tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning)\n        * Supported modalities\n          * [Text tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune_gemini/text_tune)\n          * [Document tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune_gemini/doc_tune)\n          * [Image tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune_gemini/image_tune)\n          * [Audio tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune_gemini/audio_tune)\n          * [Video tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune_gemini/video_tune)\n          * [Tune function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune-function-calling)\n      * Preference tuning\n        * [About preference tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-preference-tuning)\n        * [Prepare your data](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-preference-tuning-prepare)\n        * [Use preference tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-preference-tuning)\n      * [Use tuning checkpoints](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tuning-checkpoints)\n      * [Use continuous tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-continuous-tuning)\n    * [Open models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/open-model-tuning)\n    * Embeddings models\n      * [Tune text embeddings models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings)\n    * Imagen models\n      * [Tune a subject model](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/fine-tune-model)\n      * [Create a custom style model](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/image/fine-tune-style)\n    * Translation models\n      * [About supervised fine-tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/translation-supervised-tuning)\n      * [Prepare your data](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/translation-supervised-tuning-prepare)\n      * [Use supervised fine-tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/translation-use-supervised-tuning)\n    * [Tuning recommendations with LoRA and QLoRA](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/lora-qlora)\n    * Migrate\n    * Call Vertex AI models using OpenAI libraries\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate/openai/overview)\n      * [Authenticate](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate/openai/auth-and-credentials)\n      * [Examples](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate/openai/examples)\n  * Evaluate\n    * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)\n    * [Tutorial: Perform evaluation using the console](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-genai-console)\n    * Perform evaluation using the GenAI Client in Vertex AI SDK\n      * [Tutorial: Evaluate models using the GenAI Client in Vertex AI SDK](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-genai-sdk)\n      * Define your evaluation metrics\n        * [Define your evaluation metrics](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval)\n        * [Details for managed rubric-based metrics](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/rubric-metric-details)\n      * [Prepare your evaluation dataset](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-dataset)\n      * [Run an evaluation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation)\n      * [View and interpret evaluation results](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation)\n      * [Evaluate agents](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-agents-client)\n    * Alternative evaluation methods\n    * Evaluate using the evaluation module in Vertex AI SDK\n      * [Tutorial: Perform evaluation using the evaluation module in Vertex AI SDK](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-quickstart)\n      * [Define your evaluation metrics](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/eval-python-sdk/determine-eval)\n      * [Prepare your evaluation dataset](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/eval-python-sdk/evaluation-dataset)\n      * [Run an evaluation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/eval-python-sdk/run-evaluation)\n      * [Interpret evaluation results](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/eval-python-sdk/view-evaluation)\n      * [Templates for model-based metrics](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates)\n      * [Evaluate agents](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-agents)\n      * [Evaluate a judge model](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluate-judge-model)\n      * [Configure a judge model](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/configure-judge-model)\n    * [Run AutoSxS pipeline](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval)\n    * [Run a computation-based evaluation pipeline](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/computation-based-eval-pipeline)\n  * Deploy\n    * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/deploy/overview)\n    * Optimize cost, latency, and performance\n    * [Deployment best practices](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompt-best-practices)\n    * Cache reused prompt context\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)\n      * [Create a context cache](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-create)\n      * [Use a context cache](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-use)\n      * [Get context cache information](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-getinfo)\n      * [Update a context cache](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-update)\n      * [Delete a context cache](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-delete)\n      * [Context cache for fine-tuned Gemini models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-for-tuned-gemini)\n    * Batch inference\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini)\n      * [Create batch job from Cloud Storage](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-from-cloud-storage)\n      * [Create batch job from BigQuery](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-from-bigquery)\n      * [Resume an incomplete batch job](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-resume)\n    * Quotas and system limits\n    * [All quotas and system limits](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/quotas)\n    * [Dynamic shared quota](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/dynamic-shared-quota)\n    * Provisioned Throughput\n      * [Provisioned Throughput overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/overview)\n      * [Supported models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/supported-models)\n      * [Calculate Provisioned Throughput requirements](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/measure-provisioned-throughput)\n      * [Provisioned Throughput for Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/live-api)\n      * [Single Zone Provisioned Throughput](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/szpt)\n      * [Purchase Provisioned Throughput](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/purchase-provisioned-throughput)\n      * [Use Provisioned Throughput](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/use-provisioned-throughput)\n      * [Troubleshooting error code 429](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/error-code-429)\n  * Administer\n    * [Access control](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/access-control)\n    * [Networking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/networking)\n    * [Security controls](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/security-controls)\n    * [Control access to Model Garden models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/control-model-access)\n    * [Enable Data Access audit logs](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/enable-audit-logs)\n    * [Save and share prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/prompt-sharing)\n    * [Monitor models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/model-observability)\n    * [Monitor cost using custom metadata labels](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls)\n    * [Request-response logging](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/request-response-logging)\n    * Secure a gen AI app by using IAP\n      * [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/streamlit-genai-iap)\n      * [Set up your project and source repository](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/setup-environment)\n      * [Create a Cloud Run service](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/create-cloudrun-service)\n      * [Create a load balancer](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/create-loadbalancer)\n      * [Configure IAP](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/configure-iap)\n      * [Test your IAP-secured app](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/view-app)\n      * [Clean up your project](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/streamlit/clean-up)\n  * Go to Vertex AI documentation\n  * [Vertex AI documentation](https://docs.cloud.google.com/vertex-ai/docs)\n\n\n  * [ AI and ML  ](https://docs.cloud.google.com/docs/ai-ml)\n  * [ Application development  ](https://docs.cloud.google.com/docs/application-development)\n  * [ Application hosting  ](https://docs.cloud.google.com/docs/application-hosting)\n  * [ Compute  ](https://docs.cloud.google.com/docs/compute-area)\n  * [ Data analytics and pipelines  ](https://docs.cloud.google.com/docs/data)\n  * [ Databases  ](https://docs.cloud.google.com/docs/databases)\n  * [ Distributed, hybrid, and multicloud  ](https://docs.cloud.google.com/docs/dhm-cloud)\n  * [ Generative AI  ](https://docs.cloud.google.com/docs/generative-ai)\n  * [ Industry solutions  ](https://docs.cloud.google.com/docs/industry)\n  * [ Networking  ](https://docs.cloud.google.com/docs/networking)\n  * [ Observability and monitoring  ](https://docs.cloud.google.com/docs/observability)\n  * [ Security  ](https://docs.cloud.google.com/docs/security)\n  * [ Storage  ](https://docs.cloud.google.com/docs/storage)\n\n\n  * [ Access and resources management  ](https://docs.cloud.google.com/docs/access-resources)\n  * [ Costs and usage management  ](https://docs.cloud.google.com/docs/costs-usage)\n  * [ Infrastructure as code  ](https://docs.cloud.google.com/docs/iac)\n  * [ Migration  ](https://docs.cloud.google.com/docs/migration)\n  * [ SDK, languages, frameworks, and tools  ](https://docs.cloud.google.com/docs/devtools)\n\n\n  * [ Home ](https://docs.cloud.google.com/)\n  * [ Documentation ](https://docs.cloud.google.com/docs)\n  * [ AI and ML ](https://docs.cloud.google.com/docs/ai-ml)\n  * [ Vertex AI ](https://docs.cloud.google.com/vertex-ai/docs)\n  * [ Generative AI on Vertex AI ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n  * [ Guides ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)\n\n\nSend feedback \n#  Introduction to function calling Stay organized with collections  Save and categorize content based on your preferences. \nTo see an example of function calling, run the \"Intro to Function Calling with the Gemini API\" notebook in one of the following environments: \n[![](https://docs.cloud.google.com/static/vertex-ai/images/colab-logo-32px.png)Open in Colab](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb) | [![](https://docs.cloud.google.com/static/vertex-ai/images/colab-enterprise-logo-32px.png)Open in Colab Enterprise](https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ffunction-calling%2Fintro_function_calling.ipynb) | [![](https://docs.cloud.google.com/static/vertex-ai/images/vertex-ai-workbench-logo-32px.png)Open in Vertex AI Workbench](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ffunction-calling%2Fintro_function_calling.ipynb) | [![](https://docs.cloud.google.com/static/vertex-ai/images/github-logo-32px.png)View on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)\n**Function calling** , also known as _tool use_ , provides the LLM with definitions of external tools (for example, a `get_current_weather` function). When processing a prompt, the model intelligently determines if a tool is needed and, if so, outputs structured data specifying the tool to call and its parameters (for example, `get_current_weather(location='Boston')`). Your application then executes this tool, feeds the result back to the model, allowing it to complete its response with dynamic, real-world information or the outcome of an action. This effectively bridges the LLM with your systems and extends its capabilities.\n![Diagram illustrating the Function Calling Interaction flow](https://docs.cloud.google.com/static/vertex-ai/generative-ai/docs/multimodal/images/function-calling.png)   \n\nFunction calling enables two primary use cases:\n  * **Fetching data** : Retrieve up-to-date information for model responses, such as current weather, currency conversion, or specific data from knowledge bases and APIs (RAG).\n  * **Taking action** : Perform external operations like submitting forms, updating application state, or orchestrating agentic workflows (e.g., conversation handoffs).\n\n\nFor more use cases and examples that are powered by function calling, see [Use cases](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#use-cases).\n## Features and limitations\n  * The following models support function calling:\n    * Gemini models: \n      * [Gemini 3 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro) preview Preview model\n      * [Gemini 2.5 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro)\n      * [Gemini 2.5 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash) preview Preview model\n      * [Gemini 2.5 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite) preview Preview model\n      * [Gemini 2.5 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash)\n      * [Gemini 2.5 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite)\n      * [Gemini 2.5 Flash with Live API native audio](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-live-api) preview Preview model\n      * [Gemini 2.0 Flash with Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash#live-api) preview Preview model\n      * [Gemini 2.0 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)\n      * [Gemini 2.0 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash-lite)\n    * Open models: \n      * [DeepSeek R1-0528](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/deepseek/r1-0528)\n      * [Llama 4 Maverick](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-maverick)\n      * [Llama 4 Scout](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout)\n      * [Llama 3.3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama3-3)\n  * You can specify up to 512 [`FunctionDeclarations`](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.FunctionDeclaration)\n  * Define your functions in the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema) format.\n  * For Gemini 3 Pro and later models, you can include multimodal content (images and PDFs) in `functionResponse` messages that you send back to the model. For more information, see [Multimodal function responses](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#mm-fr).\n  * For Gemini 3 Pro and later models, you can stream function call arguments as they are generated by setting `streamFunctionCallArguments` to `true` in the `functionCallingConfig`. For more information, see [Streaming function call arguments](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#streaming-fc).\n  * For best practices related to the function declarations, including tips for names and descriptions, see [Best practices](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#best-practices).\n  * For Open Models, follow this [user guide](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/maas/capabilities/function-calling).\n\n\n## How to create a function calling application\nTo use function calling, perform the following tasks:\n  1. [Submit function declarations and prompt to the model](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#submit-prompt).\n  2. [Provide the API output to the model](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#submit-api-output).\n\n\n### Step 1: Submit the prompt and function declarations to the model\nDeclare a [`Tool`](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.Tool) in a schema format that's compatible with the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema). For more information, see [Schema examples](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#schema-examples).\nThe following examples submit a prompt and function declaration to the Gemini models:\n###  REST \n```\nPROJECT_ID=myproject\nLOCATION=us-central1\nMODEL_ID=gemini-2.0-flash-001\n\ncurl\\\n\"Authorization: Bearer $(gcloud)\"\\\n\"Content-Type: application/json\"\\\n${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent\\\n'{\n    \"contents\": [{\n      \"role\": \"user\",\n      \"parts\": [{\n        \"text\": \"What is the weather in Boston?\"\n      }]\n    }],\n    \"tools\": [{\n      \"functionDeclarations\": [\n        {\n          \"name\": \"get_current_weather\",\n          \"description\": \"Get the current weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city name of the location for which to get the weather.\",\n                \"default\": {\n                  \"string_value\": \"Boston, MA\"\n                }\n              }\n            },\n            \"required\": [\n              \"location\"\n            ]\n          }\n        }\n      ]\n    }]\n  }'\n\n```\n\n###  Python \nYou can specify the schema either manually using a Python dictionary or automatically with the `from_func` helper function. The following example demonstrates how to declare a function manually:\n```\nimport[vertexai](https://docs.cloud.google.com/python/docs/reference/vertexai/latest)\nfromvertexai.generative_modelsimport (\n    [Content](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Content.html),\n    [FunctionDeclaration](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.FunctionDeclaration.html),\n    [GenerationConfig](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.GenerationConfig.html),\n    [GenerativeModel](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.GenerativeModel.html),\n    [Part](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Part.html),\n    [Tool](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Tool.html),\n    [ToolConfig](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.ToolConfig.html)\n)\n\n# Initialize Vertex AI\n# TODO(developer): Update the project\n[vertexai](https://docs.cloud.google.com/python/docs/reference/vertexai/latest).init(project=\"PROJECT_ID\", location=\"us-central1\")\n\n# Initialize Gemini model\nmodel = GenerativeModel(model_name=\"gemini-2.0-flash\")\n\n# Manual function declaration\nget_current_weather_func = FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    # Function parameters are specified in JSON schema format\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city name of the location for which to get the weather.\",\n              \"default\": {\n                \"string_value\": \"Boston, MA\"\n              }\n           }\n        },\n    },\n)\n\nresponse = model.[generate_content](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.GenerativeModel.html#vertexai_preview_generative_models_GenerativeModel_generate_content)(\n    contents = [\n      Content(\n        role=\"user\",\n          parts=[\n              [Part](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Part.html).from_text(\"What is the weather like in Boston?\"),\n          ],\n      )\n    ],\n    generation_config = GenerationConfig(temperature=0),\n    tools = [\n      Tool(\n        function_declarations=[get_current_weather_func],\n      )\n    ]\n)\n\n```\n\nAlternatively, you can declare the function automatically with the `from_func` helper function as shown in the following example:\n```\ndefget_current_weather(location: str = \"Boston, MA\"):\n\"\"\"\n  Get the current weather in a given location\n\n  Args:\n      location: The city name of the location for which to get the weather.\n\n  \"\"\"\n  # This example uses a mock implementation.\n  # You can define a local function or import the requests library to call an API\n  return {\n    \"location\": \"Boston, MA\",\n    \"temperature\": 38,\n    \"description\": \"Partly Cloudy\",\n    \"icon\": \"partly-cloudy\",\n    \"humidity\": 65,\n    \"wind\": {\n        \"speed\": 10,\n        \"direction\": \"NW\"\n    }\n  }\nget_current_weather_func = FunctionDeclaration.from_func(get_current_weather)\n\n```\n\n###  Node.js \nThis example demonstrates a text scenario with one function and one prompt.\n### Node.js\nBefore trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using client libraries](https://docs.cloud.google.com/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Node.js API reference documentation](https://docs.cloud.google.com/nodejs/docs/reference/aiplatform/latest). \nTo authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](https://docs.cloud.google.com/docs/authentication/set-up-adc-local-dev-environment). \n```\nconst{\nVertexAI,\nFunctionDeclarationSchemaType,\n}=require('@google-cloud/vertexai[](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/overview.html)');\n\nconstfunctionDeclarations=[\n{\nfunction_declarations:[\n{\nname:'get_current_weather',\ndescription:'get weather in a given location',\nparameters:{\ntype:[FunctionDeclarationSchemaType](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/overview.html).[OBJECT](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/schematype.html),\nproperties:{\nlocation:{type:[FunctionDeclarationSchemaType](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/overview.html).[STRING](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/schematype.html)},\nunit:{\ntype:[FunctionDeclarationSchemaType](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/overview.html).[STRING](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/schematype.html),\nenum:['celsius','fahrenheit'],\n},\n},\nrequired:['location'],\n},\n},\n],\n},\n];\n\nconstfunctionResponseParts=[\n{\nfunctionResponse:{\nname:'get_current_weather',\nresponse:{name:'get_current_weather',content:{weather:'super nice'}},\n},\n},\n];\n\n/**\n * TODO(developer): Update these variables before running the sample.\n */\nasyncfunctionfunctionCallingStreamContent(\nprojectId='PROJECT_ID',\nlocation='us-central1',\nmodel='gemini-2.0-flash-001'\n){\n// Initialize Vertex with your Cloud project and location\nconstvertexAI=new[VertexAI](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/vertexai.html)({project:projectId,location:location});\n\n// Instantiate the model\nconstgenerativeModel=vertexAI.[getGenerativeModel](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/vertexai.html)({\nmodel:model,\n});\n\nconstrequest={\ncontents:[\n{role:'user',parts:[{text:'What is the weather in Boston?'}]},\n{\nrole:'ASSISTANT',\nparts:[\n{\nfunctionCall:{\nname:'get_current_weather',\nargs:{location:'Boston'},\n},\n},\n],\n},\n{role:'USER',parts:functionResponseParts},\n],\ntools:functionDeclarations,\n};\nconststreamingResp=awaitgenerativeModel.generateContentStream(request);\nforawait(constitemof[stream](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/streamgeneratecontentresult.html)ingResp.[stream](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/streamgeneratecontentresult.html)){\nconsole.log(item.[candidates](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/generatecontentresponse.html)[0].[content](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/generatecontentcandidate.html).[parts](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest/vertexai/content.html)[0].text);\n}\n}\n```\n\n###  Go \nThis example demonstrates a text scenario with one function and one prompt.\nLearn how to install or update the [Go](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview). \nTo learn more, see the [ SDK reference documentation](https://pkg.go.dev/google.golang.org/genai). \nSet environment variables to use the Gen AI SDK with Vertex AI: \n```\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexportGOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT\nexportGOOGLE_CLOUD_LOCATION=global\nexportGOOGLE_GENAI_USE_VERTEXAI=True\n```\n\n```\nimport(\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\tgenai \"google.golang.org/genai\"\n)\n\n// generateWithFuncCall shows how to submit a prompt and a function declaration to the model,\n// allowing it to suggest a call to the function to fetch external data. Returning this data\n// enables the model to generate a text response that incorporates the data.\nfunc generateWithFuncCall(w io.Writer) error {\n\tctx := context.Background()\n\n\tclient, err := genai.NewClient(ctx, &genai.ClientConfig{\n\t\tHTTPOptions: genai.HTTPOptions{APIVersion: \"v1\"},\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create genai client: %w\", err)\n\t}\n\n\tweatherFunc := &genai.FunctionDeclaration{\n\t\tDescription: \"Returns the current weather in a location.\",\n\t\tName:        \"getCurrentWeather\",\n\t\tParameters: &genai.Schema{\n\t\t\tType: \"object\",\n\t\t\tProperties: map[string]*genai.Schema{\n\t\t\t\t\"location\": {Type: \"string\"},\n\t\t\t},\n\t\t\tRequired: []string{\"location\"},\n\t\t},\n\t}\n\tconfig := &genai.GenerateContentConfig{\n\t\tTools: []*genai.Tool{\n\t\t\t{FunctionDeclarations: []*genai.FunctionDeclaration{weatherFunc}},\n\t\t},\n\t\tTemperature: genai.Ptr(float32(0.0)),\n\t}\n\n\tmodelName := \"gemini-2.5-flash\"\n\tcontents := []*genai.Content{\n\t\t{Parts: []*genai.Part{\n\t\t\t{Text: \"What is the weather like in Boston?\"},\n\t\t},\n\t\t\tRole: \"user\"},\n\t}\n\n\tresp, err := client.Models.GenerateContent(ctx, modelName, contents, config)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to generate content: %w\", err)\n\t}\n\n\tvar funcCall *genai.FunctionCall\n\tfor _, p := range resp.Candidates[0].Content.Parts {\n\t\tif p.FunctionCall != nil {\n\t\t\tfuncCall = p.FunctionCall\n\t\t\tfmt.Fprint(w, \"The model suggests to call the function \")\n\t\t\tfmt.Fprintf(w, \"%q with args: %v\\n\", funcCall.Name, funcCall.Args)\n\t\t\t// Example response:\n\t\t\t// The model suggests to call the function \"getCurrentWeather\" with args: map[location:Boston]\n\t\t}\n\t}\n\tif funcCall == nil {\n\t\treturn fmt.Errorf(\"model did not suggest a function call\")\n\t}\n\n\t// Use synthetic data to simulate a response fromthe external API.\n\t// In a real application, this would come froman actual weather API.\n\tfuncResp := &genai.FunctionResponse{\n\t\tName: \"getCurrentWeather\",\n\t\tResponse: map[string]any{\n\t\t\t\"location\":         \"Boston\",\n\t\t\t\"temperature\":      \"38\",\n\t\t\t\"temperature_unit\": \"F\",\n\t\t\t\"description\":      \"Cold and cloudy\",\n\t\t\t\"humidity\":         \"65\",\n\t\t\t\"wind\":             `{\"speed\": \"10\", \"direction\": \"NW\"}`,\n\t\t},\n\t}\n\n\t// Return conversation turns and API response to complete the model's response.\n\tcontents = []*genai.Content{\n\t\t{Parts: []*genai.Part{\n\t\t\t{Text: \"What is the weather like in Boston?\"},\n\t\t},\n\t\t\tRole: \"user\"},\n\t\t{Parts: []*genai.Part{\n\t\t\t{FunctionCall: funcCall},\n\t\t}},\n\t\t{Parts: []*genai.Part{\n\t\t\t{FunctionResponse: funcResp},\n\t\t}},\n\t}\n\n\tresp, err = client.Models.GenerateContent(ctx, modelName, contents, config)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to generate content: %w\", err)\n\t}\n\n\trespText := resp.Text()\n\n\tfmt.Fprintln(w, respText)\n\n\t// Example response:\n\t// The weather in Boston is cold and cloudy with a temperature of 38 degrees Fahrenheit. The humidity is ...\n\n\treturn nil\n}\n\n```\n\n###  C# \nThis example demonstrates a text scenario with one function and one prompt.\n### C#\nBefore trying this sample, follow the C# setup instructions in the [Vertex AI quickstart using client libraries](https://docs.cloud.google.com/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI C# API reference documentation](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest). \nTo authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](https://docs.cloud.google.com/docs/authentication/set-up-adc-local-dev-environment). \n```\n\nusing[Google.Cloud.AIPlatform.V1](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.html);\nusingSystem;\nusingSystem.Threading.Tasks;\nusingType=Google.Cloud.AIPlatform.V1.Type;\nusingValue=Google.Protobuf.WellKnownTypes.Value;\n\npublicclassFunctionCalling\n{\npublicasyncTask<string>GenerateFunctionCall(\nstringprojectId=\"your-project-id\",\nstringlocation=\"us-central1\",\nstringpublisher=\"google\",\nstringmodel=\"gemini-2.0-flash-001\")\n{\nvarpredictionServiceClient=new[PredictionServiceClientBuilder](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.PredictionServiceClientBuilder.html)\n{\nEndpoint=$\"{location}-aiplatform.googleapis.com\"\n}.Build();\n\n// Define the user's prompt in a Content object that we can reuse in\n// model calls\nvaruserPromptContent=new[Content](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Content.html)\n{\nRole=\"USER\",\nParts=\n{\nnew[Part](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Part.html){Text=\"What is the weather like in Boston?\"}\n}\n};\n\n// Specify a function declaration and parameters for an API request\nvarfunctionName=\"get_current_weather\";\nvargetCurrentWeatherFunc=new[FunctionDeclaration](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.FunctionDeclaration.html)\n{\nName=functionName,\nDescription=\"Get the current weather in a given location\",\nParameters=new[OpenApiSchema](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.OpenApiSchema.html)\n{\nType=[Type](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Type.html).[Object](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Type.html#Google_Cloud_AIPlatform_V1_Type_Object),\nProperties=\n{\n[\"location\"]=new()\n{\nType=[Type](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Type.html).[String](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Type.html#Google_Cloud_AIPlatform_V1_Type_String),\nDescription=\"Get the current weather in a given location\"\n},\n[\"unit\"]=new()\n{\nType=[Type](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Type.html).[String](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Type.html#Google_Cloud_AIPlatform_V1_Type_String),\nDescription=\"The unit of measurement for the temperature\",\nEnum={\"celsius\",\"fahrenheit\"}\n}\n},\nRequired={\"location\"}\n}\n};\n\n// Send the prompt and instruct the model to generate content using the tool that you just created\nvargenerateContentRequest=new[GenerateContentRequest](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerateContentRequest.html)\n{\nModel=$\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\",\nGenerationConfig=new[GenerationConfig](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerationConfig.html)\n{\nTemperature=0f\n},\nContents=\n{\nuserPromptContent\n},\nTools=\n{\nnew[Tool](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Tool.html)\n{\nFunctionDeclarations={getCurrentWeatherFunc}\n}\n}\n};\n\n[GenerateContentResponse](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerateContentResponse.html)response=awaitpredictionServiceClient.GenerateContentAsync(generateContentRequest);\n\nvarfunctionCall=response.[Candidates](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerateContentResponse.html#Google_Cloud_AIPlatform_V1_GenerateContentResponse_Candidates)[0].[Content](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Content.html).Parts[0].[FunctionCall](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.FunctionCall.html);\nConsole.WriteLine(functionCall);\n\nstringapiResponse=\"\";\n\n// Check the function name that the model responded with, and make an API call to an external system\nif(functionCall.Name==functionName)\n{\n// Extract the arguments to use in your API call\nstringlocationCity=functionCall.Args.[Fields](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.SyntheticExample.html#Google_Cloud_AIPlatform_V1_SyntheticExample_Fields)[\"location\"].StringValue;\n\n// Here you can use your preferred method to make an API request to\n// fetch the current weather\n\n// In this example, we'll use synthetic data to simulate a response\n// payload from an external API\napiResponse=@\"{ \"\"location\"\": \"\"Boston, MA\"\",\n                    \"\"temperature\"\": 38, \"\"description\"\": \"\"Partly Cloudy\"\"}\";\n}\n\n// Return the API response to Gemini so it can generate a model response or request another function call\ngenerateContentRequest=new[GenerateContentRequest](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerateContentRequest.html)\n{\nModel=$\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\",\nContents=\n{\nuserPromptContent,// User prompt\nresponse.[Candidates](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerateContentResponse.html#Google_Cloud_AIPlatform_V1_GenerateContentResponse_Candidates)[0].[Content](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Content.html),// Function call response,\nnew[Content](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Content.html)\n{\nParts=\n{\nnew[Part](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Part.html)\n{\nFunctionResponse=new()\n{\nName=functionName,\nResponse=new()\n{\nFields=\n{\n{\"content\",new[Value](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Value.html){StringValue=apiResponse}}\n}\n}\n}\n}\n}\n}\n},\nTools=\n{\nnew[Tool](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Tool.html)\n{\nFunctionDeclarations={getCurrentWeatherFunc}\n}\n}\n};\n\nresponse=awaitpredictionServiceClient.GenerateContentAsync(generateContentRequest);\n\nstringresponseText=response.[Candidates](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.GenerateContentResponse.html#Google_Cloud_AIPlatform_V1_GenerateContentResponse_Candidates)[0].[Content](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.Content.html).Parts[0].Text;\nConsole.WriteLine(responseText);\n\nreturnresponseText;\n}\n}\n\n```\n\n### Java\n### Java\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using client libraries](https://docs.cloud.google.com/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Java API reference documentation](https://docs.cloud.google.com/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1). \nTo authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](https://docs.cloud.google.com/docs/authentication/set-up-adc-local-dev-environment). \n```\nimportcom.google.cloud.vertexai.VertexAI[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.VertexAI.html);\nimportcom.google.cloud.vertexai.api.Content[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Content.html);\nimportcom.google.cloud.vertexai.api.FunctionDeclaration[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.FunctionDeclaration.html);\nimportcom.google.cloud.vertexai.api.GenerateContentResponse[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.GenerateContentResponse.html);\nimportcom.google.cloud.vertexai.api.Schema[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Schema.html);\nimportcom.google.cloud.vertexai.api.Tool[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Tool.html);\nimportcom.google.cloud.vertexai.api.Type[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Type.html);\nimportcom.google.cloud.vertexai.generativeai.ChatSession[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ChatSession.html);\nimportcom.google.cloud.vertexai.generativeai.ContentMaker[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ContentMaker.html);\nimportcom.google.cloud.vertexai.generativeai.GenerativeModel[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.GenerativeModel.html);\nimportcom.google.cloud.vertexai.generativeai.PartMaker[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.PartMaker.html);\nimportcom.google.cloud.vertexai.generativeai.ResponseHandler[](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ResponseHandler.html);\nimportjava.io.IOException;\nimportjava.util.Arrays;\nimportjava.util.Collections;\n\npublicclass FunctionCalling{\npublicstaticvoidmain(String[]args)throwsIOException{\n// TODO(developer): Replace these variables before running the sample.\nStringprojectId=\"your-google-cloud-project-id\";\nStringlocation=\"us-central1\";\nStringmodelName=\"gemini-2.0-flash-001\";\n\nStringpromptText=\"What's the weather like in Paris?\";\n\nwhatsTheWeatherLike(projectId,location,modelName,promptText);\n}\n\n// A request involving the interaction with an external tool\npublicstaticStringwhatsTheWeatherLike(StringprojectId,Stringlocation,\nStringmodelName,StringpromptText)\nthrowsIOException{\n// Initialize client that will be used to send requests.\n// This client only needs to be created once, and can be reused for multiple requests.\ntry([VertexAI](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.VertexAI.html)vertexAI=new[VertexAI](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.VertexAI.html)(projectId,location)){\n\n[FunctionDeclaration](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.FunctionDeclaration.html)functionDeclaration=[FunctionDeclaration](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.FunctionDeclaration.html).newBuilder()\n.setName(\"getCurrentWeather\")\n.setDescription(\"Get the current weather in a given location\")\n.setParameters(\n[Schema](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Schema.html).newBuilder()\n.setType([Type](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Type.html).OBJECT)\n.putProperties(\"location\",[Schema](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Schema.html).newBuilder()\n.setType([Type](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Type.html).STRING)\n.setDescription(\"location\")\n.build()\n)\n.addRequired(\"location\")\n.build()\n)\n.build();\n\nSystem.out.println(\"Function declaration:\");\nSystem.out.println(functionDeclaration);\n\n// Add the function to a \"tool\"\n[Tool](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Tool.html)tool=[Tool](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Tool.html).newBuilder()\n.addFunctionDeclarations(functionDeclaration)\n.build();\n\n// Start a chat session from a model, with the use of the declared function.\n[GenerativeModel](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.GenerativeModel.html)model=new[GenerativeModel](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.GenerativeModel.html)(modelName,vertexAI)\n.withTools(Arrays.asList(tool));\n[ChatSession](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ChatSession.html)chat=model.[startChat](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.GenerativeModel.html#com_google_cloud_vertexai_generativeai_GenerativeModel_startChat__)();\n\nSystem.out.println(String.format(\"Ask the question: %s\",promptText));\n[GenerateContentResponse](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.GenerateContentResponse.html)response=chat.[sendMessage](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ChatSession.html#com_google_cloud_vertexai_generativeai_ChatSession_sendMessage_com_google_cloud_vertexai_api_Content_)(promptText);\n\n// The model will most likely return a function call to the declared\n// function `getCurrentWeather` with \"Paris\" as the value for the\n// argument `location`.\nSystem.out.println(\"\\nPrint response: \");\nSystem.out.println([ResponseHandler](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ResponseHandler.html).getContent(response));\n\n// Provide an answer to the model so that it knows what the result\n// of a \"function call\" is.\n[Content](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.api.Content.html)content=\n[ContentMaker](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ContentMaker.html).fromMultiModalData(\n[PartMaker](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.PartMaker.html).fromFunctionResponse(\n\"getCurrentWeather\",\nCollections.singletonMap(\"currentWeather\",\"sunny\")));\nSystem.out.println(\"Provide the function response: \");\nSystem.out.println(content);\nresponse=chat.[sendMessage](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ChatSession.html#com_google_cloud_vertexai_generativeai_ChatSession_sendMessage_com_google_cloud_vertexai_api_Content_)(content);\n\n// See what the model replies now\nSystem.out.println(\"Print response: \");\nStringfinalAnswer=[ResponseHandler](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.ResponseHandler.html).getText(response);\nSystem.out.println(finalAnswer);\n\nreturnfinalAnswer;\n}\n}\n}\n```\n\nIf the model determines that it needs the output of a particular function, the response that the application receives from the model contains the function name and the parameter values that the function should be called with.\nThe following is an example of a model response to the user prompt \"What is the weather like in Boston?\". The model proposes calling the `get_current_weather` function with the parameter `Boston, MA`.\n```\ncandidates {\n  content {\n    role: \"model\"\n    parts {\n      function_call {\n        name: \"get_current_weather\"\n        args {\n          fields {\n            key: \"location\"\n            value {\n              string_value: \"Boston, MA\"\n            }\n          }\n        }\n      }\n    }\n  }\n  ...\n}\n\n```\n\n### Step 2: Provide the API output to the model\nInvoke the external API and pass the API output back to the model.\nThe following example uses synthetic data to simulate a response payload from an external API and submits the output back to the model:\n###  REST \n```\nPROJECT_ID=myproject\nMODEL_ID=gemini-2.0-flash\nLOCATION=\"us-central1\"\n\ncurl\\\n-H\"Authorization: Bearer $(gcloud)\"\\\n-H\"Content-Type: application/json\"\\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent\\\n-d'{\n\"contents\": [\n{\n  \"role\": \"user\",\n  \"parts\": {\n    \"text\": \"What is the weather in Boston?\"\n  }\n},\n{\n  \"role\": \"model\",\n  \"parts\": [\n    {\n      \"functionCall\": {\n        \"name\": \"get_current_weather\",\n        \"args\": {\n          \"location\": \"Boston, MA\"\n        }\n      }\n    }\n  ]\n},\n{\n  \"role\": \"user\",\n  \"parts\": [\n    {\n      \"functionResponse\": {\n        \"name\": \"get_current_weather\",\n        \"response\": {\n          \"temperature\": 20,\n          \"unit\": \"C\"\n        }\n      }\n    }\n  ]\n}\n],\n\"tools\": [\n{\n  \"function_declarations\": [\n    {\n      \"name\": \"get_current_weather\",\n      \"description\": \"Get the current weather in a specific location\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city name of the location for which to get the weather.\"\n          }\n        },\n        \"required\": [\n          \"location\"\n        ]\n      }\n    }\n  ]\n}\n]\n}'\n\n```\n\n###  Python \n```\nfunction_response_contents = []\nfunction_response_parts = []\n\n# Iterates through the function calls in the response in case there are parallel function call requests\nfor function_call in response.candidates[0].function_calls:\n    print(f\"Function call: {function_call.name}\")\n\n    # In this example, we'll use synthetic data to simulate a response payload from an external API\n    if (function_call.args['location'] == \"Boston, MA\"):\n      api_response = { \"location\": \"Boston, MA\", \"temperature\": 38, \"description\": \"Partly Cloudy\" }\n    if (function_call.args['location'] == \"San Francisco, CA\"):\n      api_response = { \"location\": \"San Francisco, CA\", \"temperature\": 58, \"description\": \"Sunny\" }\n\n    function_response_parts.append(\n        Part.from_function_response(\n            name=function_call.name,\n            response={\"contents\": api_response}\n        )\n    )\n    # Add the function call response to the contents\n    function_response_contents = Content(role=\"user\", parts=function_response_parts)\n\n# Submit the User's prompt, model's response, and API output back to the model\nresponse = model.generate_content(\n  [\n    Content( # User prompt\n      role=\"user\",\n      parts=[\n          Part.from_text(\"What is the weather like in Boston?\"),\n      ],\n    ),\n    response.candidates[0].content,  # Function call response\n    function_response_contents   # API output\n  ],\n  tools=[\n    Tool(\n      function_declarations=[get_current_weather_func],\n    )\n  ],\n)\n# Get the model summary response\nprint(response.text)\n\n```\n\nFor best practices related to API invocation, see [Best practices - API invocation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#invoke-api-bp).\nIf the model had proposed several parallel function calls, the application must provide all of the responses back to the model. To learn more, see [Parallel function calling example](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#parallel-samples).\nThe model may determine that the output of another function is necessary for responding to the prompt. In this case, the response that the application receives from the model contains another function name and another set of parameter values.\nIf the model determines that the API response is sufficient for responding to the user's prompt, it creates a natural language response and returns it to the application. In this case, the application must pass the response back to the user. The following is an example of a natural language response:\n```\nIt is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies.\n\n```\n\n## Function calling with thoughts\nTo see an example of how to use thought signatures, run the \"Intro to thought signatures\" notebook in one of the following environments: \n[![](https://docs.cloud.google.com/static/vertex-ai/images/colab-logo-32px.png)Open in Colab](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/thinking/intro_thought_signatures.ipynb) | [![](https://docs.cloud.google.com/static/vertex-ai/images/colab-enterprise-logo-32px.png)Open in Colab Enterprise](https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fthinking%2Fintro_thought_signatures.ipynb) | [![](https://docs.cloud.google.com/static/vertex-ai/images/vertex-ai-workbench-logo-32px.png)Open in Vertex AI Workbench](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fthinking%2Fintro_thought_signatures.ipynb) | [![](https://docs.cloud.google.com/static/vertex-ai/images/github-logo-32px.png)View on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/thinking/intro_thought_signatures.ipynb)\nWhen calling functions with [thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking) enabled, you'll need to get the [`thought_signature`](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures) from the model response object and return it when you send the result of the function execution back to the model. For example:\n### Python\n```\n# Call the model with function declarations\n# ...Generation config, Configure the client, and Define user prompt (No changes)\n\n# Send request with declarations (using a thinking model)\nresponse = client.models.generate_content(\n  model=\"gemini-2.5-flash\", config=config, contents=contents)\n\n# See thought signatures\nfor part in response.candidates[0].content.parts:\n  if not part.text:\n    continue\n  if part.thought and part.thought_signature:\n    print(\"Thought signature:\")\n    print(part.thought_signature)\n\n```\n\nViewing thought signatures isn't required, but you will need to adjust [Step 2](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#invoke-api) to return them along with the result of the function execution so it can incorporate the thoughts into its final response:\n### Python\n```\n# Create user friendly response with function result and call the model again\n# ...Create a function response part (No change)\n\n# Append thought signatures, function call and result of the function execution to contents\nfunction_call_content = response.candidates[0].content\n# Append the model's function call message, which includes thought signatures\ncontents.append(function_call_content)\ncontents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n\nfinal_response = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    config=config,\n    contents=contents,\n)\n\nprint(final_response.text)\n\n```\n\nWhen returning thought signatures, follow these guidelines:\n  * The model returns signatures within other parts in the response, for example function calling or text, text, or thought summaries parts. Return the entire response with all parts back to the model in subsequent turns.\n  * Don't merge part with one signature with another part which also contains a signature. Signatures can't be concatenated together.\n  * Don't merge one part with a signature with another part without a signature. This breaks the correct positioning of the thought represented by the signature.\n\n\nLearn more about limitations and usage of [thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures), and about [thinking models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking).\n## Parallel function calling\nFor prompts such as \"Get weather details in Boston and San Francisco?\", the model may propose several parallel function calls. For a list of models that support parallel function calling, see [Supported models](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#features).\nTo learn about parallel function calling with an end-to-end Jupyter notebook tutorial, see [Working with Parallel Function Calls and Multiple Function Responses in Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/parallel_function_calling.ipynb)\n###  REST \nThis example demonstrates a scenario with one `get_current_weather` function. The user prompt is \"Get weather details in Boston and San Francisco?\". The model proposes two parallel `get_current_weather` function calls: one with the parameter `Boston` and the other with the parameter `San Francisco`.\nTo learn more about the request parameters, see [Gemini API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini).\n```\n{\n\"candidates\": [\n  {\n    \"content\": {\n      \"role\": \"model\",\n      \"parts\": [\n        {\n          \"functionCall\": {\n            \"name\": \"get_current_weather\",\n            \"args\": {\n              \"location\": \"Boston\"\n            }\n          }\n        },\n        {\n          \"functionCall\": {\n            \"name\": \"get_current_weather\",\n            \"args\": {\n              \"location\": \"San Francisco\"\n            }\n          }\n        }\n      ]\n    },\n    ...\n  }\n],\n...\n}\n\n```\n\nThe following command demonstrates how you can provide the function output to the model. Replace my-project with the name of your Google Cloud project.\n#### Model request\n```\nPROJECT_ID=my-project\nMODEL_ID=gemini-2.0-flash\nLOCATION=\"us-central1\"\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent \\\n-d '{\n\"contents\": [\n{\n  \"role\": \"user\",\n  \"parts\": {\n    \"text\": \"What is difference in temperature in Boston and San Francisco?\"\n  }\n},\n{\n  \"role\": \"model\",\n  \"parts\": [\n    {\n      \"functionCall\": {\n        \"name\": \"get_current_weather\",\n        \"args\": {\n          \"location\": \"Boston\"\n        }\n      }\n    },\n    {\n      \"functionCall\": {\n        \"name\": \"get_current_weather\",\n        \"args\": {\n          \"location\": \"San Francisco\"\n        }\n      }\n    }\n  ]\n},\n{\n  \"role\": \"user\",\n  \"parts\": [\n    {\n      \"functionResponse\": {\n        \"name\": \"get_current_weather\",\n        \"response\": {\n          \"temperature\": 30.5,\n          \"unit\": \"C\"\n        }\n      }\n    },\n    {\n      \"functionResponse\": {\n        \"name\": \"get_current_weather\",\n        \"response\": {\n          \"temperature\": 20,\n          \"unit\": \"C\"\n        }\n      }\n    }\n  ]\n}\n],\n\"tools\": [\n{\n  \"function_declarations\": [\n    {\n      \"name\": \"get_current_weather\",\n      \"description\": \"Get the current weather in a specific location\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city name of the location for which to get the weather.\"\n          }\n        },\n        \"required\": [\n          \"location\"\n        ]\n      }\n    }\n  ]\n}\n]\n}'\n  \n```\n\nThe natural language response created by the model is similar to the following:\n#### Model response\n```\n[\n{\n    \"candidates\": [\n        {\n            \"content\": {\n                \"parts\": [\n                    {\n                        \"text\": \"The temperature in Boston is 30.5C and the temperature in San Francisco is 20C. The difference is 10.5C. \\n\"\n                    }\n                ]\n            },\n            \"finishReason\": \"STOP\",\n            ...\n        }\n    ]\n    ...\n}\n]\n  \n```\n\n###  Python \nThis example demonstrates a scenario with one `get_current_weather` function. The user prompt is \"What is the weather like in Boston and San Francisco?\".\nReplace my-project with the name of your Google Cloud project.\n```\nimport[vertexai](https://docs.cloud.google.com/python/docs/reference/vertexai/latest)\nfromvertexai.generative_modelsimport (\n    [Content](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Content.html),\n    [FunctionDeclaration](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.FunctionDeclaration.html),\n    [GenerationConfig](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.GenerationConfig.html),\n    [GenerativeModel](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.GenerativeModel.html),\n    [Part](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Part.html),\n    [Tool](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Tool.html),\n    [ToolConfig](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.ToolConfig.html)\n)\n\n# Initialize Vertex AI\n# TODO(developer): Update the project\n[vertexai](https://docs.cloud.google.com/python/docs/reference/vertexai/latest).init(project=\"my-project\", location=\"us-central1\")\n\n# Initialize Gemini model\nmodel = GenerativeModel(model_name=\"gemini-2.0-flash\")\n\n# Manual function declaration\nget_current_weather_func = FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    # Function parameters are specified in JSON schema format\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city name of the location for which to get the weather.\",\n              \"default\": {\n                \"string_value\": \"Boston, MA\"\n              }\n          }\n        },\n    },\n)\n\nresponse = model.[generate_content](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.GenerativeModel.html#vertexai_preview_generative_models_GenerativeModel_generate_content)(\n    contents = [\n      Content(\n        role=\"user\",\n          parts=[\n              [Part](https://docs.cloud.google.com/python/docs/reference/vertexai/latest/vertexai.preview.generative_models.Part.html).from_text(\"What is the weather like in Boston and San Francisco?\"),\n          ],\n      )\n    ],\n    generation_config = GenerationConfig(temperature=0),\n    tools = [\n      Tool(\n        function_declarations=[get_current_weather_func],\n      )\n    ]\n)\n\n```\n\nThe following command demonstrates how you can provide the function output to the model.\n```\nfunction_response_contents = []\nfunction_response_parts = []\n\n# You can have parallel function call requests for the same function type.\n# For example, 'location_to_lat_long(\"London\")' and 'location_to_lat_long(\"Paris\")'\n# In that case, collect API responses in parts and send them back to the model\n\nfor function_call in response.candidates[0].function_calls:\n    print(f\"Function call: {function_call.name}\")\n\n    # In this example, we'll use synthetic data to simulate a response payload from an external API\n    if (function_call.args['location'] == \"Boston, MA\"):\n      api_response = { \"location\": \"Boston, MA\", \"temperature\": 38, \"description\": \"Partly Cloudy\" }\n    if (function_call.args['location'] == \"San Francisco, CA\"):\n      api_response = { \"location\": \"San Francisco, CA\", \"temperature\": 58, \"description\": \"Sunny\" }\n\n    function_response_parts.append(\n        Part.from_function_response(\n            name=function_call.name,\n            response={\"contents\": api_response}\n        )\n    )\n    # Add the function call response to the contents\n    function_response_contents = Content(role=\"user\", parts=function_response_parts)\n\nfunction_response_contents\n\nresponse = model.generate_content(\n    contents = [\n        Content(\n        role=\"user\",\n          parts=[\n              Part.from_text(\"What is the weather like in Boston and San Francisco?\"),\n          ],\n        ),  # User prompt\n        response.candidates[0].content,  # Function call response\n        function_response_contents,  # Function response\n    ],\n    tools = [\n      Tool(\n        function_declarations=[get_current_weather_func],\n      )\n    ]\n)\n# Get the model summary response\nprint(response.text)\n\n```\n\n###  Go \n```\nimport(\n\"context\"\n\"encoding/json\"\n\"errors\"\n\"fmt\"\n\"io\"\n\n\"cloud.google.com/go/vertexai/genai\"\n)\n\n// parallelFunctionCalling shows how to execute multiple function calls in parallel\n// and return their results to the model for generating a complete response.\nfuncparallelFunctionCalling(wio.Writer,projectID,location,modelNamestring)error{\n// location = \"us-central1\"\n// modelName = \"gemini-2.0-flash-001\"\nctx:=context.Background()\nclient,err:=genai.[NewClient](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Client_NewClient)(ctx,projectID,location)\niferr!=nil{\nreturnfmt.Errorf(\"failed to create GenAI client: %w\",err)\n}\ndeferclient.[Close](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Client_Close)()\n\nmodel:=client.GenerativeModel(modelName)\n// Set temperature to 0.0 for maximum determinism in function calling.\nmodel.[SetTemperature](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_GenerationConfig_SetTemperature)(0.0)\n\nfuncName:=\"getCurrentWeather\"\nfuncDecl:=genai.[FunctionDeclaration](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_FunctionDeclaration){\nName:funcName,\nDescription:\"Get the current weather in a given location\",\nParameters:genai.[Schema](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Schema){\nType:genai.[TypeObject](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_TypeUnspecified_TypeString_TypeNumber_TypeInteger_TypeBoolean_TypeArray_TypeObject),\nProperties:map[string]*genai.[Schema](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Schema){\n\"location\":{\nType:genai.[TypeString](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_TypeUnspecified_TypeString_TypeNumber_TypeInteger_TypeBoolean_TypeArray_TypeObject),\nDescription:\"The location for which to get the weather. \"+\n\"It can be a city name, a city name and state, or a zip code. \"+\n\"Examples: 'San Francisco', 'San Francisco, CA', '95616', etc.\",\n},\n},\nRequired:[]string{\"location\"},\n},\n}\n// Add the weather function to our model toolbox.\nmodel.Tools=[]*genai.[Tool](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Tool){\n{\nFunctionDeclarations:[]*genai.[FunctionDeclaration](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_FunctionDeclaration){funcDecl},\n},\n}\n\nprompt:=genai.[Text](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Text)(\"Get weather details in New Delhi and San Francisco?\")\nresp,err:=model.[GenerateContent](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_GenerativeModel_GenerateContent)(ctx,prompt)\n\niferr!=nil{\nreturnfmt.Errorf(\"failed to generate content: %w\",err)\n}\niflen(resp.Candidates)==0{\nreturnerrors.[New](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai/tokenizer.html#cloud_google_com_go_vertexai_genai_tokenizer_Tokenizer_New)(\"got empty response from model\")\n}elseiflen(resp.Candidates[0].[FunctionCalls](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Candidate_FunctionCalls)())==0{\nreturnerrors.[New](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai/tokenizer.html#cloud_google_com_go_vertexai_genai_tokenizer_Tokenizer_New)(\"got no function call suggestions from model\")\n}\n\n// In a production environment, consider adding validations for function names and arguments.\nfor_,fnCall:=rangeresp.Candidates[0].[FunctionCalls](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Candidate_FunctionCalls)(){\nfmt.Fprintf(w,\"The model suggests to call the function %q with args: %v\\n\",fnCall.[Name](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_GenerativeModel_Name),fnCall.Args)\n// Example response:\n// The model suggests to call the function \"getCurrentWeather\" with args: map[location:New Delhi]\n// The model suggests to call the function \"getCurrentWeather\" with args: map[location:San Francisco]\n}\n\n// Use synthetic data to simulate responses from the external API.\n// In a real application, this would come from an actual weather API.\nmockAPIResp1,err:=json.Marshal(map[string]string{\n\"location\":\"New Delhi\",\n\"temperature\":\"42\",\n\"temperature_unit\":\"C\",\n\"description\":\"Hot and humid\",\n\"humidity\":\"65\",\n})\niferr!=nil{\nreturnfmt.Errorf(\"failed to marshal function response to JSON: %w\",err)\n}\n\nmockAPIResp2,err:=json.Marshal(map[string]string{\n\"location\":\"San Francisco\",\n\"temperature\":\"36\",\n\"temperature_unit\":\"F\",\n\"description\":\"Cold and cloudy\",\n\"humidity\":\"N/A\",\n})\niferr!=nil{\nreturnfmt.Errorf(\"failed to marshal function response to JSON: %w\",err)\n}\n\n// Note, that the function calls don't have to be chained. We can obtain both responses in parallel\n// and return them to Gemini at once.\nfuncResp1:=genai.[FunctionResponse](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_FunctionResponse){\nName:funcName,\nResponse:map[string]any{\n\"content\":mockAPIResp1,\n},\n}\nfuncResp2:=genai.[FunctionResponse](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_FunctionResponse){\nName:funcName,\nResponse:map[string]any{\n\"content\":mockAPIResp2,\n},\n}\n\n// Return both API responses to the model allowing it to complete its response.\nresp,err=model.[GenerateContent](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_GenerativeModel_GenerateContent)(ctx,prompt,funcResp1,funcResp2)\niferr!=nil{\nreturnfmt.Errorf(\"failed to generate content: %w\",err)\n}\niflen(resp.Candidates)==0||len(resp.Candidates[0].[Content](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Content).Parts)==0{\nreturnerrors.[New](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai/tokenizer.html#cloud_google_com_go_vertexai_genai_tokenizer_Tokenizer_New)(\"got empty response from model\")\n}\n\nfmt.Fprintln(w,resp.Candidates[0].[Content](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest/genai.html#cloud_google_com_go_vertexai_genai_Content).Parts[0])\n// Example response:\n// The weather in New Delhi is hot and humid with a humidity of 65 and a temperature of 42°C. The weather in San Francisco ...\n\nreturnnil\n}\n\n```\n\n## Multimodal function responses\n**Preview**\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [Service Specific Terms](https://docs.cloud.google.com/terms/service-terms#1). Pre-GA features are available \"as is\" and might have limited support. For more information, see the [launch stage descriptions](https://cloud.google.com/products/#product-launch-stages). \nFor Gemini 3 Pro and later models, you can include multimodal content in the function response parts that you send to the model. The model can process this multimodal content in its next turn to produce a more informed response. The following MIME types are supported for multimodal content in function responses:\n  * **Images** : `image/png`, `image/jpeg`, `image/webp`\n  * **Documents** : `application/pdf`, `text/plain`\n\n\nTo include multimodal data in a function response, include it as one or more parts nested within the `functionResponse` part. Each multimodal part must contain `inlineData` or `fileData`. If you reference a multimodal part from within the structured `response` field, it must contain a unique `displayName`.\nYou can also reference a multimodal part from within the structured `response` field of the `functionResponse` part by using the JSON reference format `{\"$ref\": \"<displayName>\"}`. The model substitutes the reference with the multimodal content when processing the response. Each `displayName` can only be referenced once in the structured `response` field.\nThe following example shows a message containing a `functionResponse` for a function named `get_image` and a nested part containing image data with `displayName: \"wakeupcat.jpg\"`. The `functionResponse`'s `response` field references this image part:\n###  Python \n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\nclient = genai.Client()\n\n# This is a manual, two turn multimodal function calling workflow:\n\n# 1. Define the function tool\nget_image_declaration = types.FunctionDeclaration(\n  name=\"get_image\",\n  description=\"Retrieves the image file reference for a specific order item.\",\n  parameters={\n      \"type\": \"object\",\n      \"properties\": {\n          \"item_name\": {\n              \"type\": \"string\",\n              \"description\": \"The name or description of the item ordered (e.g., 'green shirt').\"\n          }\n      },\n      \"required\": [\"item_name\"],\n  },\n)\ntool_config = types.Tool(function_declarations=[get_image_declaration])\n\n# 2. Send a message that triggers the tool\nprompt = \"Show me the green shirt I ordered last month.\"\nresponse_1 = client.models.generate_content(\n  model=\"gemini-3-pro-preview\",\n  contents=[prompt],\n  config=types.GenerateContentConfig(\n      tools=[tool_config],\n  )\n)\n\n# 3. Handle the function call\nfunction_call = response_1.function_calls[0]\nrequested_item = function_call.args[\"item_name\"]\nprint(f\"Model wants to call: {function_call.name}\")\n\n# Execute your tool (e.g., call an API)\n# (This is a mock response for the example)\nprint(f\"Calling external tool for: {requested_item}\")\n\nfunction_response_data = {\n  \"image_ref\": {\"$ref\": \"dress.jpg\"},\n}\n\nfunction_response_multimodal_data = types.FunctionResponsePart(\n  file_data=types.FunctionResponseFileData(\n    mime_type=\"image/png\",\n    display_name=\"dress.jpg\",\n    file_uri=\"gs://cloud-samples-data/generative-ai/image/dress.jpg\",\n  )\n)\n\n# 4. Send the tool's result back\n# Append this turn's messages to history for a final response.\nhistory = [\n  types.Content(role=\"user\", parts=[types.Part(text=prompt)]),\n  response_1.candidates[0].content,\n  types.Content(\n    role=\"tool\",\n    parts=[\n        types.Part.from_function_response(\n          name=function_call.name,\n          response=function_response_data,\n          parts=[function_response_multimodal_data]\n        )\n    ],\n  )\n]\n\nresponse_2 = client.models.generate_content(\n  model=\"gemini-3-pro-preview\",\n  contents=history,\n  config=types.GenerateContentConfig(\n      tools=[tool_config],\n      thinking_config=types.ThinkingConfig(include_thoughts=True)\n  ),\n)\n\nprint(f\"\\nFinal model response: {response_2.text}\")\n\n```\n\n###  REST \n```\n\"contents\":[\n...,\n{\n\"role\":\"user\",\n\"parts\":[\n{\n\"functionResponse\":{\n\"name\":\"get_image\",\n\"response\":{\n\"image_ref\":{\n\"$ref\":\"wakeupcat.jpg\"\n}\n},\n\"parts\":[\n{\n\"fileData\":{\n\"displayName\":\"wakeupcat.jpg\",\n\"mimeType\":\"image/jpeg\",\n\"fileUri\":\"gs://cloud-samples-data/vision/label/wakeupcat.jpg\"\n}\n}\n]\n}\n}\n]\n}\n]\n\n```\n\n## Streaming function call arguments\n**Preview**\nThis feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [Service Specific Terms](https://docs.cloud.google.com/terms/service-terms#1). Pre-GA features are available \"as is\" and might have limited support. For more information, see the [launch stage descriptions](https://cloud.google.com/products/#product-launch-stages). \nFor Gemini 3 Pro and later models, you can request that function call arguments be streamed as they are generated by the model, rather than waiting for the full set of arguments to be generated. This is useful for reducing perceived latency when functions need to be called.\nThis feature has the following limitations:\n  * This feature is available in both `v1` and `v1beta1` API versions.\n\n\nTo enable streaming for function call arguments, set `streamFunctionCallArguments` to `true` inside `toolConfig.functionCallingConfig` when calling `streamGenerateContent`.\nWhen `streamFunctionCallArguments` is enabled, intermediate responses will contain a `functionCall` object with `partialArgs` and `willContinue` fields. `partialArgs` contains argument fragments as they are generated, and `willContinue` indicates if more fragments for the function call are expected.\n  * `partialArgs`: An array of `PartialArg` objects, each containing: \n    * `jsonPath`: A JSONPath string indicating the path to this fragment within the function's parameters object. The path can point to an argument (e.g. `$.location`) or an element inside an argument if the argument is an object (e.g. `$.location.latitude`).\n    * A value for the fragment, which can be `numberValue`, `stringValue`, `boolValue`, or `nullValue`.\n    * `willContinue`: A boolean within a `partialArgs` object. This field is only `true` for `stringValue` fragments when the string value is being streamed in chunks and more chunks are expected for this argument.\n  * `willContinue`: A boolean within the `functionCall` object. If `true`, more `partialArgs` are expected for the overall function call in subsequent streamed responses. If `false` or absent, this is the final streamed response for the current function call.\n\n\nThe following example shows a sequence of streamed `generateContent` chunks where arguments for a single function call are streamed:\n```\n{\n\"parts\":[\n{\n\"functionCall\":{\n\"name\":\"controlLight\",\n\"partialArgs\":[\n{\n\"jsonPath\":\"$.brightness\",\n\"numberValue\":50\n}\n],\n\"willContinue\":true\n}\n}\n],\n\"role\":\"model\"\n}\n{\n\"parts\":[\n{\n\"functionCall\":{\n\"partialArgs\":[\n{\n\"jsonPath\":\"$.colorTemperature\",\n\"stringValue\":\"warm\",\n\"willContinue\":true\n}\n],\n\"willContinue\":true\n}\n}\n],\n\"role\":\"model\"\n}\n{\n\"parts\":[\n{\n\"functionCall\":{\n\"partialArgs\":[\n{\n\"jsonPath\":\"$.colorTemperature\"\n}\n],\n\"willContinue\":true\n}\n}\n],\n\"role\":\"model\"\n}\n{\n\"parts\":[\n{\n\"functionCall\":{}\n}\n],\n\"role\":\"model\"\n}\n\n```\n\nThe following example shows how arguments for parallel function calls might be streamed across multiple responses for the prompt \"What is difference in temperature in New Delhi and San Francisco?\":\n```\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n\"name\":\"get_current_weather\",\n\"willContinue\":true\n},\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n\"partialArgs\":[{\n\"jsonPath\":\"$.location\",\n\"stringValue\":\"New Delhi\",\n\"willContinue\":true\n}],\n\"willContinue\":true\n}\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n\"partialArgs\":[{\n\"jsonPath\":\"$.location\",\n\"stringValue\":\"\"\n}],\n\"willContinue\":true\n}\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n}\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n\"name\":\"get_current_weather\",\n\"willContinue\":true\n},\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n\"partialArgs\":[{\n\"jsonPath\":\"$.location\",\n\"stringValue\":\"San Francisco\",\n\"willContinue\":true\n}],\n\"willContinue\":true\n}\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n\"partialArgs\":[{\n\"jsonPath\":\"$.location\",\n\"stringValue\":\"\"\n}],\n\"willContinue\":true\n}\n}]\n}\n}],\n}\n{\n\"candidates\":[{\n\"content\":{\n\"role\":\"model\",\n\"parts\":[{\n\"functionCall\":{\n}\n}]\n}\n}],\n}\n\n```\n\n###  Python \n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\nclient = genai.Client()\n\nget_weather_declaration = types.FunctionDeclaration(\n  name=\"get_weather\",\n  description=\"Gets the current weather temperature for a given location.\",\n  parameters={\n      \"type\": \"object\",\n      \"properties\": {\"location\": {\"type\": \"string\"}},\n      \"required\": [\"location\"],\n  },\n)\nget_weather_tool = types.Tool(function_declarations=[get_weather_declaration])\n\nfor chunk in client.models.generate_content_stream(\n  model=\"gemini-3-pro-preview\",\n  contents=\"What's the weather in London and New York?\",\n  config=types.GenerateContentConfig(\n      tools=[get_weather_tool],\n      tool_config = types.ToolConfig(\n          function_calling_config=types.FunctionCallingConfig(\n              mode=types.FunctionCallingConfigMode.AUTO,\n              stream_function_call_arguments=True,\n          )\n      ),\n  ),\n):\n  function_call = chunk.function_calls[0]\n  if function_call and function_call.name:\n      print(f\"{function_call.name}\")\n      print(f\"will_continue={function_call.will_continue}\")\n\n\n```\n\n## Function calling modes\nYou can control how the model uses the provided tools (function declarations) by setting the mode within the `function_calling_config`.\nMode | Description  \n---|---  \n`AUTO` | The default model behavior. The model decides whether to predict function calls or respond with natural language based on the context. This is the most flexible mode and recommended for most scenarios.  \n`VALIDATED` (Preview) | The model is constrained to predict either function calls or natural language, and ensures function schema adherence. If `allowed_function_names` is not provided, the model picks from all of the available function declarations. If `allowed_function_names` is provided, the model picks from the set of allowed functions.  \n`ANY` | The model is constrained to always predict one or more function calls and ensures function schema adherence. If `allowed_function_names` is not provided, the model picks from all of the available function declarations. If `allowed_function_names` is provided, the model picks from the set of allowed functions. Use this mode when you require a function call response to every prompt (if applicable).  \n`NONE` | The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this mode to temporarily disable function calling without removing your tool definitions.  \n**Note:** Preview features are only available in the v1beta1 version of the API.\n### Forced function calling\nInstead of allowing the model to choose between a natural language response and a function call, you can force it to only predict function calls. This is known as _forced function calling_. You can also choose to provide the model with a full set of function declarations, but restrict its responses to a subset of these functions.\nThe following example is forced to predict only `get_weather` function calls.\n###  Python \n```\nresponse = model.generate_content(\n    contents = [\n      Content(\n        role=\"user\",\n          parts=[\n              Part.from_text(\"What is the weather like in Boston?\"),\n          ],\n      )\n    ],\n    generation_config = GenerationConfig(temperature=0),\n    tools = [\n      Tool(\n        function_declarations=[get_weather_func, some_other_function],\n      )\n    ],\n    tool_config=ToolConfig(\n        function_calling_config=ToolConfig.FunctionCallingConfig(\n            # ANY mode forces the model to predict only function calls\n            mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n            # Allowed function calls to predict when the mode is ANY. If empty, any of\n            # the provided function calls will be predicted.\n            allowed_function_names=[\"get_weather\"],\n        )\n    )\n)\n\n```\n\n## Function schema examples\nFunction declarations are compatible with the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema). We support the following attributes: `type`, `nullable`, `required`, `format`, `description`, `properties`, `items`, `enum`, `anyOf`, `$ref`, and `$defs`. Remaining attributes are not supported.\n#### Function with object and array parameters\nThe following example uses a Python dictionary to declare a function that takes both object and array parameters:\n```\nextract_sale_records_func = FunctionDeclaration(\n  name=\"extract_sale_records\",\n  description=\"Extract sale records from a document.\",\n  parameters={\n      \"type\": \"object\",\n      \"properties\": {\n          \"records\": {\n              \"type\": \"array\",\n              \"description\": \"A list of sale records\",\n              \"items\": {\n                  \"description\": \"Data for a sale record\",\n                  \"type\": \"object\",\n                  \"properties\": {\n                      \"id\": {\"type\": \"integer\", \"description\": \"The unique id of the sale.\"},\n                      \"date\": {\"type\": \"string\", \"description\": \"Date of the sale, in the format of MMDDYY, e.g., 031023\"},\n                      \"total_amount\": {\"type\": \"number\", \"description\": \"The total amount of the sale.\"},\n                      \"customer_name\": {\"type\": \"string\", \"description\": \"The name of the customer, including first name and last name.\"},\n                      \"customer_contact\": {\"type\": \"string\", \"description\": \"The phone number of the customer, e.g., 650-123-4567.\"},\n                  },\n                  \"required\": [\"id\", \"date\", \"total_amount\"],\n              },\n          },\n      },\n      \"required\": [\"records\"],\n  },\n)\n  \n```\n\n#### Function with enum parameter\nThe following example uses a Python dictionary to declare a function that takes an integer `enum` parameter:\n```\nset_status_func = FunctionDeclaration(\n  name=\"set_status\",\n  description=\"set a ticket's status field\",\n  # Function parameters are specified in JSON schema format\n  parameters={\n      \"type\": \"object\",\n      \"properties\": {\n        \"status\": {\n          \"type\": \"integer\",\n          \"enum\": [ \"10\", \"20\", \"30\" ],   # Provide integer (or any other type) values as strings.\n        }\n      },\n  },\n)\n  \n```\n\n#### Function with ref and def\nThe following JSON function declaration uses the `ref` and `defs` attributes:\n```\n{\n  \"contents\": ...,\n  \"tools\": [\n    {\n      \"function_declarations\": [\n        {\n          \"name\": \"get_customer\",\n          \"description\": \"Search for a customer by name\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"first_name\": { \"ref\": \"#/defs/name\" },\n              \"last_name\": { \"ref\": \"#/defs/name\" }\n            },\n            \"defs\": {\n              \"name\": { \"type\": \"string\" }\n            }\n          }\n        }\n      ]\n    }\n  ]\n}\n  \n```\n\nUsage notes:\n  * Unlike, the OpenAPI schema, specify `ref` and `defs` without the `$` symbol.\n  * `ref` must refer to direct child of `defs`; no external references.\n  * The maximum depth of nested schema is 32.\n  * Recursion depth in `defs` (self-reference) is limited to two.\n\n\n####  `from_func` with array parameter\nThe following code sample declares a function that multiplies an array of numbers and uses `from_func` to generate the `FunctionDeclaration` schema.\n```\nfromtypingimport List\n\n# Define a function. Could be a local function or you can import the requests library to call an API\ndefmultiply_numbers(numbers: List[int] = [1, 1]) -> int:\n\"\"\"\n  Calculates the product of all numbers in an array.\n\n  Args:\n      numbers: An array of numbers to be multiplied.\n\n  Returns:\n      The product of all the numbers. If the array is empty, returns 1.\n  \"\"\"\n\n  if not numbers:  # Handle empty array\n      return 1\n\n  product = 1\n  for num in numbers:\n      product *= num\n\n  return product\n\nmultiply_number_func = FunctionDeclaration.from_func(multiply_numbers)\n\n\"\"\"\nmultiply_number_func contains the following schema:\n\n{'name': 'multiply_numbers',\n  'description': 'Calculates the product of all numbers in an array.',\n  'parameters': {'properties': {'numbers': {'items': {'type': 'INTEGER'},\n    'description': 'list of numbers',\n    'default': [1.0, 1.0],\n    'title': 'Numbers',\n    'type': 'ARRAY'}},\n  'description': 'Calculates the product of all numbers in an array.',\n  'title': 'multiply_numbers',\n  'property_ordering': ['numbers'],\n  'type': 'OBJECT'}}\n\"\"\"\n  \n```\n\n## Best practices for function calling\nTo improve your results when using function calling, follow these best practices:\n### Write clear and detailed function names, parameter descriptions, and instructions\n  * Function names should start with a letter or an underscore and contain only characters a-z, A-Z, 0-9, underscores, dots or dashes with a maximum length of 64.\n  * Be extremely clear and specific in your function and parameter descriptions. The model relies on these to choose the correct function and provide appropriate arguments. For example, a `book_flight_ticket` function could have the description `book flight tickets after confirming users' specific requirements, such as time, departure, destination, party size and preferred airline`\n\n\n### Use strong typed parameters\nIf the parameter values are from a finite set, add an `enum` field instead of putting the set of values into the description. If the parameter value is always an integer, set the type to `integer` rather than `number`.\n### Tool Selection\nWhile the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. If you have a large total number of tools, consider dynamic tool selection based on conversation context.\nIf you provide generic, low-level tools (like `bash`) the model might use the tool more often, but with less accuracy. If you provide a specific, high-level tool (like `get_weather`), the model will be able to use the tool more accurately, but the tool might not be used as often.\n### Use system instructions\nWhen using functions with date, time, or location parameters, include the current date, time, or relevant location information (for example, city and country) in the system instruction. This provides the model with the necessary context to process the request accurately, even if the user's prompt lacks details.\n### Prompt engineering\nFor best results, prepend the user prompt with the following details:\n  * Additional context for the model-for example, `You are a flight API assistant to help with searching flights based on user preferences.`\n  * Details or instructions on how and when to use the functions-for example, `Don't make assumptions on the departure or destination airports. Always use a future date for the departure or destination time.`\n  * Instructions to ask clarifying questions if user queries are ambiguous-for example, `Ask clarifying questions if not enough information is available.`\n\n\n### Use generation configuration\nFor the temperature parameter, use `0` or another low value. This instructs the model to generate more confident results and reduces hallucinations.\n### Use structured output\nFunction calling can be used together with [structured output](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) to let the model always predict function calls or outputs that adheres to a specific schema, so that you receive consistently formatted responses when model does not generate function calls.\n### Validate the API call\nIf the model proposes the invocation of a function that would send an order, update a database, or otherwise have significant consequences, validate the function call with the user before executing it.\n### Use thought signatures\n[Thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures) should always be used with function calling for best results.\n## Pricing\nThe pricing for function calling is based on the number of characters within the text inputs and outputs. To learn more, see [Vertex AI pricing](https://docs.cloud.google.com/vertex-ai/generative-ai/pricing).\nHere, text input (prompt) refers to the user prompt for the current conversation turn, the function declarations for the current conversation turn, and the history of the conversation. The history of the conversation includes the queries, the function calls, and the function responses of previous conversation turns. Vertex AI truncates the history of the conversation at 32,000 characters.\nText output (response) refers to the function calls and the text responses for the current conversation turn.\n## Use cases of function calling\nYou can use function calling for the following tasks:\nUse Case | Example description | Example link  \n---|---|---  \nIntegrate with external APIs | Get weather information using a meteorological API | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)  \nConvert addresses to latitude/longitude coordinates | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)  \nConvert currencies using a currency exchange API | [Codelab](https://codelabs.developers.google.com/codelabs/gemini-function-calling)  \nBuild advanced chatbots | Answer customer questions about products and services | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)  \nCreate an assistant to answer financial and news questions about companies | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/use_case_company_news_and_insights.ipynb)  \nStructure and control function calls | Extract structured entities from raw log data | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)  \nExtract single or multiple parameters from user input | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/function_calling_data_structures.ipynb)  \nHandle lists and nested data structures in function calls | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/function_calling_data_structures.ipynb)  \nHandle function calling behavior | Handle parallel function calls and responses | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/parallel_function_calling.ipynb)  \nManage when and which functions the model can call | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/forced_function_calling.ipynb)  \nQuery databases with natural language | Convert natural language questions into SQL queries for BigQuery | [Sample app](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/function-calling/sql-talk-app)  \nMultimodal function calling | Use images, videos, audio, and PDFs as input to trigger function calls | [Notebook tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/multimodal_function_calling.ipynb)  \nHere are some more use cases:\n  * **Interpret voice commands** : Create functions that correspond with in-vehicle tasks. For example, you can create functions that turn on the radio or activate the air conditioning. Send audio files of the user's voice commands to the model, and ask the model to convert the audio into text and identify the function that the user wants to call.\n  * **Automate workflows based on environmental triggers** : Create functions to represent processes that can be automated. Provide the model with data from environmental sensors and ask it to parse and process the data to determine whether one or more of the workflows should be activated. For example, a model could process temperature data in a warehouse and choose to activate a sprinkler function.\n  * **Automate the assignment of support tickets** : Provide the model with support tickets, logs, and context-aware rules. Ask the model to process all of this information to determine who the ticket should be assigned to. Call a function to assign the ticket to the person suggested by the model.\n  * **Retrieve information from a knowledge base** : Create functions that retrieve academic articles on a given subject and summarize them. Enable the model to answer questions about academic subjects and provide citations for its answers.\n\n\n## What's next\n  * See the [API reference for function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling).\n  * Learn about [Vertex AI Agent Engine](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview).\n\n\nSend feedback \nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-27 UTC.\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Hard to understand\",\"hardToUnderstand\",\"thumb-down\"],[\"Incorrect information or sample code\",\"incorrectInformationOrSampleCode\",\"thumb-down\"],[\"Missing the information/samples I need\",\"missingTheInformationSamplesINeed\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-11-27 UTC.\"],[],[]] \n  * ### Products and pricing\n    * [ See all products ](https://cloud.google.com/products/)\n    * [ Google Cloud pricing ](https://cloud.google.com/pricing/)\n    * [ Google Cloud Marketplace ](https://cloud.google.com/marketplace/)\n    * [ Contact sales ](https://cloud.google.com/contact/)\n  * ### Support\n    * [ Community forums ](https://discuss.google.dev/c/google-cloud/14/)\n    * [ Support ](https://cloud.google.com/support-hub/)\n    * [ Release Notes ](https://docs.cloud.google.com/release-notes)\n    * [ System status ](https://status.cloud.google.com)\n  * ### Resources\n    * [ GitHub ](https://github.com/googlecloudPlatform/)\n    * [ Getting Started with Google Cloud ](https://docs.cloud.google.com/docs/get-started/)\n    * [ Code samples ](https://docs.cloud.google.com/docs/samples)\n    * [ Cloud Architecture Center ](https://docs.cloud.google.com/architecture/)\n    * [ Training and Certification ](https://cloud.google.com/learn/training/)\n  * ### Engage\n    * [ Blog ](https://cloud.google.com/blog/)\n    * [ Events ](https://cloud.google.com/events/)\n    * [ X (Twitter) ](https://x.com/googlecloud)\n    * [ Google Cloud on YouTube ](https://www.youtube.com/googlecloud)\n    * [ Google Cloud Tech on YouTube ](https://www.youtube.com/googlecloudplatform)\n\n\n  * [ About Google ](https://about.google/)\n  * [ Privacy ](https://policies.google.com/privacy)\n  * [ Site terms ](https://policies.google.com/terms?hl=en)\n  * [ Google Cloud terms ](https://cloud.google.com/product-terms)\n  * [ Manage cookies ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)\n  * [ Our third decade of climate action: join us ](https://cloud.google.com/sustainability)\n  * Sign up for the Google Cloud newsletter [ Subscribe ](https://cloud.google.com/newsletter/)\n\n\n  * English\n  * Deutsch\n  * Español\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Português\n  * Português – Brasil\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n\n\n\n\n# Content from https://ai.google.dev/gemini-api/docs/function-calling\n\n[ Skip to main content ](https://ai.google.dev/gemini-api/docs/function-calling#main-content)\n[ ![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg) ](https://ai.google.dev/)\n`/`\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Polski\n  * Português – Brasil\n  * Shqip\n  * Tiếng Việt\n  * Türkçe\n  * Русский\n  * עברית\n  * العربيّة\n  * فارسی\n  * हिंदी\n  * বাংলা\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n[ Get API key ](https://aistudio.google.com/apikey) [ Cookbook ](https://github.com/google-gemini/cookbook) [ Community ](https://discuss.ai.google.dev/c/gemini-api/) Sign in\n[ Gemini API Docs ](https://ai.google.dev/gemini-api/docs) [ API reference ](https://ai.google.dev/api)\n[ ![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg) ](https://ai.google.dev/)\n  * [ Gemini API  ](https://ai.google.dev/gemini-api/docs)\n    * [ Gemini API Docs  ](https://ai.google.dev/gemini-api/docs)\n    * [ API reference  ](https://ai.google.dev/api)\n  * [ Get API key  ](https://aistudio.google.com/apikey)\n  * [ Cookbook  ](https://github.com/google-gemini/cookbook)\n  * [ Community  ](https://discuss.ai.google.dev/c/gemini-api/)\n\n\n  * Get started\n  * [Overview](https://ai.google.dev/gemini-api/docs)\n  * [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n  * [API keys](https://ai.google.dev/gemini-api/docs/api-key)\n  * [Vibe code in Build mode](https://ai.google.dev/gemini-api/docs/aistudio-build-mode)\n  * [Libraries](https://ai.google.dev/gemini-api/docs/libraries)\n  * [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)\n  * Models\n  * [Gemini](https://ai.google.dev/gemini-api/docs/models)\n  * [Gemini 3](https://ai.google.dev/gemini-api/docs/gemini-3)\n  * [Imagen (image generation)](https://ai.google.dev/gemini-api/docs/imagen)\n  * [Veo (video generation)](https://ai.google.dev/gemini-api/docs/video)\n  * [Lyria (music generation)](https://ai.google.dev/gemini-api/docs/music-generation)\n  * [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)\n  * [Robotics](https://ai.google.dev/gemini-api/docs/robotics-overview)\n  * [Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n  * [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n  * Core capabilities\n  * [Text](https://ai.google.dev/gemini-api/docs/text-generation)\n  * Image\n    * [Image generation 🍌](https://ai.google.dev/gemini-api/docs/image-generation)\n    * [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)\n  * [Video](https://ai.google.dev/gemini-api/docs/video-understanding)\n  * [Documents](https://ai.google.dev/gemini-api/docs/document-processing)\n  * Speech and audio\n    * [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)\n    * [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)\n  * Thinking and thought signatures\n    * [Thinking](https://ai.google.dev/gemini-api/docs/thinking)\n    * [Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)\n  * [Structured outputs](https://ai.google.dev/gemini-api/docs/structured-output)\n  * [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)\n  * [Long context](https://ai.google.dev/gemini-api/docs/long-context)\n  * Tools\n  * [Overview](https://ai.google.dev/gemini-api/docs/tools)\n  * [Google Search](https://ai.google.dev/gemini-api/docs/google-search)\n  * [Google Maps](https://ai.google.dev/gemini-api/docs/maps-grounding)\n  * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)\n  * [URL context](https://ai.google.dev/gemini-api/docs/url-context)\n  * [Computer Use](https://ai.google.dev/gemini-api/docs/computer-use)\n  * [File Search](https://ai.google.dev/gemini-api/docs/file-search)\n  * Live API\n  * [Get started](https://ai.google.dev/gemini-api/docs/live)\n  * [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)\n  * [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)\n  * [Session management](https://ai.google.dev/gemini-api/docs/live-session)\n  * [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)\n  * Guides\n  * [Batch API](https://ai.google.dev/gemini-api/docs/batch-api)\n  * [Files API](https://ai.google.dev/gemini-api/docs/files)\n  * [Context caching](https://ai.google.dev/gemini-api/docs/caching)\n  * [Media resolution](https://ai.google.dev/gemini-api/docs/media-resolution)\n  * [Token counting](https://ai.google.dev/gemini-api/docs/tokens)\n  * [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n  * Logs and datasets\n    * [Get started with logs](https://ai.google.dev/gemini-api/docs/logs-datasets)\n    * [Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)\n  * Safety\n    * [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)\n    * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)\n  * Open-Source frameworks\n    * [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)\n    * [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)\n    * [LlamaIndex](https://ai.google.dev/gemini-api/docs/llama-index)\n    * [Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)\n  * Resources\n  * [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)\n  * [Release notes](https://ai.google.dev/gemini-api/docs/changelog)\n  * [Deprecations](https://ai.google.dev/gemini-api/docs/deprecations)\n  * [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)\n  * [Billing info](https://ai.google.dev/gemini-api/docs/billing)\n  * Google AI Studio\n    * [Google AI Studio quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)\n    * [LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)\n    * [AI Studio troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)\n    * [Google Workspace](https://ai.google.dev/gemini-api/docs/workspace)\n  * Google Cloud Platform\n    * [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)\n    * [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)\n  * Policies\n  * [Terms of service](https://ai.google.dev/gemini-api/terms)\n  * [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)\n  * [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)\n\n\nGemini 3 is here. Read the [developer guide](https://ai.google.dev/gemini-api/docs/gemini-3) to get started with our most advanced model yet. \n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Gemini API Docs ](https://ai.google.dev/gemini-api/docs)\n\n\nSend feedback \n#  Function calling with the Gemini API\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model determines when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.\n\n\nGet Weather Schedule Meeting Create Chart\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Define the function declaration for the model\nschedule_meeting_function = {\n    \"name\": \"schedule_meeting\",\n    \"description\": \"Schedules a meeting with specified attendees at a given time and date.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"attendees\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"List of people attending the meeting.\",\n            },\n            \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date of the meeting (e.g., '2024-07-29')\",\n            },\n            \"time\": {\n                \"type\": \"string\",\n                \"description\": \"Time of the meeting (e.g., '15:00')\",\n            },\n            \"topic\": {\n                \"type\": \"string\",\n                \"description\": \"The subject or topic of the meeting.\",\n            },\n        },\n        \"required\": [\"attendees\", \"date\", \"time\", \"topic\"],\n    },\n}\n\n# Configure the client and tools\nclient = genai.Client()\ntools = types.Tool(function_declarations=[schedule_meeting_function])\nconfig = types.GenerateContentConfig(tools=[tools])\n\n# Send request with function declarations\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning.\",\n    config=config,\n)\n\n# Check for a function call\nif response.candidates[0].content.parts[0].function_call:\n    function_call = response.candidates[0].content.parts[0].function_call\n    print(f\"Function to call: {function_call.name}\")\n    print(f\"Arguments: {function_call.args}\")\n    #  In a real app, you would call your function here:\n    #  result = schedule_meeting(**function_call.args)\nelse:\n    print(\"No function call found in the response.\")\n    print(response.text)\n\n```\n\n### JavaScript\n```\nimport{GoogleGenAI,Type}from'@google/genai';\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Define the function declaration for the model\nconstscheduleMeetingFunctionDeclaration={\nname:'schedule_meeting',\ndescription:'Schedules a meeting with specified attendees at a given time and date.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nattendees:{\ntype:Type.ARRAY,\nitems:{type:Type.STRING},\ndescription:'List of people attending the meeting.',\n},\ndate:{\ntype:Type.STRING,\ndescription:'Date of the meeting (e.g., \"2024-07-29\")',\n},\ntime:{\ntype:Type.STRING,\ndescription:'Time of the meeting (e.g., \"15:00\")',\n},\ntopic:{\ntype:Type.STRING,\ndescription:'The subject or topic of the meeting.',\n},\n},\nrequired:['attendees','date','time','topic'],\n},\n};\n\n// Send request with function declarations\nconstresponse=awaitai.models.generateContent({\nmodel:'gemini-2.5-flash',\ncontents:'Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.',\nconfig:{\ntools:[{\nfunctionDeclarations:[scheduleMeetingFunctionDeclaration]\n}],\n},\n});\n\n// Check for function calls in the response\nif(response.functionCalls && response.functionCalls.length > 0){\nconstfunctionCall=response.functionCalls[0];// Assuming one function call\nconsole.log(`Function to call: ${functionCall.name}`);\nconsole.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n// In a real app, you would call your actual function here:\n// const result = await scheduleMeeting(functionCall.args);\n}else{\nconsole.log(\"No function call found in the response.\");\nconsole.log(response.text);\n}\n\n```\n\n### REST\n```\ncurl\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\"\\\n\"x-goog-api-key: $GEMINI_API_KEY\"\\\n'Content-Type: application/json'\\\n\\\n'{\n    \"contents\": [\n      {\n        \"role\": \"user\",\n        \"parts\": [\n          {\n            \"text\": \"Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.\"\n          }\n        ]\n      }\n    ],\n    \"tools\": [\n      {\n        \"functionDeclarations\": [\n          {\n            \"name\": \"schedule_meeting\",\n            \"description\": \"Schedules a meeting with specified attendees at a given time and date.\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"attendees\": {\n                  \"type\": \"array\",\n                  \"items\": {\"type\": \"string\"},\n                  \"description\": \"List of people attending the meeting.\"\n                },\n                \"date\": {\n                  \"type\": \"string\",\n                  \"description\": \"Date of the meeting (e.g., '2024-07-29')\"\n                },\n                \"time\": {\n                  \"type\": \"string\",\n                  \"description\": \"Time of the meeting (e.g., '15:00')\"\n                },\n                \"topic\": {\n                  \"type\": \"string\",\n                  \"description\": \"The subject or topic of the meeting.\"\n                }\n              },\n              \"required\": [\"attendees\", \"date\", \"time\", \"topic\"]\n            }\n          }\n        ]\n      }\n    ]\n  }'\n\n```\n\n## How function calling works\n![function calling\noverview](https://ai.google.dev/static/gemini-api/docs/images/function-calling-overview.png)\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](https://ai.google.dev/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](https://ai.google.dev/gemini-api/docs/function-calling#compositional_function_calling)).\n### Step 1: Define a function declaration\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n### Python\n```\n# Define a function that the model can call to control smart lights\nset_light_values_declaration = {\n    \"name\": \"set_light_values\",\n    \"description\": \"Sets the brightness and color temperature of a light.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"brightness\": {\n                \"type\": \"integer\",\n                \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n            },\n            \"color_temp\": {\n                \"type\": \"string\",\n                \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n            },\n        },\n        \"required\": [\"brightness\", \"color_temp\"],\n    },\n}\n\n# This is the actual function that would be called based on the model's suggestion\ndefset_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n\"\"\"Set the brightness and color temperature of a room light. (mock API).\n\n    Args:\n        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n\n    Returns:\n        A dictionary containing the set brightness and color temperature.\n    \"\"\"\n    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n\n```\n\n### JavaScript\n```\nimport{Type}from'@google/genai';\n\n// Define a function that the model can call to control smart lights\nconstsetLightValuesFunctionDeclaration={\nname:'set_light_values',\ndescription:'Sets the brightness and color temperature of a light.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nbrightness:{\ntype:Type.NUMBER,\ndescription:'Light level from 0 to 100. Zero is off and 100 is full brightness',\n},\ncolor_temp:{\ntype:Type.STRING,\nenum:['daylight','cool','warm'],\ndescription:'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n},\n},\nrequired:['brightness','color_temp'],\n},\n};\n\n/**\n\n*   Set the brightness and color temperature of a room light. (mock API)\n*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n*   @return {Object} A dictionary containing the set brightness and color temperature.\n*/\nfunctionsetLightValues(brightness,color_temp){\nreturn{\nbrightness:brightness,\ncolorTemperature:color_temp\n};\n}\n\n```\n\n### Step 2: Call the model with function declarations\nOnce you have defined your function declarations, you can prompt the model to use them. It analyzes the prompt and function declarations and decides whether to respond directly or to call a function. If a function is called, the response object will contain a function call suggestion.\n### Python\n```\nfromgoogle.genaiimport types\n\n# Configure the client and tools\nclient = genai.Client()\ntools = types.Tool(function_declarations=[set_light_values_declaration])\nconfig = types.GenerateContentConfig(tools=[tools])\n\n# Define user prompt\ncontents = [\n    types.Content(\n        role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n    )\n]\n\n# Send request with function declarations\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=contents\n    config=config,\n)\n\nprint(response.candidates[0].content.parts[0].function_call)\n\n```\n\n### JavaScript\n```\nimport{GoogleGenAI}from'@google/genai';\n\n// Generation config with function declaration\nconstconfig={\ntools:[{\nfunctionDeclarations:[setLightValuesFunctionDeclaration]\n}]\n};\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Define user prompt\nconstcontents=[\n{\nrole:'user',\nparts:[{text:'Turn the lights down to a romantic level'}]\n}\n];\n\n// Send request with function declarations\nconstresponse=awaitai.models.generateContent({\nmodel:'gemini-2.5-flash',\ncontents:contents,\nconfig:config\n});\n\nconsole.log(response.functionCalls[0]);\n\n```\n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n### Python\n```\nid=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n\n```\n\n### JavaScript\n```\n{\nname:'set_light_values',\nargs:{brightness:25,color_temp:'warm'}\n}\n\n```\n\n### Step 3: Execute set_light_values function code\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function.\n### Python\n```\n# Extract tool call details, it may not be in the first part.\ntool_call = response.candidates[0].content.parts[0].function_call\n\nif tool_call.name == \"set_light_values\":\n    result = set_light_values(**tool_call.args)\n    print(f\"Function execution result: {result}\")\n\n```\n\n### JavaScript\n```\n// Extract tool call details\nconsttool_call=response.functionCalls[0]\n\nletresult;\nif(tool_call.name==='set_light_values'){\nresult=setLightValues(tool_call.args.brightness,tool_call.args.color_temp);\nconsole.log(`Function execution result: ${JSON.stringify(result)}`);\n}\n\n```\n\n### Step 4: Create user friendly response with function result and call the model again\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Create a function response part\nfunction_response_part = types.Part.from_function_response(\n    name=tool_call.name,\n    response={\"result\": result},\n)\n\n# Append function call and result of the function execution to contents\ncontents.append(response.candidates[0].content) # Append the content from the model's response.\ncontents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n\nclient = genai.Client()\nfinal_response = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    config=config,\n    contents=contents,\n)\n\nprint(final_response.text)\n\n```\n\n### JavaScript\n```\n// Create a function response part\nconstfunction_response_part={\nname:tool_call.name,\nresponse:{result}\n}\n\n// Append function call and result of the function execution to contents\ncontents.push(response.candidates[0].content);\ncontents.push({role:'user',parts:[{functionResponse:function_response_part}]});\n\n// Get the final response from the model\nconstfinal_response=awaitai.models.generateContent({\nmodel:'gemini-2.5-flash',\ncontents:contents,\nconfig:config\n});\n\nconsole.log(final_response.text);\n\n```\n\nThis completes the function calling flow. The model successfully used the `set_light_values` function to perform the request action of the user.\n## Function declarations\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more `function declarations`. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemaw) format. A single function declaration can include the following parameters:\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\nYou can also construct `FunctionDeclarations` from Python functions directly using `types.FunctionDeclaration.from_callable(client=client, callable=your_function)`.\n## Function calling with thinking models\nGemini 3 and 2.5 series models use an internal [\"thinking\"](https://ai.google.dev/gemini-api/docs/thinking) process to reason through requests. This significantly improves function calling performance, allowing the model to better determine when to call a function and which parameters to use. Because the Gemini API is stateless, models use [thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures) to maintain context across multi-turn conversations.\nThis section covers advanced management of thought signatures and is only necessary if you're manually constructing API requests (e.g., via REST) or manipulating conversation history.\n**If you're using the[Google GenAI SDKs](https://ai.google.dev/gemini-api/docs/libraries) (our official libraries), you don't need to manage this process**. The SDKs automatically handle the necessary steps, as shown in the earlier [example](https://ai.google.dev/gemini-api/docs/function-calling#step-4).\n### Managing conversation history manually\nIf you modify the conversation history manually, instead of sending the [complete previous response](https://ai.google.dev/gemini-api/docs/function-calling#step-4) you must correctly handle the `thought_signature` included in the model's turn.\nFollow these rules to ensure the model's context is preserved:\n  * Always send the `thought_signature` back to the model inside its original [`Part`](https://ai.google.dev/api#request-body-structure).\n  * Don't merge a `Part` containing a signature with one that does not. This breaks the positional context of the thought.\n  * Don't combine two `Parts` that both contain signatures, as the signature strings cannot be merged.\n\n\n#### Gemini 3 thought signatures\nIn Gemini 3, any [`Part`](https://ai.google.dev/api#request-body-structure) of a model response may contain a thought signature. While we generally recommend returning signatures from all `Part` types, passing back thought signatures is mandatory for function calling. Unless you are manipulating conversation history manually, the Google GenAI SDK will handle thought signatures automatically.\nIf you are manipulating conversation history manually, refer to the [Thoughts Signatures](https://ai.google.dev/gemini-api/docs/thought-signatures) page for complete guidance and details on handling thought signatures for Gemini 3.\n### Inspecting thought signatures\nWhile not necessary for implementation, you can inspect the response to see the `thought_signature` for debugging or educational purposes.\n### Python\n```\nimportbase64\n# After receiving a response from a model with thinking enabled\n# response = client.models.generate_content(...)\n\n# The signature is attached to the response part containing the function call\npart = response.candidates[0].content.parts[0]\nif part.thought_signature:\n  print(base64.b64encode(part.thought_signature).decode(\"utf-8\"))\n\n```\n\n### JavaScript\n```\n// After receiving a response from a model with thinking enabled\n// const response = await ai.models.generateContent(...)\n\n// The signature is attached to the response part containing the function call\nconstpart=response.candidates[0].content.parts[0];\nif(part.thoughtSignature){\nconsole.log(part.thoughtSignature);\n}\n\n```\n\nLearn more about limitations and usage of thought signatures, and about thinking models in general, on the [Thinking](https://ai.google.dev/gemini-api/docs/thinking#signatures) page.\n## Parallel function calling\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n### Python\n```\npower_disco_ball = {\n    \"name\": \"power_disco_ball\",\n    \"description\": \"Powers the spinning disco ball.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"power\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether to turn the disco ball on or off.\",\n            }\n        },\n        \"required\": [\"power\"],\n    },\n}\n\nstart_music = {\n    \"name\": \"start_music\",\n    \"description\": \"Play some music matching the specified parameters.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"energetic\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether the music is energetic or not.\",\n            },\n            \"loud\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether the music is loud or not.\",\n            },\n        },\n        \"required\": [\"energetic\", \"loud\"],\n    },\n}\n\ndim_lights = {\n    \"name\": \"dim_lights\",\n    \"description\": \"Dim the lights.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"brightness\": {\n                \"type\": \"number\",\n                \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n            }\n        },\n        \"required\": [\"brightness\"],\n    },\n}\n\n```\n\n### JavaScript\n```\nimport{Type}from'@google/genai';\n\nconstpowerDiscoBall={\nname:'power_disco_ball',\ndescription:'Powers the spinning disco ball.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\npower:{\ntype:Type.BOOLEAN,\ndescription:'Whether to turn the disco ball on or off.'\n}\n},\nrequired:['power']\n}\n};\n\nconststartMusic={\nname:'start_music',\ndescription:'Play some music matching the specified parameters.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nenergetic:{\ntype:Type.BOOLEAN,\ndescription:'Whether the music is energetic or not.'\n},\nloud:{\ntype:Type.BOOLEAN,\ndescription:'Whether the music is loud or not.'\n}\n},\nrequired:['energetic','loud']\n}\n};\n\nconstdimLights={\nname:'dim_lights',\ndescription:'Dim the lights.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nbrightness:{\ntype:Type.NUMBER,\ndescription:'The brightness of the lights, 0.0 is off, 1.0 is full.'\n}\n},\nrequired:['brightness']\n}\n};\n\n```\n\nConfigure the function calling mode to allow using all of the specified tools. To learn more, you can read about [configuring function calling](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_modes).\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Configure the client and tools\nclient = genai.Client()\nhouse_tools = [\n    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n]\nconfig = types.GenerateContentConfig(\n    tools=house_tools,\n    automatic_function_calling=types.AutomaticFunctionCallingConfig(\n        disable=True\n    ),\n    # Force the model to call 'any' function, instead of chatting.\n    tool_config=types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(mode='ANY')\n    ),\n)\n\nchat = client.chats.create(model=\"gemini-2.5-flash\", config=config)\nresponse = chat.send_message(\"Turn this place into a party!\")\n\n# Print out each of the function calls requested from this single call\nprint(\"Example 1: Forced function calling\")\nfor fn in response.function_calls:\n    args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n    print(f\"{fn.name}({args})\")\n\n```\n\n### JavaScript\n```\nimport{GoogleGenAI}from'@google/genai';\n\n// Set up function declarations\nconsthouseFns=[powerDiscoBall,startMusic,dimLights];\n\nconstconfig={\ntools:[{\nfunctionDeclarations:houseFns\n}],\n// Force the model to call 'any' function, instead of chatting.\ntoolConfig:{\nfunctionCallingConfig:{\nmode:'any'\n}\n}\n};\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Create a chat session\nconstchat=ai.chats.create({\nmodel:'gemini-2.5-flash',\nconfig:config\n});\nconstresponse=awaitchat.sendMessage({message:'Turn this place into a party!'});\n\n// Print out each of the function calls requested from this single call\nconsole.log(\"Example 1: Forced function calling\");\nfor(constfnofresponse.functionCalls){\nconstargs=Object.entries(fn.args)\n.map(([key,val])=>`${key}=${val}`)\n.join(', ');\nconsole.log(`${fn.name}(${args})`);\n}\n\n```\n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.\nThe Python SDK supports [automatic function calling](https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only), which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Actual function implementations\ndefpower_disco_ball_impl(power: bool) -> dict:\n\"\"\"Powers the spinning disco ball.\n\n    Args:\n        power: Whether to turn the disco ball on or off.\n\n    Returns:\n        A status dictionary indicating the current state.\n    \"\"\"\n    return {\"status\": f\"Disco ball powered {'on'ifpowerelse'off'}\"}\n\ndefstart_music_impl(energetic: bool, loud: bool) -> dict:\n\"\"\"Play some music matching the specified parameters.\n\n    Args:\n        energetic: Whether the music is energetic or not.\n        loud: Whether the music is loud or not.\n\n    Returns:\n        A dictionary containing the music settings.\n    \"\"\"\n    music_type = \"energetic\" if energetic else \"chill\"\n    volume = \"loud\" if loud else \"quiet\"\n    return {\"music_type\": music_type, \"volume\": volume}\n\ndefdim_lights_impl(brightness: float) -> dict:\n\"\"\"Dim the lights.\n\n    Args:\n        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n\n    Returns:\n        A dictionary containing the new brightness setting.\n    \"\"\"\n    return {\"brightness\": brightness}\n\n# Configure the client\nclient = genai.Client()\nconfig = types.GenerateContentConfig(\n    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n)\n\n# Make the request\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Do everything you need to this place into party!\",\n    config=config,\n)\n\nprint(\"\\nExample 2: Automatic function calling\")\nprint(response.text)\n# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n\n```\n\n## Compositional function calling\nCompositional or sequential function calling allows Gemini to chain multiple function calls together to fulfill a complex request. For example, to answer \"Get the temperature in my current location\", the Gemini API might first invoke a `get_current_location()` function followed by a `get_weather()` function that takes the location as a parameter.\nThe following example demonstrates how to implement compositional function calling using the Python SDK and automatic function calling.\n### Python\nThis example uses the automatic function calling feature of the `google-genai` Python SDK. The SDK automatically converts the Python functions to the required schema, executes the function calls when requested by the model, and sends the results back to the model to complete the task.\n```\nimportos\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Example Functions\ndefget_weather_forecast(location: str) -> dict:\n\"\"\"Gets the current weather temperature for a given location.\"\"\"\n    print(f\"Tool Call: get_weather_forecast(location={location})\")\n    # TODO: Make API call\n    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n\ndefset_thermostat_temperature(temperature: int) -> dict:\n\"\"\"Sets the thermostat to a desired temperature.\"\"\"\n    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n    # TODO: Interact with a thermostat API\n    print(\"Tool Response: {'status': 'success'}\")\n    return {\"status\": \"success\"}\n\n# Configure the client and model\nclient = genai.Client()\nconfig = types.GenerateContentConfig(\n    tools=[get_weather_forecast, set_thermostat_temperature]\n)\n\n# Make the request\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n    config=config,\n)\n\n# Print the final, user-facing response\nprint(response.text)\n\n```\n\n**Expected Output**\nWhen you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.\n```\nTool Call: get_weather_forecast(location=London)\nTool Response: {'temperature': 25, 'unit': 'celsius'}\nTool Call: set_thermostat_temperature(temperature=20)\nTool Response: {'status': 'success'}\nOK. I've set the thermostat to 20°C.\n\n```\n\n### JavaScript\nThis example shows how to use JavaScript/TypeScript SDK to do comopositional function calling using a manual execution loop.\n```\nimport{GoogleGenAI,Type}from\"@google/genai\";\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Example Functions\nfunctionget_weather_forecast({location}){\nconsole.log(`Tool Call: get_weather_forecast(location=${location})`);\n// TODO: Make API call\nconsole.log(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\");\nreturn{temperature:25,unit:\"celsius\"};\n}\n\nfunctionset_thermostat_temperature({temperature}){\nconsole.log(\n`Tool Call: set_thermostat_temperature(temperature=${temperature})`,\n);\n// TODO: Make API call\nconsole.log(\"Tool Response: {'status': 'success'}\");\nreturn{status:\"success\"};\n}\n\nconsttoolFunctions={\nget_weather_forecast,\nset_thermostat_temperature,\n};\n\nconsttools=[\n{\nfunctionDeclarations:[\n{\nname:\"get_weather_forecast\",\ndescription:\n\"Gets the current weather temperature for a given location.\",\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nlocation:{\ntype:Type.STRING,\n},\n},\nrequired:[\"location\"],\n},\n},\n{\nname:\"set_thermostat_temperature\",\ndescription:\"Sets the thermostat to a desired temperature.\",\nparameters:{\ntype:Type.OBJECT,\nproperties:{\ntemperature:{\ntype:Type.NUMBER,\n},\n},\nrequired:[\"temperature\"],\n},\n},\n],\n},\n];\n\n// Prompt for the model\nletcontents=[\n{\nrole:\"user\",\nparts:[\n{\ntext:\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n},\n],\n},\n];\n\n// Loop until the model has no more function calls to make\nwhile(true){\nconstresult=awaitai.models.generateContent({\nmodel:\"gemini-2.5-flash\",\ncontents,\nconfig:{tools},\n});\n\nif(result.functionCalls && result.functionCalls.length > 0){\nconstfunctionCall=result.functionCalls[0];\n\nconst{name,args}=functionCall;\n\nif(!toolFunctions[name]){\nthrownewError(`Unknown function call: ${name}`);\n}\n\n// Call the function and get the response.\nconsttoolResponse=toolFunctions[name](args);\n\nconstfunctionResponsePart={\nname:functionCall.name,\nresponse:{\nresult:toolResponse,\n},\n};\n\n// Send the function response back to the model.\ncontents.push({\nrole:\"model\",\nparts:[\n{\nfunctionCall:functionCall,\n},\n],\n});\ncontents.push({\nrole:\"user\",\nparts:[\n{\nfunctionResponse:functionResponsePart,\n},\n],\n});\n}else{\n// No more function calls, break the loop.\nconsole.log(result.text);\nbreak;\n}\n}\n\n```\n\n**Expected Output**\nWhen you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.\n```\nToolCall:get_weather_forecast(location=London)\nToolResponse:{'temperature':25,'unit':'celsius'}\nToolCall:set_thermostat_temperature(temperature=20)\nToolResponse:{'status':'success'}\nOK.It's 25°C in London, so I'vesetthethermostatto20°C.\n\n```\n\nCompositional function calling is a native [Live API](https://ai.google.dev/gemini-api/docs/live) feature. This means Live API can handle the function calling similar to the Python SDK.\n### Python\n```\n# Light control schemas\nturn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\nturn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n\nprompt = \"\"\"\n  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n  \"\"\"\n\ntools = [\n    {'code_execution': {}},\n    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n]\n\nawait run(prompt, tools=tools, modality=\"AUDIO\")\n\n```\n\n### JavaScript\n```\n// Light control schemas\nconstturnOnTheLightsSchema={name:'turn_on_the_lights'};\nconstturnOffTheLightsSchema={name:'turn_off_the_lights'};\n\nconstprompt=`\n  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n`;\n\nconsttools=[\n{codeExecution:{}},\n{functionDeclarations:[turnOnTheLightsSchema,turnOffTheLightsSchema]}\n];\n\nawaitrun(prompt,tools=tools,modality=\"AUDIO\")\n\n```\n\n## Function calling modes\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.`function_calling_config`.\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantees function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n  * `VALIDATED` (Preview): The model is constrained to predict either function calls or natural language, and ensures function schema adherence. If `allowed_function_names` is not provided, the model picks from all of the available function declarations. If `allowed_function_names` is provided, the model picks from the set of allowed functions.\n\n\n### Python\n```\nfromgoogle.genaiimport types\n\n# Configure function calling mode\ntool_config = types.ToolConfig(\n    function_calling_config=types.FunctionCallingConfig(\n        mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n    )\n)\n\n# Create the generation config\nconfig = types.GenerateContentConfig(\n    tools=[tools],  # not defined here.\n    tool_config=tool_config,\n)\n\n```\n\n### JavaScript\n```\nimport{FunctionCallingConfigMode}from'@google/genai';\n\n// Configure function calling mode\nconsttoolConfig={\nfunctionCallingConfig:{\nmode:FunctionCallingConfigMode.ANY,\nallowedFunctionNames:['get_current_temperature']\n}\n};\n\n// Create the generation config\nconstconfig={\ntools:tools,// not defined here.\ntoolConfig:toolConfig,\n};\n\n```\n\n## Automatic function calling (Python only)\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK converts these functions into declarations, manages the function call execution, and handles the response cycle for you. Define your function with type hints and a docstring. For optimal results, it is recommended to use [Google-style docstrings.](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) The SDK will then automatically:\n  1. Detect function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Send the function's response back to the model.\n  4. Return the model's final text response.\n\n\nThe SDK currently does not parse argument descriptions into the property description slots of the generated function declaration. Instead, it sends the entire docstring as the top-level function description.\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Define the function with type hints and docstring\ndefget_current_temperature(location: str) -> dict:\n\"\"\"Gets the current temperature for a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n\n    Returns:\n        A dictionary containing the temperature and unit.\n    \"\"\"\n    # ... (implementation) ...\n    return {\"temperature\": 25, \"unit\": \"Celsius\"}\n\n# Configure the client\nclient = genai.Client()\nconfig = types.GenerateContentConfig(\n    tools=[get_current_temperature]\n)  # Pass the function itself\n\n# Make the request\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"What's the temperature in Boston?\",\n    config=config,\n)\n\nprint(response.text)  # The SDK handles the function call and returns the final text\n\n```\n\nYou can disable automatic function calling with:\n### Python\n```\nconfig = types.GenerateContentConfig(\n    tools=[get_current_temperature],\n    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n)\n\n```\n\n### Automatic function schema declaration\nThe API is able to describe any of the following types. `Pydantic` types are allowed, as long as the fields defined on them are also composed of allowed types. Dict types (like `dict[str: int]`) are not well supported here, don't use them.\n### Python\n```\nAllowedType = (\n  int | float | bool | str | list['AllowedType'] | pydantic.BaseModel)\n\n```\n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\ndefmultiply(a: float, b: float):\n\"\"\"Returns a * b.\"\"\"\n    return a * b\n\nclient = genai.Client()\nfn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n\n# to_json_dict() provides a clean JSON representation.\nprint(fn_decl.to_json_dict())\n\n```\n\n## Multi-tool use: Combine native tools with function calling\nYou can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](https://ai.google.dev/gemini-api/docs/grounding) and [code execution](https://ai.google.dev/gemini-api/docs/code-execution), in a request using the [Live API](https://ai.google.dev/gemini-api/docs/live).\n**Note:** Multi-tool use is a-[Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n### Python\n```\n# Multiple tasks example - combining lights, code execution, and search\nprompt = \"\"\"\n  Hey, I need you to do three things for me.\n\n    1.  Turn on the lights.\n    2.  Then compute the largest prime palindrome under 100000.\n    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n\n  Thanks!\n  \"\"\"\n\ntools = [\n    {'google_search': {}},\n    {'code_execution': {}},\n    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n]\n\n# Execute the prompt with specified tools in audio modality\nawait run(prompt, tools=tools, modality=\"AUDIO\")\n\n```\n\n### JavaScript\n```\n// Multiple tasks example - combining lights, code execution, and search\nconstprompt=`\n  Hey, I need you to do three things for me.\n\n    1.  Turn on the lights.\n    2.  Then compute the largest prime palindrome under 100000.\n    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n\n  Thanks!\n`;\n\nconsttools=[\n{googleSearch:{}},\n{codeExecution:{}},\n{functionDeclarations:[turnOnTheLightsSchema,turnOffTheLightsSchema]}// not defined here.\n];\n\n// Execute the prompt with specified tools in audio modality\nawaitrun(prompt,{tools:tools,modality:\"AUDIO\"});\n\n```\n\nPython developers can try this out in the [Live API Tool Use notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n## Model context protocol (MCP)\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard for connecting AI applications with external tools and data. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts.\nThe Gemini SDKs have built-in support for the MCP, reducing boilerplate code and offering [automatic tool calling](https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only) for MCP tools. When the model generates an MCP tool call, the Python and JavaScript client SDK can automatically execute the MCP tool and send the response back to the model in a subsequent request, continuing this loop until no more tool calls are made by the model.\nHere, you can find an example of how to use a local MCP server with Gemini and `mcp` SDK.\n### Python\nMake sure the latest version of the [`mcp` SDK](https://modelcontextprotocol.io/introduction) is installed on your platform of choice.\n```\npip\n```\n**Note:** Python supports automatic tool calling by passing in the `ClientSession` into the `tools` parameters. If you want to disable it, you can provide `automatic_function_calling` with disabled `True`.```\nimportos\nimportasyncio\nfromdatetimeimport datetime\nfrommcpimport ClientSession, StdioServerParameters\nfrommcp.client.stdioimport stdio_client\nfromgoogleimport genai\n\nclient = genai.Client()\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"npx\",  # Executable\n    args=[\"-y\", \"@philschmid/weather-mcp\"],  # MCP Server\n    env=None,  # Optional environment variables\n)\n\nasync defrun():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Prompt to get the weather for the current day in London.\n            prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n\n            # Initialize the connection between client and server\n            await session.initialize()\n\n            # Send request to the model with MCP function declarations\n            response = await client.aio.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt,\n                config=genai.types.GenerateContentConfig(\n                    temperature=0,\n                    tools=[session],  # uses the session, will automatically call the tool\n                    # Uncomment if you **don't** want the SDK to automatically call the tool\n                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(\n                    #     disable=True\n                    # ),\n                ),\n            )\n            print(response.text)\n\n# Start the asyncio event loop and run the main function\nasyncio.run(run())\n\n```\n\n### JavaScript\nMake sure the latest version of the `mcp` SDK is installed on your platform of choice.\n```\nnpm\n```\n**Note:** JavaScript supports automatic tool calling by wrapping the `client` with `mcpToTool`. If you want to disable it, you can provide `automaticFunctionCalling` with disabled `true`.```\nimport{GoogleGenAI,FunctionCallingConfigMode,mcpToTool}from'@google/genai';\nimport{Client}from\"@modelcontextprotocol/sdk/client/index.js\";\nimport{StdioClientTransport}from\"@modelcontextprotocol/sdk/client/stdio.js\";\n\n// Create server parameters for stdio connection\nconstserverParams=newStdioClientTransport({\ncommand:\"npx\",// Executable\nargs:[\"-y\",\"@philschmid/weather-mcp\"]// MCP Server\n});\n\nconstclient=newClient(\n{\nname:\"example-client\",\nversion:\"1.0.0\"\n}\n);\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Initialize the connection between client and server\nawaitclient.connect(serverParams);\n\n// Send request to the model with MCP tools\nconstresponse=awaitai.models.generateContent({\nmodel:\"gemini-2.5-flash\",\ncontents:`What is the weather in London in ${newDate().toLocaleDateString()}?`,\nconfig:{\ntools:[mcpToTool(client)],// uses the session, will automatically call the tool\n// Uncomment if you **don't** want the sdk to automatically call the tool\n// automaticFunctionCalling: {\n//   disable: true,\n// },\n},\n});\nconsole.log(response.text)\n\n// Close the connection\nawaitclient.close();\n\n```\n\n### Limitations with built-in MCP support\nBuilt-in MCP support is a [experimental](https://ai.google.dev/gemini-api/docs/models#preview) feature in our SDKs and has the following limitations:\n  * Only tools are supported, not resources nor prompts\n  * It is available for the Python and JavaScript/TypeScript SDK.\n  * Breaking changes might occur in future releases.\n\n\nManual integration of MCP servers is always an option if these limit what you're building.\n## Supported models\nThis section lists models and their function calling capabilities. Experimental models are not included. You can find a comprehensive capabilities overview on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling  \n---|---|---|---  \nGemini 2.5 Pro | ✔️ | ✔️ | ✔️  \nGemini 2.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.5 Flash-Lite | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \n## Best practices\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n    * See [Agentic workflows](https://ai.google.dev/gemini-api/docs/prompting-strategies#agentic-workflows) for further strategies on designing these prompts. Here is an example of a tested [system instruction](https://ai.google.dev/gemini-api/docs/prompting-strategies#agentic-si-template).\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\nWhen using Gemini 3 models, we strongly recommend keeping the `temperature` at its default value of 1.0. Changing the temperature (setting it below 1.0) may lead to unexpected behavior, such as looping or degraded performance, particularly in complex mathematical or reasoning tasks. \n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Check Finish Reason:** Always check the [`finishReason`](https://ai.google.dev/api/generate-content#FinishReason) in the model's response to handle cases where the model failed to generate a valid function call.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n## Notes and limitations\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\nSend feedback \nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-24 UTC.\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-11-24 UTC.\"],[],[]] \n  * [ Terms ](https://policies.google.com/terms)\n  * [ Privacy ](https://policies.google.com/privacy)\n  * [ Manage cookies ](https://ai.google.dev/gemini-api/docs/function-calling)\n\n\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Polski\n  * Português – Brasil\n  * Shqip\n  * Tiếng Việt\n  * Türkçe\n  * Русский\n  * עברית\n  * العربيّة\n  * فارسی\n  * हिंदी\n  * বাংলা\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n\n\n\n\n# Content from https://github.com/google/adk-python/issues/969\n\n[Skip to content](https://github.com/google/adk-python/issues/969#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues%2F969)\nAppearance settings\n  * Platform \n    * [ GitHub Copilot  Write better code with AI  ](https://github.com/features/copilot)\n    * [ GitHub Spark  New  Build and deploy intelligent apps  ](https://github.com/features/spark)\n    * [ GitHub Models  New  Manage and compare prompts  ](https://github.com/features/models)\n    * [ GitHub Advanced Security  Find and fix vulnerabilities  ](https://github.com/security/advanced-security)\n    * [ Actions  Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces  Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues  Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review  Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions  Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search  Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ Why GitHub ](https://github.com/why-github)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\nIntegrations\n    * [ GitHub Marketplace ](https://github.com/marketplace)\n    * [ MCP Registry ](https://github.com/mcp)\n[ View all features ](https://github.com/features)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ App Modernization ](https://github.com/solutions/use-case/app-modernization)\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles?topic=ai)\n    * [ DevOps ](https://github.com/resources/articles?topic=devops)\n    * [ Security ](https://github.com/resources/articles?topic=security)\n    * [ Software Development ](https://github.com/resources/articles?topic=software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://github.com/resources/events)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://github.com/partners)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors  Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project  GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform  AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ GitHub Advanced Security  Enterprise-grade security features  ](https://github.com/security/advanced-security)\n    * [ Copilot for business  Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support  Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues%2F969)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=google%2Fadk-python)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/google/adk-python/issues/969) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/google/adk-python/issues/969) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/google/adk-python/issues/969) to refresh your session. Dismiss alert\n{{ message }}\n[ google ](https://github.com/google) / **[adk-python](https://github.com/google/adk-python) ** Public\n  * [ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ 2.5k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n  * [ Star  15.7k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n\n\n  * [ Code ](https://github.com/google/adk-python)\n  * [ Issues 350 ](https://github.com/google/adk-python/issues)\n  * [ Pull requests 101 ](https://github.com/google/adk-python/pulls)\n  * [ Discussions ](https://github.com/google/adk-python/discussions)\n  * [ Actions ](https://github.com/google/adk-python/actions)\n  * [ Projects 0 ](https://github.com/google/adk-python/projects)\n  * [ Security ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n### [ Uh oh!  ](https://github.com/google/adk-python/security)\n[There was an error while loading. ](https://github.com/google/adk-python/security)[Please reload this page](https://github.com/google/adk-python/issues/969).\n  * [ Insights ](https://github.com/google/adk-python/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/google/adk-python)\n  * [ Issues  ](https://github.com/google/adk-python/issues)\n  * [ Pull requests  ](https://github.com/google/adk-python/pulls)\n  * [ Discussions  ](https://github.com/google/adk-python/discussions)\n  * [ Actions  ](https://github.com/google/adk-python/actions)\n  * [ Projects  ](https://github.com/google/adk-python/projects)\n  * [ Security  ](https://github.com/google/adk-python/security)\n  * [ Insights  ](https://github.com/google/adk-python/pulse)\n\n\n#  Master Issue: built-in tools cannot co-exist with FunctionDeclaration tools. #969\n[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues/969)\nCopy link\n[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues/969)\nCopy link\n[Feature](https://github.com/google/adk-python/issues?q=type:\"Feature\")\n1 / 11 of 1 issue completed\n[Master Issue: built-in tools cannot co-exist with FunctionDeclaration tools.](https://github.com/google/adk-python/issues/969#top)#969\nFeature\n1 / 11 of 1 issue completed\nCopy link\nAssignees\n[![Jacksunwei](https://avatars.githubusercontent.com/u/1281348?s=64&u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4)](https://github.com/Jacksunwei)\nLabels\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22core%22)[Component] This issue is related to the core interface and implementation\nMilestone\n[Feature Roadmap](https://github.com/google/adk-python/milestone/3)\n[![@Jacksunwei](https://avatars.githubusercontent.com/u/1281348?u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4&size=80)](https://github.com/Jacksunwei)\n## Description\n[![@Jacksunwei](https://avatars.githubusercontent.com/u/1281348?u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4&size=48)](https://github.com/Jacksunwei)\n[Jacksunwei](https://github.com/Jacksunwei)\nopened [on May 27, 2025](https://github.com/google/adk-python/issues/969#issue-3094720871) · edited by [Jacksunwei](https://github.com/Jacksunwei)\nEdits\nCollaborator\nIssue body actions\nRight now, the Gemini server is limited to only one type of tools among the following list:\n  * Function declaration (mostly used for python function `FunctionTool` and other subclass of `BaseTool`)\n  * Google Search (Gemini model built-in tool)\n  * Vertex AI Search (Gemini model built-in tool)\n  * BuiltInCodeExecutor (Gemini model built-in tool)\n  * ...\n\n\nFull list of built-in tools are at <https://github.com/googleapis/python-genai/blob/main/google/genai/types.py#L2692>\n**If your agent is using more than one type of tools, currently it is not supported**.\n* * *\nWorkaround options:\n  1. Use a sub-agent that only carries the built-in tool\n  2. Use AgentTool that only carries the built-in tool and store the response to state for other agents to use.\n\n\n👍React with 👍16sqave, wmetaw, sm-mbrt, pedro-fraczek-tc, eikoshelev and 11 more\n## Sub-issues\n###  Collapse Sub-issuesSub-issues\n1 of 11 of 1 Issues completed\n  * [Feature: ](https://github.com/google/adk-python/issues?q=type%3AFeature)[Support for built-in tools within sub-agents #413](https://github.com/google/adk-python/issues/413)\n[![seanzhou1023](https://avatars.githubusercontent.com/u/4022557?u=9e6829960c346e8651b5d2918cf695e99a8f63fb&v=4&size=40)](https://github.com/google/adk-python/issues?q=assignee%3Aseanzhou1023)[![klateefa](https://avatars.githubusercontent.com/u/227978367?v=4&size=40)](https://github.com/google/adk-python/issues?q=assignee%3Aklateefa)\nManage this item ` control⌃ shift⇧ uU`\n\n\nTo pick up a draggable item, press the space bar. While dragging, use the arrow keys to move the item. Press space again to drop the item in its new position, or press escape to cancel. \n## Activity\n[![](https://avatars.githubusercontent.com/u/1281348?s=64&u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4)Jacksunwei](https://github.com/Jacksunwei)\nmentioned this [on May 27, 2025](https://github.com/google/adk-python/issues/969#event-2527002672)\n  * [ADK: Root agent with sub_agents fails if sub-agents use a mix of VertexAiSearchTool and custom function tools #899](https://github.com/google/adk-python/issues/899)\n\n\n[![](https://avatars.githubusercontent.com/u/1281348?s=64&u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4)Jacksunwei](https://github.com/Jacksunwei)\nadded \n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22core%22)[Component] This issue is related to the core interface and implementation\n[on May 27, 2025](https://github.com/google/adk-python/issues/969#event-17835519982)\n[![](https://avatars.githubusercontent.com/u/1281348?s=64&u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4)Jacksunwei](https://github.com/Jacksunwei)\nadded the\n[Feature](https://github.com/google/adk-python/issues?q=type:\"Feature\")\nissue type [on May 27, 2025](https://github.com/google/adk-python/issues/969#event-17835520660)\n[![sqave](https://avatars.githubusercontent.com/u/106431046?u=ca3f5a590434f4d990c32b13ddfaa74bdabaa86e&v=4&size=80)](https://github.com/sqave)\n### sqave commented on May 29, 2025 \n[![@sqave](https://avatars.githubusercontent.com/u/106431046?u=ca3f5a590434f4d990c32b13ddfaa74bdabaa86e&v=4&size=48)](https://github.com/sqave)\n[sqave](https://github.com/sqave)\n[on May 29, 2025](https://github.com/google/adk-python/issues/969#issuecomment-2917726249)\nMore actions\nCould you please clarify the first workaround? I’ve tried to attach a dedicated sub-agent with only this built-in tool to the root agent, but the same error still persists.\n```\nsearch_agent = Agent(\n    model='gemini-2.0-flash',\n    name='search_agent',\n    description=\"(...)\",\n    instruction=\"(...)\"\n    tools=[google_search],\n)\n\nroot_agent = Agent(\n    name=\"root_agent\",\n    model=\"gemini-2.0-flash\",\n    description=\"(...)\",\n    instruction=\"(...)\"\n    sub_agents=[weather_agent, search_agent]\n\n```\n\n[![Eluticz](https://avatars.githubusercontent.com/u/197428481?v=4&size=80)](https://github.com/Eluticz)\n### Eluticz commented on May 29, 2025 \n[![@Eluticz](https://avatars.githubusercontent.com/u/197428481?v=4&size=48)](https://github.com/Eluticz)\n[Eluticz](https://github.com/Eluticz)\n[on May 29, 2025](https://github.com/google/adk-python/issues/969#issuecomment-2919016724)\nMore actions\nSame issue fix this\n👍React with 👍2wmetaw and agolebiowska\n[![ramesh8v](https://avatars.githubusercontent.com/u/12216865?v=4&size=80)](https://github.com/ramesh8v)\n### ramesh8v commented on May 29, 2025 \n[![@ramesh8v](https://avatars.githubusercontent.com/u/12216865?v=4&size=48)](https://github.com/ramesh8v)\n[ramesh8v](https://github.com/ramesh8v)\n[on May 29, 2025](https://github.com/google/adk-python/issues/969#issuecomment-2920013644)\nMore actions\nRight now, it looks like there are no workarounds. I'd appreciate it if you get it fixed ASAP.\n👍React with 👍3wmetaw, eikoshelev and agolebiowska\n[![](https://avatars.githubusercontent.com/u/5482153?s=64&u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4)hangfei](https://github.com/hangfei)\nadded this to the [Feature Roadmap](https://github.com/google/adk-python/milestone/3) milestone [on May 31, 2025](https://github.com/google/adk-python/issues/969#event-17892100235)\n[![](https://avatars.githubusercontent.com/u/5482153?s=64&u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4)hangfei](https://github.com/hangfei)\nassigned \n[Jacksunwei](https://github.com/Jacksunwei)\n[on May 31, 2025](https://github.com/google/adk-python/issues/969#event-17892108585)\n[![](https://avatars.githubusercontent.com/u/5482153?s=64&u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4)hangfei](https://github.com/hangfei)\nadded a sub-issue [on May 31, 2025](https://github.com/google/adk-python/issues/969#event-17903240769)\n  * [Support for built-in tools within sub-agents #413](https://github.com/google/adk-python/issues/413)\n\n\n[![](https://avatars.githubusercontent.com/u/5482153?s=64&u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4)hangfei](https://github.com/hangfei)\nmentioned this [on May 31, 2025](https://github.com/google/adk-python/issues/969#event-2547938576)\n  * [Support for built-in tools within sub-agents #413](https://github.com/google/adk-python/issues/413)\n\n\n[![cerebro96](https://avatars.githubusercontent.com/u/74527971?u=c02e3ce4ec81f51a78752cf08f9c7c8ed8090cf5&v=4&size=80)](https://github.com/cerebro96)\n### cerebro96 commented on Jun 7, 2025 \n[![@cerebro96](https://avatars.githubusercontent.com/u/74527971?u=c02e3ce4ec81f51a78752cf08f9c7c8ed8090cf5&v=4&size=48)](https://github.com/cerebro96)\n[cerebro96](https://github.com/cerebro96)\n[on Jun 7, 2025](https://github.com/google/adk-python/issues/969#issuecomment-2951701550) · edited by [cerebro96](https://github.com/cerebro96)\nEdits\nMore actions\nDo you have any updates regarding this? I am encountering the following error: fast_api.py:817 - Error in event_generator: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}} [@hangfei](https://github.com/hangfei)\n👍React with 👍4sm-mbrt, MR-GREEN1337, makkopai and jerrytohvan\n[![gbmerrall](https://avatars.githubusercontent.com/u/133076218?v=4&size=80)](https://github.com/gbmerrall)\n### gbmerrall commented on Jun 19, 2025 \n[![@gbmerrall](https://avatars.githubusercontent.com/u/133076218?v=4&size=48)](https://github.com/gbmerrall)\n[gbmerrall](https://github.com/gbmerrall)\n[on Jun 19, 2025](https://github.com/google/adk-python/issues/969#issuecomment-2986149575)\nMore actions\nI had this working. Hope it helps\n```\nsearch_agent = LlmAgent(\n    name=\"basic_search_agent\",\n    model=\"gemini-2.0-flash\",\n    description=\"Agent to answer questions using Google Search.\",\n    instruction=\"I can answer your questions by searching the internet. Just ask me anything!\",\n    tools=[google_search],\n)\n\ndoc_qa_agent = LlmAgent(\n    name=\"doc_qa_agent\",\n    model=\"gemini-2.0-flash\", # Requires Gemini model\n    tools=[vertex_search_tool],\n    instruction=f\"\"\"You are a helpful assistant that answers questions based on information found in the document store: {YOUR_DATASTORE_ID}.\n    Use the search tool to find relevant information before answering.\n    If the answer isn't in the documents, say that you couldn't find the information.\n    \"\"\",\n    description=\"Answers questions using a specific Vertex AI Search datastore.\",\n)\n\ncode_agent = LlmAgent(\n    name=\"calculator_agent\",\n    model=\"gemini-2.0-flash\",\n    code_executor=BuiltInCodeExecutor(),\n    instruction=\"\"\"You are a calculator agent.\n    When given a mathematical expression, write and execute Python code to calculate the result.\n    Return only the final numerical result as plain text, without markdown or code blocks.\n    \"\"\",\n    description=\"Executes Python code to perform calculations.\",\n)\n\n# Wrap the specialized agents as tools\nsearch_tool = AgentTool(agent=search_agent)\ndoc_qa_tool = AgentTool(agent=doc_qa_agent)\ncode_tool = AgentTool(agent=code_agent)\n\n# Create the root agent with the search agent\nroot_agent = LlmAgent(\n    name=\"root_agent\",\n    model=\"gemini-2.0-flash\",\n    tools=[search_tool, doc_qa_tool, code_tool],\n    instruction=\"\"\"You are a helpful assistant that can answer questions and perform calculations.\n    You have 2 specialized agents to choose from:\n    - basic_search_agent: to search the internet\n    - doc_qa_agent: to answer questions based on the document store\n    - calculator_agent: to perform calculations\n    The user will provide a question which defines where the answer should come from, \n    and you should delegate to the appropriate agent to find the answer.\n    \"\"\",\n    description=\"A helpful assistant that can answer questions and perform calculations.\"\n)\n```\n\n👍React with 👍3pszymacha, AbdullahKhanASI and tinazhang-illuma\n[![chsmendes](https://avatars.githubusercontent.com/u/184537416?u=5aa85be41a9eb49ea47c2f976c97af6671a21dad&v=4&size=80)](https://github.com/chsmendes)\n### chsmendes commented on Jul 18, 2025 \n[![@chsmendes](https://avatars.githubusercontent.com/u/184537416?u=5aa85be41a9eb49ea47c2f976c97af6671a21dad&v=4&size=48)](https://github.com/chsmendes)\n[chsmendes](https://github.com/chsmendes)\n[on Jul 18, 2025](https://github.com/google/adk-python/issues/969#issuecomment-3089770404)\nMore actions\nI'm looking to integrate data retrieval from Vertex AI Search into an agent's workflow. Specifically, I need to fetch information using the Vertex AI Search tool and then update the agent's state based on the retrieved data. What's the recommended approach to achieve this?  \nWhen I create an agent and try to use a tool for `vertex_ai_search` and other tool only to `save data in state`, I got the `INVALID_ARGUMENT` error also, as mentioned above.\n👍React with 👍1agolebiowska\n[![iret-hesai](https://avatars.githubusercontent.com/u/126760389?u=2602ee3f9314dcd445741c50db9dcd33ed7b843d&v=4&size=80)](https://github.com/iret-hesai)\n### iret-hesai commented on Aug 5, 2025 \n[![@iret-hesai](https://avatars.githubusercontent.com/u/126760389?u=2602ee3f9314dcd445741c50db9dcd33ed7b843d&v=4&size=48)](https://github.com/iret-hesai)\n[iret-hesai](https://github.com/iret-hesai)\n[on Aug 5, 2025](https://github.com/google/adk-python/issues/969#issuecomment-3153266537) · edited by [iret-hesai](https://github.com/iret-hesai)\nEdits\nMore actions\nThanks to [@gbmerrall](https://github.com/gbmerrall) idea [#969 (comment)](https://github.com/google/adk-python/issues/969#issuecomment-2986149575)  \nuse search tool wrap up by a agent and a agent tool work.  \nbut this have a little problem that AgentTools only return the text content of the agent response generated by llm.\nIf u need the grounding info like referred webpage or data source ,  \ni tried below by override the AgentTool's run_async method and this can bypass the reference as a part o f the agent_tool response and it work.\n```\nclass CustomAgentToolForGoogleSearch(AgentTool):\n    @override\n    async def run_async(\n        self,\n        *,\n        args: dict[str, Any],\n        tool_context: ToolContext,\n    ) -> Any:\n        from google.adk.agents.llm_agent import LlmAgent\n\n        if self.skip_summarization:\n            tool_context.actions.skip_summarization = True\n\n        if isinstance(self.agent, LlmAgent) and self.agent.input_schema:\n            input_value = self.agent.input_schema.model_validate(args)\n            content = types.Content(\n                role='user',\n                parts=[\n                    types.Part.from_text(\n                        text=input_value.model_dump_json(exclude_none=True)\n                    )\n                ],\n            )\n        else:\n            content = types.Content(\n                role='user',\n                parts=[types.Part.from_text(text=args['request'])],\n            )\n\n        # Use the tool_context directly, bypassing the agent's tools' context\n        runner = Runner(\n            app_name=self.agent.name,\n            agent=self.agent,\n            artifact_service=ForwardingArtifactService(tool_context),\n            session_service=InMemorySessionService(),\n            memory_service=InMemoryMemoryService(),\n        )\n\n        session = await runner.session_service.create_session(\n            app_name=self.agent.name,\n            user_id='tmp_user',\n            state=tool_context.state.to_dict(),\n        )\n\n        last_event = None\n        async for event in runner.run_async(\n            user_id=session.user_id, session_id=session.id, new_message=content\n        ):\n            # Forward state delta to parent session.\n            if event.actions.state_delta:\n                tool_context.state.update(event.actions.state_delta)\n            last_event = event\n\n        return last_event\n\n        # Agent_Tool original return value as below commented out\n\n        # if not last_event or not last_event.content or not last_event.content.parts:\n        #     return ''\n        # merged_text = '\\n'.join(p.text for p in last_event.content.parts if p.text)\n        # if isinstance(self.agent, LlmAgent) and self.agent.output_schema:\n        #     tool_result = self.agent.output_schema.model_validate_json(\n        #     merged_text\n        # ).model_dump(exclude_none=True)\n        # else:\n        #     tool_result = merged_text\n        # return tool_result\n\ngoogle_search_agent = LlmAgent(\n    name=\"google_search_agent_tool\",\n    model=\"gemini-2.5-flash\",\n    instruction=\"I can answer your questions by searching the internet. Just ask me anything!\",\n    tools=[google_search],\n)\n\ngoogle_search_agent_tool = CustomAgentToolForGoogleSearch(google_search_agent)\n\n```\n\n[Copilot](https://github.com/apps/copilot-swe-agent)\nmentioned this [on Aug 12, 2025](https://github.com/google/adk-python/issues/969#event-2986654467)\n  * [Document tool-runner agent pattern for mixing RAG with custom tools #2491](https://github.com/google/adk-python/pull/2491)\n\n\n###  7 remaining items\nLoad more\nLoading\n[Sign up for free](https://github.com/signup?return_to=https://github.com/google/adk-python/issues/969)**to join this conversation on GitHub.** Already have an account? [Sign in to comment](https://github.com/login?return_to=https://github.com/google/adk-python/issues/969)\n## Metadata\n## Metadata\n### Assignees\n  * [![@Jacksunwei](https://avatars.githubusercontent.com/u/1281348?s=64&u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4)Jacksunwei](https://github.com/Jacksunwei)\n\n\n### Labels\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22core%22)[Component] This issue is related to the core interface and implementation\n### Type\n[Feature](https://github.com/google/adk-python/issues?q=type:\"Feature\")\n### Projects\nNo projects\n### Milestone\n  * [ Feature RoadmapNo due date ](https://github.com/google/adk-python/milestone/3)\n\n\n### Relationships\nNone yet\n### Development\nCode with agent mode\nSelect code repository\nNo branches or pull requests\n### Participants\n[![@palladius](https://avatars.githubusercontent.com/u/132802?s=64&u=1281d162f13248a277d630fd8c4a8e663b2231f3&v=4)](https://github.com/palladius)[![@Jacksunwei](https://avatars.githubusercontent.com/u/1281348?s=64&u=7b19b6ad8744205efd69519e6f340661efdde8b7&v=4)](https://github.com/Jacksunwei)[![@harendra](https://avatars.githubusercontent.com/u/1376275?s=64&v=4)](https://github.com/harendra)[![@hangfei](https://avatars.githubusercontent.com/u/5482153?s=64&u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4)](https://github.com/hangfei)[![@ramesh8v](https://avatars.githubusercontent.com/u/12216865?s=64&v=4)](https://github.com/ramesh8v)\n+8\n## Issue actions\n## Footer\n[ ](https://github.com) © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Community](https://github.community/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nMaster Issue: built-in tools cannot co-exist with FunctionDeclaration tools. · Issue #969 · google/adk-python\n\n\n\n# Content from https://github.com/google/adk-python/issues/134\n\n[Skip to content](https://github.com/google/adk-python/issues/134#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues%2F134)\nAppearance settings\n  * Platform \n    * [ GitHub Copilot  Write better code with AI  ](https://github.com/features/copilot)\n    * [ GitHub Spark  New  Build and deploy intelligent apps  ](https://github.com/features/spark)\n    * [ GitHub Models  New  Manage and compare prompts  ](https://github.com/features/models)\n    * [ GitHub Advanced Security  Find and fix vulnerabilities  ](https://github.com/security/advanced-security)\n    * [ Actions  Automate any workflow  ](https://github.com/features/actions)\n    * [ Codespaces  Instant dev environments  ](https://github.com/features/codespaces)\n    * [ Issues  Plan and track work  ](https://github.com/features/issues)\n    * [ Code Review  Manage code changes  ](https://github.com/features/code-review)\n    * [ Discussions  Collaborate outside of code  ](https://github.com/features/discussions)\n    * [ Code Search  Find more, search less  ](https://github.com/features/code-search)\nExplore\n    * [ Why GitHub ](https://github.com/why-github)\n    * [ Documentation ](https://docs.github.com)\n    * [ GitHub Skills ](https://skills.github.com)\n    * [ Blog ](https://github.blog)\nIntegrations\n    * [ GitHub Marketplace ](https://github.com/marketplace)\n    * [ MCP Registry ](https://github.com/mcp)\n[ View all features ](https://github.com/features)\n  * Solutions \nBy company size\n    * [ Enterprises ](https://github.com/enterprise)\n    * [ Small and medium teams ](https://github.com/team)\n    * [ Startups ](https://github.com/enterprise/startups)\n    * [ Nonprofits ](https://github.com/solutions/industry/nonprofits)\nBy use case\n    * [ App Modernization ](https://github.com/solutions/use-case/app-modernization)\n    * [ DevSecOps ](https://github.com/solutions/use-case/devsecops)\n    * [ DevOps ](https://github.com/solutions/use-case/devops)\n    * [ CI/CD ](https://github.com/solutions/use-case/ci-cd)\n    * [ View all use cases ](https://github.com/solutions/use-case)\nBy industry\n    * [ Healthcare ](https://github.com/solutions/industry/healthcare)\n    * [ Financial services ](https://github.com/solutions/industry/financial-services)\n    * [ Manufacturing ](https://github.com/solutions/industry/manufacturing)\n    * [ Government ](https://github.com/solutions/industry/government)\n    * [ View all industries ](https://github.com/solutions/industry)\n[ View all solutions ](https://github.com/solutions)\n  * Resources \nTopics\n    * [ AI ](https://github.com/resources/articles?topic=ai)\n    * [ DevOps ](https://github.com/resources/articles?topic=devops)\n    * [ Security ](https://github.com/resources/articles?topic=security)\n    * [ Software Development ](https://github.com/resources/articles?topic=software-development)\n    * [ View all ](https://github.com/resources/articles)\nExplore\n    * [ Learning Pathways ](https://resources.github.com/learn/pathways)\n    * [ Events & Webinars ](https://github.com/resources/events)\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\n    * [ Customer Stories ](https://github.com/customer-stories)\n    * [ Partners ](https://github.com/partners)\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\n  * Open Source \n    * [ GitHub Sponsors  Fund open source developers  ](https://github.com/sponsors)\n    * [ The ReadME Project  GitHub community articles  ](https://github.com/readme)\nRepositories\n    * [ Topics ](https://github.com/topics)\n    * [ Trending ](https://github.com/trending)\n    * [ Collections ](https://github.com/collections)\n  * Enterprise \n    * [ Enterprise platform  AI-powered developer platform  ](https://github.com/enterprise)\nAvailable add-ons\n    * [ GitHub Advanced Security  Enterprise-grade security features  ](https://github.com/security/advanced-security)\n    * [ Copilot for business  Enterprise-grade AI features  ](https://github.com/features/copilot/copilot-business)\n    * [ Premium Support  Enterprise-grade 24/7 support  ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues%2F134)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=google%2Fadk-python)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/google/adk-python/issues/134) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/google/adk-python/issues/134) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/google/adk-python/issues/134) to refresh your session. Dismiss alert\n{{ message }}\n[ google ](https://github.com/google) / **[adk-python](https://github.com/google/adk-python) ** Public\n  * [ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ 2.5k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n  * [ Star  15.7k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n\n\n  * [ Code ](https://github.com/google/adk-python)\n  * [ Issues 350 ](https://github.com/google/adk-python/issues)\n  * [ Pull requests 101 ](https://github.com/google/adk-python/pulls)\n  * [ Discussions ](https://github.com/google/adk-python/discussions)\n  * [ Actions ](https://github.com/google/adk-python/actions)\n  * [ Projects 0 ](https://github.com/google/adk-python/projects)\n  * [ Security ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n### [ Uh oh!  ](https://github.com/google/adk-python/security)\n[There was an error while loading. ](https://github.com/google/adk-python/security)[Please reload this page](https://github.com/google/adk-python/issues/134).\n  * [ Insights ](https://github.com/google/adk-python/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/google/adk-python)\n  * [ Issues  ](https://github.com/google/adk-python/issues)\n  * [ Pull requests  ](https://github.com/google/adk-python/pulls)\n  * [ Discussions  ](https://github.com/google/adk-python/discussions)\n  * [ Actions  ](https://github.com/google/adk-python/actions)\n  * [ Projects  ](https://github.com/google/adk-python/projects)\n  * [ Security  ](https://github.com/google/adk-python/security)\n  * [ Insights  ](https://github.com/google/adk-python/pulse)\n\n\n#  400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}} #134\n[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues/134)\nCopy link\n[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues/134)\nCopy link\n[400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}}](https://github.com/google/adk-python/issues/134#top)#134\nCopy link\n[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=80)](https://github.com/kevinnkansah)\n## Description\n[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=48)](https://github.com/kevinnkansah)\n[kevinnkansah](https://github.com/kevinnkansah)\nopened [on Apr 13, 2025](https://github.com/google/adk-python/issues/134#issue-2990718457) · edited by [kevinnkansah](https://github.com/kevinnkansah)\nEdits\nIssue body actions\nI get this error but this is very confusing because Gemini 2.0 does support function calling as per [the docs](https://ai.google.dev/gemini-api/docs/models) what is happening\n```\nfrom google.adk.agents import LlmAgent, Agent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import google_search\nfrom google.adk.tools.agent_tool import AgentTool\nfrom google.genai import types\n\nMODEL = \"gemini-2.0-flash\"\n\n# Define a summarizer agent\nsummarizer = LlmAgent(\n    name=\"Summarizer\",\n    description=\"Summarizes text.\",\n    model=MODEL,  # Or your preferred Gemini model\n    instruction=\"You are an expert summarizer. Please read the following text and provide a concise summary of the key points.\"\n)\n\n# Define a web searcher agent\nweb_searcher = LlmAgent(\n    name=\"WebSearch\",\n    description=\"Performs web searches for facts.\",\n    model=MODEL,  # Or your preferred Gemini model\n    instruction=\"You are a web search expert. When given a topic or question, use the 'google_search' tool to find relevant information. Focus on retrieving factual and reliable sources.\",\n    tools=[google_search]  # Equip with the built-in Google Search tool\n)\n\n# Define the research assistant agent that orchestrates the search and summarization\nresearch_assistant = Agent(\n    name=\"ResearchAssistant\",\n    description=\"Finds and summarizes information on a topic.\",\n    model=MODEL,  # Or your preferred Gemini model\n    instruction=\"You are a research assistant. When the user provides a topic, first use the WebSearch tool to find relevant documents or web pages. Then, use the Summarizer tool to summarize the information you found. Present the summary to the user.\",\n    sub_agents=[web_searcher, summarizer]  # Equip with the defined subagents\n)\n\n# Define a high-level report writer agent that delegates research\nreport_writer = Agent(\n    name=\"ReportWriter\",\n    model=MODEL,  # Or your preferred Gemini model\n    instruction=\"You are a report writer. When the user asks you to write a report on a specific topic, delegate the task of gathering and summarizing information to the 'ResearchAssistant' tool. Once you receive the summary, format it into a coherent report for the user.\",\n    sub_agents=[research_assistant],  # Equip with the defined subagent\n)\n\n# The main research agent that the user will interact with\nresearch_agent = report_writer\n\n# This is the entry point that ADK looks for when loading your agent\nroot_agent = research_agent\n\nAPP_NAME = \"research_app\"\nUSER_ID = \"user123\"\nSESSION_ID = \"session1\"\n\nsession_service = InMemorySessionService()\nsession = session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\nrunner = Runner(agent=research_agent, app_name=APP_NAME, session_service=session_service)\n\nasync def call_agent(query):\n    content = types.Content(role='user', parts=[types.Part(text=query)])\n    events = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n    for event in events:\n        if event.is_final_response():\n            final_response = event.content.parts[0].text\n            print(\"Agent Response:\", final_response)\n\n\n```\n\n## Activity\n[![hangfei](https://avatars.githubusercontent.com/u/5482153?u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4&size=80)](https://github.com/hangfei)\n### hangfei commented on Apr 13, 2025 \n[![@hangfei](https://avatars.githubusercontent.com/u/5482153?u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4&size=48)](https://github.com/hangfei)\n[hangfei](https://github.com/hangfei)\n[on Apr 13, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2799197782)\nCollaborator\nMore actions\nDo you have the full error trace?\n[![kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=80)](https://github.com/kevinnkansah)\n### kevinnkansah commented on Apr 13, 2025 \n[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=48)](https://github.com/kevinnkansah)\n[kevinnkansah](https://github.com/kevinnkansah)\n[on Apr 13, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2799898244)\nAuthor\nMore actions\n[@hangfei](https://github.com/hangfei)\n```\nINFO:     Started server process [22853]\nINFO:     Waiting for application startup.\n+-----------------------------------------------------------------------------+\n| ADK Web Server started                                                      |\n|                                                                             |\n| For local testing, access at http://localhost:8000.                         |\n+-----------------------------------------------------------------------------+\n\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://*******:8000 (Press CTRL+C to quit)\nINFO:     *********:43564 - \"GET / HTTP/1.1\" 307 Temporary Redirect\nINFO:     *********:43564 - \"GET /dev-ui HTTP/1.1\" 200 OK\nINFO:     *********:43580 - \"GET /main-SLIAU2JL.js HTTP/1.1\" 304 Not Modified\nINFO:     *********:43564 - \"GET /polyfills-FFHMD2TL.js HTTP/1.1\" 304 Not Modified\nINFO:     *********:43596 - \"GET /styles-4VDSPQ37.css HTTP/1.1\" 304 Not Modified\nINFO:     *********:43596 - \"GET /assets/config/runtime-config.json HTTP/1.1\" 304 Not Modified\nINFO:     *********:43596 - \"GET /list-apps?relative_path=./ HTTP/1.1\" 200 OK\n2025-04-13 12:31:01,013 - INFO - fast_api.py:294 - New session created\nINFO:     *********:43596 - \"POST /apps/research-agent/users/user/sessions HTTP/1.1\" 200 OK\nINFO:     *********:43596 - \"GET /apps/research-agent/eval_sets HTTP/1.1\" 200 OK\nINFO:     *********:43596 - \"GET /apps/research-agent/users/user/sessions HTTP/1.1\" 200 OK\nINFO:     *********:43596 - \"GET /apps/research-agent/users/user/sessions HTTP/1.1\" 200 OK\nINFO:     *********:43596 - \"POST /run_sse HTTP/1.1\" 200 OK\n2025-04-13 12:31:04,059 - INFO - envs.py:47 - Loaded .env file for research-agent at /home/kevin/Documents/adk-research-agent/.env\n2025-04-13 12:31:04,059 - INFO - envs.py:53 - Reloaded .env file for research-agent at /home/kevin/Documents/adk-research-agent/.env\n2025-04-13 12:31:04,113 - INFO - google_llm.py:83 - Sending out request, model: gemini-2.0-flash, backend: ml_dev, stream: False\n2025-04-13 12:31:04,114 - INFO - google_llm.py:89 - \nLLM Request:\n-----------------------------------------------------------\nSystem Instruction:\nYou are a report writer. When the user asks you to write a report on a specific topic, delegate the task of gathering and summarizing information to the 'ResearchAssistant' tool. Once you receive the summary, format it into a coherent report for the user.\n\nYou are an agent. Your internal name is \"ReportWriter\".\n\n\nYou have a list of other agents to transfer to:\n\n\nAgent name: ResearchAssistant\nAgent description: Finds and summarizes information on a topic.\n\n\nIf you are the best to answer the question according to your description, you\ncan answer it.\n\nIf another agent is better for answering the question according to its\ndescription, call `transfer_to_agent` function to transfer the\nquestion to that agent. When transfering, do not generate any text other than\nthe function call.\n\n-----------------------------------------------------------\nContents:\n{\"parts\":[{\"text\":\"hI\"}],\"role\":\"user\"}\n-----------------------------------------------------------\nFunctions:\ntransfer_to_agent: {'agent_name': {'type': <Type.STRING: 'STRING'>}} -> None\n-----------------------------------------------------------\n\n2025-04-13 12:31:04,114 - INFO - models.py:6423 - AFC is enabled with max remote calls: 10.\n2025-04-13 12:31:05,285 - INFO - _client.py:1740 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n2025-04-13 12:31:05,295 - INFO - google_llm.py:145 - \nLLM Response:\n-----------------------------------------------------------\nText:\nHello! How can I help you today?\n\n-----------------------------------------------------------\nFunction calls:\n\n-----------------------------------------------------------\nRaw response:\n{\"candidates\":[{\"content\":{\"parts\":[{\"text\":\"Hello! How can I help you today?\\n\"}],\"role\":\"model\"},\"finish_reason\":\"STOP\",\"avg_logprobs\":-0.021260035037994385}],\"model_version\":\"gemini-2.0-flash\",\"usage_metadata\":{\"candidates_token_count\":10,\"candidates_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":10}],\"prompt_token_count\":185,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":185}],\"total_token_count\":195},\"automatic_function_calling_history\":[]}\n-----------------------------------------------------------\n\n2025-04-13 12:31:05,296 - INFO - fast_api.py:613 - Generated event in agent run streaming: {\"content\":{\"parts\":[{\"text\":\"Hello! How can I help you today?\\n\"}],\"role\":\"model\"},\"invocation_id\":\"e-************************************\",\"author\":\"ReportWriter\",\"actions\":{\"state_delta\":{},\"artifact_delta\":{},\"requested_auth_configs\":{}},\"id\":\"B1qiH6r0\",\"timestamp\":1744540264.07067}\nINFO:     *********:43596 - \"GET /apps/research-agent/users/user/sessions/************************************ HTTP/1.1\" 200 OK\nINFO:     *********:50214 - \"POST /run_sse HTTP/1.1\" 200 OK\n2025-04-13 12:31:25,999 - INFO - google_llm.py:83 - Sending out request, model: gemini-2.0-flash, backend: ml_dev, stream: False\n2025-04-13 12:31:25,999 - INFO - google_llm.py:89 - \nLLM Request:\n-----------------------------------------------------------\nSystem Instruction:\nYou are a report writer. When the user asks you to write a report on a specific topic, delegate the task of gathering and summarizing information to the 'ResearchAssistant' tool. Once you receive the summary, format it into a coherent report for the user.\n\nYou are an agent. Your internal name is \"ReportWriter\".\n\n\nYou have a list of other agents to transfer to:\n\n\nAgent name: ResearchAssistant\nAgent description: Finds and summarizes information on a topic.\n\n\nIf you are the best to answer the question according to your description, you\ncan answer it.\n\nIf another agent is better for answering the question according to its\ndescription, call `transfer_to_agent` function to transfer the\nquestion to that agent. When transfering, do not generate any text other than\nthe function call.\n\n-----------------------------------------------------------\nContents:\n{\"parts\":[{\"text\":\"hI\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"Hello! How can I help you today?\\n\"}],\"role\":\"model\"}\n{\"parts\":[{\"text\":\"i need information about the latest UCL fixtures and predictions\"}],\"role\":\"user\"}\n-----------------------------------------------------------\nFunctions:\ntransfer_to_agent: {'agent_name': {'type': <Type.STRING: 'STRING'>}} -> None\n-----------------------------------------------------------\n\n2025-04-13 12:31:25,999 - INFO - models.py:6423 - AFC is enabled with max remote calls: 10.\n2025-04-13 12:31:27,354 - INFO - _client.py:1740 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n2025-04-13 12:31:27,355 - WARNING - types.py:3082 - Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n2025-04-13 12:31:27,356 - INFO - google_llm.py:145 - \nLLM Response:\n-----------------------------------------------------------\nText:\nI can write a report about the latest UCL fixtures and predictions, but first, I need to gather the information. I will transfer this request to the ResearchAssistant.\n\n\n-----------------------------------------------------------\nFunction calls:\nname: transfer_to_agent, args: {'agent_name': 'ResearchAssistant'}\n-----------------------------------------------------------\nRaw response:\n{\"candidates\":[{\"content\":{\"parts\":[{\"text\":\"I can write a report about the latest UCL fixtures and predictions, but first, I need to gather the information. I will transfer this request to the ResearchAssistant.\\n\\n\"},{\"function_call\":{\"args\":{\"agent_name\":\"ResearchAssistant\"},\"name\":\"transfer_to_agent\"}}],\"role\":\"model\"},\"finish_reason\":\"STOP\",\"avg_logprobs\":-0.11377541585402055}],\"model_version\":\"gemini-2.0-flash\",\"usage_metadata\":{\"candidates_token_count\":44,\"candidates_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":44}],\"prompt_token_count\":205,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":205}],\"total_token_count\":249},\"automatic_function_calling_history\":[]}\n-----------------------------------------------------------\n\n2025-04-13 12:31:27,356 - INFO - fast_api.py:613 - Generated event in agent run streaming: {\"content\":{\"parts\":[{\"text\":\"I can write a report about the latest UCL fixtures and predictions, but first, I need to gather the information. I will transfer this request to the ResearchAssistant.\\n\\n\"},{\"functionCall\":{\"id\":\"adk-************************************\",\"args\":{\"agent_name\":\"ResearchAssistant\"},\"name\":\"transfer_to_agent\"}}],\"role\":\"model\"},\"invocation_id\":\"e-************************************\",\"author\":\"ReportWriter\",\"actions\":{\"state_delta\":{},\"artifact_delta\":{},\"requested_auth_configs\":{}},\"long_running_tool_ids\":[],\"id\":\"9BUjMcB1\",\"timestamp\":1744540285.939834}\n2025-04-13 12:31:27,357 - INFO - fast_api.py:613 - Generated event in agent run streaming: {\"content\":{\"parts\":[{\"functionResponse\":{\"id\":\"adk-************************************\",\"name\":\"transfer_to_agent\",\"response\":{}}}],\"role\":\"user\"},\"invocation_id\":\"e-************************************\",\"author\":\"ReportWriter\",\"actions\":{\"state_delta\":{},\"artifact_delta\":{},\"transfer_to_agent\":\"ResearchAssistant\",\"requested_auth_configs\":{}},\"id\":\"TcLu314I\",\"timestamp\":1744540287.356908}\n2025-04-13 12:31:27,396 - INFO - google_llm.py:83 - Sending out request, model: gemini-2.0-flash, backend: ml_dev, stream: False\n2025-04-13 12:31:27,397 - INFO - google_llm.py:89 - \nLLM Request:\n-----------------------------------------------------------\nSystem Instruction:\nYou are a research assistant. When the user provides a topic, first use the WebSearch tool to find relevant documents or web pages. Then, use the Summarizer tool to summarize the information you found. Present the summary to the user.\n\nYou are an agent. Your internal name is \"ResearchAssistant\".\n\n The description about you is \"Finds and summarizes information on a topic.\"\n\n\nYou have a list of other agents to transfer to:\n\n\nAgent name: WebSearch\nAgent description: Performs web searches for facts.\n\n\nAgent name: Summarizer\nAgent description: Summarizes text.\n\n\nAgent name: ReportWriter\nAgent description: \n\n\nIf you are the best to answer the question according to your description, you\ncan answer it.\n\nIf another agent is better for answering the question according to its\ndescription, call `transfer_to_agent` function to transfer the\nquestion to that agent. When transfering, do not generate any text other than\nthe function call.\n\nYour parent agent is ReportWriter. If neither the other agents nor\nyou are best for answering the question according to the descriptions, transfer\nto your parent agent. If you don't have parent agent, try answer by yourself.\n\n-----------------------------------------------------------\nContents:\n{\"parts\":[{\"text\":\"hI\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ReportWriter] said: Hello! How can I help you today?\\n\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"i need information about the latest UCL fixtures and predictions\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ReportWriter] said: I can write a report about the latest UCL fixtures and predictions, but first, I need to gather the information. I will transfer this request to the ResearchAssistant.\\n\\n\"},{\"text\":\"[ReportWriter] called tool `transfer_to_agent` with parameters: {'agent_name': 'ResearchAssistant'}\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ReportWriter] `transfer_to_agent` tool returned result: {}\"}],\"role\":\"user\"}\n-----------------------------------------------------------\nFunctions:\ntransfer_to_agent: {'agent_name': {'type': <Type.STRING: 'STRING'>}} -> None\n-----------------------------------------------------------\n\n2025-04-13 12:31:27,397 - INFO - models.py:6423 - AFC is enabled with max remote calls: 10.\n2025-04-13 12:31:28,336 - INFO - _client.py:1740 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n2025-04-13 12:31:28,338 - WARNING - types.py:3082 - Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n2025-04-13 12:31:28,338 - INFO - google_llm.py:145 - \nLLM Response:\n-----------------------------------------------------------\nText:\nNone\n-----------------------------------------------------------\nFunction calls:\nname: transfer_to_agent, args: {'agent_name': 'WebSearch'}\n-----------------------------------------------------------\nRaw response:\n{\"candidates\":[{\"content\":{\"parts\":[{\"function_call\":{\"args\":{\"agent_name\":\"WebSearch\"},\"name\":\"transfer_to_agent\"}}],\"role\":\"model\"},\"finish_reason\":\"STOP\",\"avg_logprobs\":-0.011882495880126954}],\"model_version\":\"gemini-2.0-flash\",\"usage_metadata\":{\"candidates_token_count\":10,\"candidates_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":10}],\"prompt_token_count\":384,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":384}],\"total_token_count\":394},\"automatic_function_calling_history\":[]}\n-----------------------------------------------------------\n\n2025-04-13 12:31:28,339 - INFO - fast_api.py:613 - Generated event in agent run streaming: {\"content\":{\"parts\":[{\"functionCall\":{\"id\":\"adk-************************************\",\"args\":{\"agent_name\":\"WebSearch\"},\"name\":\"transfer_to_agent\"}}],\"role\":\"model\"},\"invocation_id\":\"e-************************************\",\"author\":\"ResearchAssistant\",\"actions\":{\"state_delta\":{},\"artifact_delta\":{},\"requested_auth_configs\":{}},\"long_running_tool_ids\":[],\"id\":\"LoB08Khh\",\"timestamp\":1744540287.357733}\n2025-04-13 12:31:28,339 - INFO - fast_api.py:613 - Generated event in agent run streaming: {\"content\":{\"parts\":[{\"functionResponse\":{\"id\":\"adk-************************************\",\"name\":\"transfer_to_agent\",\"response\":{}}}],\"role\":\"user\"},\"invocation_id\":\"e-************************************\",\"author\":\"ResearchAssistant\",\"actions\":{\"state_delta\":{},\"artifact_delta\":{},\"transfer_to_agent\":\"WebSearch\",\"requested_auth_configs\":{}},\"id\":\"emVQbABp\",\"timestamp\":1744540288.339786}\n2025-04-13 12:31:28,385 - INFO - google_llm.py:83 - Sending out request, model: gemini-2.0-flash, backend: ml_dev, stream: False\n2025-04-13 12:31:28,385 - INFO - google_llm.py:89 - \nLLM Request:\n-----------------------------------------------------------\nSystem Instruction:\nYou are a web search expert. When given a topic or question, use the 'google_search' tool to find relevant information. Focus on retrieving factual and reliable sources.\n\nYou are an agent. Your internal name is \"WebSearch\".\n\n The description about you is \"Performs web searches for facts.\"\n\n\nYou have a list of other agents to transfer to:\n\n\nAgent name: ResearchAssistant\nAgent description: Finds and summarizes information on a topic.\n\n\nAgent name: Summarizer\nAgent description: Summarizes text.\n\n\nIf you are the best to answer the question according to your description, you\ncan answer it.\n\nIf another agent is better for answering the question according to its\ndescription, call `transfer_to_agent` function to transfer the\nquestion to that agent. When transfering, do not generate any text other than\nthe function call.\n\nYour parent agent is ResearchAssistant. If neither the other agents nor\nyou are best for answering the question according to the descriptions, transfer\nto your parent agent. If you don't have parent agent, try answer by yourself.\n\n-----------------------------------------------------------\nContents:\n{\"parts\":[{\"text\":\"hI\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ReportWriter] said: Hello! How can I help you today?\\n\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"i need information about the latest UCL fixtures and predictions\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ReportWriter] said: I can write a report about the latest UCL fixtures and predictions, but first, I need to gather the information. I will transfer this request to the ResearchAssistant.\\n\\n\"},{\"text\":\"[ReportWriter] called tool `transfer_to_agent` with parameters: {'agent_name': 'ResearchAssistant'}\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ReportWriter] `transfer_to_agent` tool returned result: {}\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ResearchAssistant] called tool `transfer_to_agent` with parameters: {'agent_name': 'WebSearch'}\"}],\"role\":\"user\"}\n{\"parts\":[{\"text\":\"For context:\"},{\"text\":\"[ResearchAssistant] `transfer_to_agent` tool returned result: {}\"}],\"role\":\"user\"}\n-----------------------------------------------------------\nFunctions:\ntransfer_to_agent: {'agent_name': {'type': <Type.STRING: 'STRING'>}} -> None\n-----------------------------------------------------------\n\n2025-04-13 12:31:28,385 - INFO - models.py:6423 - AFC is enabled with max remote calls: 10.\n2025-04-13 12:31:28,847 - INFO - _client.py:1740 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 400 Bad Request\"\n2025-04-13 12:31:28,863 - ERROR - fast_api.py:616 - Error in event_generator: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}}\nTraceback (most recent call last):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/cli/fast_api.py\", line 605, in event_generator\n    async for event in runner.run_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/runners.py\", line 197, in run_async\n    async for event in invocation_context.agent.run_async(invocation_context):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py\", line 141, in run_async\n    async for event in self._run_async_impl(ctx):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/agents/llm_agent.py\", line 232, in _run_async_impl\n    async for event in self._llm_flow.run_async(ctx):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 231, in run_async\n    async for event in self._run_one_step_async(invocation_context):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 261, in _run_one_step_async\n    async for event in self._postprocess_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 329, in _postprocess_async\n    async for event in self._postprocess_handle_function_calls_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 419, in _postprocess_handle_function_calls_async\n    async for event in agent_to_run.run_async(invocation_context):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py\", line 141, in run_async\n    async for event in self._run_async_impl(ctx):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/agents/llm_agent.py\", line 232, in _run_async_impl\n    async for event in self._llm_flow.run_async(ctx):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 231, in run_async\n    async for event in self._run_one_step_async(invocation_context):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 261, in _run_one_step_async\n    async for event in self._postprocess_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 329, in _postprocess_async\n    async for event in self._postprocess_handle_function_calls_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 419, in _postprocess_handle_function_calls_async\n    async for event in agent_to_run.run_async(invocation_context):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py\", line 141, in run_async\n    async for event in self._run_async_impl(ctx):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/agents/llm_agent.py\", line 232, in _run_async_impl\n    async for event in self._llm_flow.run_async(ctx):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 231, in run_async\n    async for event in self._run_one_step_async(invocation_context):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 257, in _run_one_step_async\n    async for llm_response in self._call_llm_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 470, in _call_llm_async\n    async for llm_response in llm.generate_content_async(\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/models/google_llm.py\", line 140, in generate_content_async\n    response = await self.api_client.aio.models.generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/genai/models.py\", line 6429, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/genai/models.py\", line 5431, in _generate_content\n    response_dict = await self._api_client.async_request(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/genai/_api_client.py\", line 708, in async_request\n    result = await self._async_request(http_request=http_request, stream=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/genai/_api_client.py\", line 652, in _async_request\n    await errors.APIError.raise_for_async_response(response)\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/genai/errors.py\", line 129, in raise_for_async_response\n    raise ClientError(status_code, response_json, response)\ngoogle.genai.errors.ClientError: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}}\n2025-04-13 12:31:28,865 - ERROR - __init__.py:157 - Failed to detach context\nTraceback (most recent call last):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 587, in use_span\n    yield span\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 452, in start_as_current_span\n    yield span\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 487, in _call_llm_async\n    yield llm_response\nGeneratorExit\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7f4dc2ab1210> at 0x7f4db9a94d40> was created in a different Context\n2025-04-13 12:31:28,866 - ERROR - __init__.py:157 - Failed to detach context\nTraceback (most recent call last):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 587, in use_span\n    yield span\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1105, in start_as_current_span\n    yield span\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 452, in start_as_current_span\n    yield span\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 487, in _call_llm_async\n    yield llm_response\nGeneratorExit\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 155, in detach\n    _RUNTIME_CONTEXT.detach(token)\n  File \"/home/kevin/Documents/adk-research-agent/.venv/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 53, in detach\n    self._current_context.reset(token)\nValueError: <Token var=<ContextVar name='current_context' default={} at 0x7f4dc2ab1210> at 0x7f4db9a79380> was created in a different Context\nINFO:     *********:50214 - \"GET /apps/research-agent/users/user/sessions/************************************ HTTP/1.1\" 200 OK\n\n```\n\n👍React with 👍1leslysandra\n[![IAmNo1Special](https://avatars.githubusercontent.com/u/186975440?u=03c3d32b2eab97b3fb0a24a66bc0137e7e57f398&v=4&size=80)](https://github.com/IAmNo1Special)\n### IAmNo1Special commented on Apr 14, 2025 \n[![@IAmNo1Special](https://avatars.githubusercontent.com/u/186975440?u=03c3d32b2eab97b3fb0a24a66bc0137e7e57f398&v=4&size=48)](https://github.com/IAmNo1Special)\n[IAmNo1Special](https://github.com/IAmNo1Special)\n[on Apr 14, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2800128726) · edited by [IAmNo1Special](https://github.com/IAmNo1Special)\nEdits\nMore actions\n[@kevinnkansah](https://github.com/kevinnkansah) try changing:\n```\nevents = runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n```\n\nTo:\n```\nevents = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n\n```\n\nLet me know if that fixes it for you\n[![kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=80)](https://github.com/kevinnkansah)\n### kevinnkansah commented on Apr 14, 2025 \n[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=48)](https://github.com/kevinnkansah)\n[kevinnkansah](https://github.com/kevinnkansah)\n[on Apr 14, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2800685369)\nAuthor\nMore actions\n[@IAmNo1Special](https://github.com/IAmNo1Special) it did not change anything, this must be a bug, [@hangfei](https://github.com/hangfei)\n[![kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=80)](https://github.com/kevinnkansah)\n### kevinnkansah commented on Apr 14, 2025 \n[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=48)](https://github.com/kevinnkansah)\n[kevinnkansah](https://github.com/kevinnkansah)\n[on Apr 14, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2800693008)\nAuthor\nMore actions\n### Same Issue, even if i use `gemini-2.5-pro-exp-03-25`\n[![Image](https://private-user-images.githubusercontent.com/144918669/433259021-91c3d92c-c770-44e1-9fc2-a25d476091a2.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyNTIwNTksIm5iZiI6MTc2NDI1MTc1OSwicGF0aCI6Ii8xNDQ5MTg2NjkvNDMzMjU5MDIxLTkxYzNkOTJjLWM3NzAtNDRlMS05ZmMyLWEyNWQ0NzYwOTFhMi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEyN1QxMzU1NTlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wZGUyNzhiMzYzZmNlOTljMDAyMjcyOWU2MTE4NzM5MmM4ZTMzZGZiNDZiNzFhZDBiNmRlMzE4ZDdhYWRjZmE2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.qOwrDGvo-99wWJRRiCO-XA7lXGLSumorKK7pds2c7oQ)](https://private-user-images.githubusercontent.com/144918669/433259021-91c3d92c-c770-44e1-9fc2-a25d476091a2.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjQyNTIwNTksIm5iZiI6MTc2NDI1MTc1OSwicGF0aCI6Ii8xNDQ5MTg2NjkvNDMzMjU5MDIxLTkxYzNkOTJjLWM3NzAtNDRlMS05ZmMyLWEyNWQ0NzYwOTFhMi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTEyN1QxMzU1NTlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wZGUyNzhiMzYzZmNlOTljMDAyMjcyOWU2MTE4NzM5MmM4ZTMzZGZiNDZiNzFhZDBiNmRlMzE4ZDdhYWRjZmE2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.qOwrDGvo-99wWJRRiCO-XA7lXGLSumorKK7pds2c7oQ)\n[![nelsonfrugeri-tech](https://avatars.githubusercontent.com/u/3945843?u=062c56bc89b31146220b3a78e8a9d82f14e7f8bc&v=4&size=80)](https://github.com/nelsonfrugeri-tech)\n### nelsonfrugeri-tech commented on Apr 14, 2025 \n[![@nelsonfrugeri-tech](https://avatars.githubusercontent.com/u/3945843?u=062c56bc89b31146220b3a78e8a9d82f14e7f8bc&v=4&size=48)](https://github.com/nelsonfrugeri-tech)\n[nelsonfrugeri-tech](https://github.com/nelsonfrugeri-tech)\n[on Apr 14, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2801710804) · edited by [nelsonfrugeri-tech](https://github.com/nelsonfrugeri-tech)\nEdits\nMore actions\nThanks for raising this! I faced the same issue and after some investigation, here’s what I believe is happening — hope it helps clarify the underlying mechanism:\nThe problem seems to stem from how ADK handles function calling + tool usage in the same interaction turn. Specifically:  \n• When a sub-agent is registered using the sub_agents parameter, it’s exposed to the orchestrating LLM as a function.  \n• When this function (i.e., the sub-agent) is invoked by the orchestrator, the LLM is effectively issuing a function_call.  \n• However, once inside that function-called context, the sub-agent is no longer allowed to invoke any tools, such as google_search.\nEven though the sub-agent is properly configured with tools, the ADK backend seems to block any tool use from within a function execution context. This results in the error:\n```\n400 INVALID_ARGUMENT: Tool use with function calling is unsupported\n```\n\nThis becomes a limitation when you’re trying to design modular agent architectures, where each sub-agent handles a specific responsibility (e.g. a search agent, a calculator agent). With the current behavior, sub-agents lose access to their tools when invoked through the orchestrator.\nI detailed this in issue [#53](https://github.com/google/adk-python/issues/53) as well, including a reproduction case and suggestions. Hope this helps the team better understand and prioritize a fix or official workaround.\nLet me know if you’ve found any alternate solution!\n👍React with 👍1aamanlamba\n[![kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=80)](https://github.com/kevinnkansah)\n### kevinnkansah commented on Apr 14, 2025 \n[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?v=4&size=48)](https://github.com/kevinnkansah)\n[kevinnkansah](https://github.com/kevinnkansah)\n[on Apr 14, 2025](https://github.com/google/adk-python/issues/134#issuecomment-2801819559)\nAuthor\nMore actions\nOkay, I understand, hopefullly the issue gets resolved. I have seen the workaround in [#53 ](https://github.com/google/adk-python/issues/53#issuecomment-2798906767). I will close this issue\n[![](https://avatars.githubusercontent.com/u/144918669?s=64&v=4)kevinnkansah](https://github.com/kevinnkansah)\nclosed this as [completed](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aclosed%20archived%3Afalse%20reason%3Acompleted)[on Apr 14, 2025](https://github.com/google/adk-python/issues/134#event-17249658400)\n[![](https://avatars.githubusercontent.com/u/2855795?s=64&v=4)buggyj](https://github.com/buggyj)\nmentioned this [on May 6, 2025](https://github.com/google/adk-python/issues/134#event-2414976240)\n  * [Tool use with function calling is unsupported buggyj/simplifai#1](https://github.com/buggyj/simplifai/issues/1)\n\n\n[Sign up for free](https://github.com/signup?return_to=https://github.com/google/adk-python/issues/134)**to join this conversation on GitHub.** Already have an account? [Sign in to comment](https://github.com/login?return_to=https://github.com/google/adk-python/issues/134)\n## Metadata\n## Metadata\n### Assignees\nNo one assigned\n### Labels\nNo labels\nNo labels\n### Type\nNo type\n### Projects\nNo projects\n### Milestone\nNo milestone\n### Relationships\nNone yet\n### Development\nCode with agent mode\nSelect code repository\nNo branches or pull requests\n### Participants\n[![@nelsonfrugeri-tech](https://avatars.githubusercontent.com/u/3945843?s=64&u=062c56bc89b31146220b3a78e8a9d82f14e7f8bc&v=4)](https://github.com/nelsonfrugeri-tech)[![@hangfei](https://avatars.githubusercontent.com/u/5482153?s=64&u=82ea952c1b444c70c6c72d4262daf95c4e643e64&v=4)](https://github.com/hangfei)[![@kevinnkansah](https://avatars.githubusercontent.com/u/144918669?s=64&v=4)](https://github.com/kevinnkansah)[![@IAmNo1Special](https://avatars.githubusercontent.com/u/186975440?s=64&u=03c3d32b2eab97b3fb0a24a66bc0137e7e57f398&v=4)](https://github.com/IAmNo1Special)\n## Issue actions\n## Footer\n[ ](https://github.com) © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Community](https://github.community/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\n400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tool use with function calling is unsupported', 'status': 'INVALID_ARGUMENT'}} · Issue #134 · google/adk-python\n\n\n\n# Content from https://community.home-assistant.io/t/2024-4-0-google-generative-ai-with-assist-error/713606\n\n[Skip to main content](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606#main-container)\n[ ![Home Assistant Community](https://community-assets.home-assistant.io/original/4X/c/6/b/c6b150b91322bbf7137c5124594bff3f5de0a3e4.png) ](https://community.home-assistant.io/)\nSign Up Log In\n\n\n  * [ Topics  ](https://community.home-assistant.io/latest \"All topics\")\n  * [ Discord  ](https://discord.gg/c5DvZ4e \"Discord\")\n  * More \n\n\nCategories \n  * [ Blog  ](https://community.home-assistant.io/c/blog/30 \"This category will stay in sync with our blog and be used for commenting.\")\n  * [ Configuration  ](https://community.home-assistant.io/c/configuration/11 \"Questions about configuring Home Assistant to automate your house.\")\n  * [ Blueprints Exchange  ](https://community.home-assistant.io/c/blueprints-exchange/53 \"Blueprints are reusable automations and scripts that can easily be shared. For more information, see the blueprint documentation.\")\n  * [ Installation  ](https://community.home-assistant.io/c/installation/17 \"Need support installing Home Assistant? The community is happy to be of help!\")\n  * [ Share your Projects!  ](https://community.home-assistant.io/c/projects/9 \"Show off the cool projects, automations and configurations you made with Home Assistant!\")\n  * [ All categories  ](https://community.home-assistant.io/categories)\n\n\nTags \n  * [ automation  ](https://community.home-assistant.io/tag/automation)\n  * [ templates  ](https://community.home-assistant.io/tag/templates)\n  * [ mqtt  ](https://community.home-assistant.io/tag/mqtt)\n  * [ switch  ](https://community.home-assistant.io/tag/switch)\n  * [ zwave  ](https://community.home-assistant.io/tag/zwave)\n  * [ All tags  ](https://community.home-assistant.io/tags)\n\n\n#  [ Home Assistant Crashing After Z-Wave USB Disconnects ](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606)\n[Installation](https://community.home-assistant.io/c/installation/17)\nYou have selected **0** posts.\n[ select all ](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606)\n[ cancel selecting ](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606)\n[ Apr 2024  ](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606/1 \"Jump to the first post\")\n1 / 1 \nApr 2024 \n[ Apr 2024 ](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606/1)\n[![](https://community.home-assistant.io/user_avatar/community.home-assistant.io/sirtobil/48/531922_2.png)](https://community.home-assistant.io/u/SirTobil)\n[SirTobil](https://community.home-assistant.io/u/SirTobil)[Anthony Howard](https://community.home-assistant.io/u/SirTobil)\n[Apr 2024](https://community.home-assistant.io/t/home-assistant-crashing-after-z-wave-usb-disconnects/713606 \"Post date\")\nHello!\nTo start, I am running HA on Windows 10 in a VM using VirtualBox. I also have a [Zooz ZST10 700 - Z-Wave Plus S2 USB Stick](https://www.thesmartesthouse.com/products/zooz-usb-700-series-z-wave-plus-s2-stick-zst10-700) in one of the computers USB ports, which I then pass to the VM. HA version information in the image below.\nEverything appears to work fine except that I’m getting these random USB disconnects that crashes HA. An error window from VB pops up and there are a few lines printed in the console of HA. Screenshots of these are in the image.\n[![Screenshot 2024-04-05 182006](https://community-assets.home-assistant.io/optimized/4X/f/0/c/f0cfba64e1e027a0892615781afb327ea5b9b01c_2_540x500.png) Screenshot 2024-04-05 182006603×558 74.4 KB ](https://community-assets.home-assistant.io/original/4X/f/0/c/f0cfba64e1e027a0892615781afb327ea5b9b01c.png \"Screenshot 2024-04-05 182006\")\nI have already tried redoing HA from scratch. It seemed to fix it for a couple of months, but then the issue started coming back again. I have also tried reinstalling drivers, switching USB ports, and so-on and like before it “fixes” it for a while and then returns. I’m not really sure what else to check at this point.\nIf there’s any more information I can provide, please let me know.\nThank you!\n  \n\n241 views \nReply\n  \n\n###  New & Unread Topics \nTopic list, column headers with buttons are sortable. Topic  |  Replies  |  Views  |  Activity   \n---|---|---|---  \n[HA & DNS gives me headache](https://community.home-assistant.io/t/ha-dns-gives-me-headache/916070) [Installation](https://community.home-assistant.io/c/installation/17) [home-assistant-os](https://community.home-assistant.io/tag/home-assistant-os) |  [ 9 ](https://community.home-assistant.io/t/ha-dns-gives-me-headache/916070/1) |  179 |  [Aug 2](https://community.home-assistant.io/t/ha-dns-gives-me-headache/916070/10)  \n[Why have docker Version 2025.8.3 not latest Tag?](https://community.home-assistant.io/t/why-have-docker-version-2025-8-3-not-latest-tag/923965) [Installation](https://community.home-assistant.io/c/installation/17) |  [ 3 ](https://community.home-assistant.io/t/why-have-docker-version-2025-8-3-not-latest-tag/923965/1) |  159 |  [Aug 26](https://community.home-assistant.io/t/why-have-docker-version-2025-8-3-not-latest-tag/923965/4)  \n[Help! Home assistant host suddenly without network](https://community.home-assistant.io/t/help-home-assistant-host-suddenly-without-network/830910) [Installation](https://community.home-assistant.io/c/installation/17) |  [ 5 ](https://community.home-assistant.io/t/help-home-assistant-host-suddenly-without-network/830910/1) |  326 |  [Jan 18](https://community.home-assistant.io/t/help-home-assistant-host-suddenly-without-network/830910/6)  \n[New Installation help!](https://community.home-assistant.io/t/new-installation-help/826662) [Installation](https://community.home-assistant.io/c/installation/17) |  [ 9 ](https://community.home-assistant.io/t/new-installation-help/826662/1) |  201 |  [Jan 11](https://community.home-assistant.io/t/new-installation-help/826662/10)  \n[[SOLVED] I am seriously stack with old Home Assistant](https://community.home-assistant.io/t/solved-i-am-seriously-stack-with-old-home-assistant/828281) [Home Assistant OS](https://community.home-assistant.io/c/installation/home-assistant-os/25) |  [ 15 ](https://community.home-assistant.io/t/solved-i-am-seriously-stack-with-old-home-assistant/828281/1) |  753 |  [Jan 19](https://community.home-assistant.io/t/solved-i-am-seriously-stack-with-old-home-assistant/828281/16)  \n###  Want to read more? Browse other topics in [Installation](https://community.home-assistant.io/c/installation/17) or [view latest topics](https://community.home-assistant.io/latest). \n[ Powered by Discourse ](https://discourse.org/powered-by)\nInvalid date \n\n\n\n# Content from https://ai.google.dev/gemini-api/docs/troubleshooting\n\n[ Skip to main content ](https://ai.google.dev/gemini-api/docs/troubleshooting#main-content)\n[ ![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg) ](https://ai.google.dev/)\n`/`\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Polski\n  * Português – Brasil\n  * Shqip\n  * Tiếng Việt\n  * Türkçe\n  * Русский\n  * עברית\n  * العربيّة\n  * فارسی\n  * हिंदी\n  * বাংলা\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n[ Get API key ](https://aistudio.google.com/apikey) [ Cookbook ](https://github.com/google-gemini/cookbook) [ Community ](https://discuss.ai.google.dev/c/gemini-api/) Sign in\n[ Gemini API Docs ](https://ai.google.dev/gemini-api/docs) [ API reference ](https://ai.google.dev/api)\n[ ![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg) ](https://ai.google.dev/)\n  * [ Gemini API  ](https://ai.google.dev/gemini-api/docs)\n    * [ Gemini API Docs  ](https://ai.google.dev/gemini-api/docs)\n    * [ API reference  ](https://ai.google.dev/api)\n  * [ Get API key  ](https://aistudio.google.com/apikey)\n  * [ Cookbook  ](https://github.com/google-gemini/cookbook)\n  * [ Community  ](https://discuss.ai.google.dev/c/gemini-api/)\n\n\n  * Get started\n  * [Overview](https://ai.google.dev/gemini-api/docs)\n  * [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n  * [API keys](https://ai.google.dev/gemini-api/docs/api-key)\n  * [Vibe code in Build mode](https://ai.google.dev/gemini-api/docs/aistudio-build-mode)\n  * [Libraries](https://ai.google.dev/gemini-api/docs/libraries)\n  * [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)\n  * Models\n  * [Gemini](https://ai.google.dev/gemini-api/docs/models)\n  * [Gemini 3](https://ai.google.dev/gemini-api/docs/gemini-3)\n  * [Imagen (image generation)](https://ai.google.dev/gemini-api/docs/imagen)\n  * [Veo (video generation)](https://ai.google.dev/gemini-api/docs/video)\n  * [Lyria (music generation)](https://ai.google.dev/gemini-api/docs/music-generation)\n  * [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)\n  * [Robotics](https://ai.google.dev/gemini-api/docs/robotics-overview)\n  * [Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n  * [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n  * Core capabilities\n  * [Text](https://ai.google.dev/gemini-api/docs/text-generation)\n  * Image\n    * [Image generation 🍌](https://ai.google.dev/gemini-api/docs/image-generation)\n    * [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)\n  * [Video](https://ai.google.dev/gemini-api/docs/video-understanding)\n  * [Documents](https://ai.google.dev/gemini-api/docs/document-processing)\n  * Speech and audio\n    * [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)\n    * [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)\n  * Thinking and thought signatures\n    * [Thinking](https://ai.google.dev/gemini-api/docs/thinking)\n    * [Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)\n  * [Structured outputs](https://ai.google.dev/gemini-api/docs/structured-output)\n  * [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)\n  * [Long context](https://ai.google.dev/gemini-api/docs/long-context)\n  * Tools\n  * [Overview](https://ai.google.dev/gemini-api/docs/tools)\n  * [Google Search](https://ai.google.dev/gemini-api/docs/google-search)\n  * [Google Maps](https://ai.google.dev/gemini-api/docs/maps-grounding)\n  * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)\n  * [URL context](https://ai.google.dev/gemini-api/docs/url-context)\n  * [Computer Use](https://ai.google.dev/gemini-api/docs/computer-use)\n  * [File Search](https://ai.google.dev/gemini-api/docs/file-search)\n  * Live API\n  * [Get started](https://ai.google.dev/gemini-api/docs/live)\n  * [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)\n  * [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)\n  * [Session management](https://ai.google.dev/gemini-api/docs/live-session)\n  * [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)\n  * Guides\n  * [Batch API](https://ai.google.dev/gemini-api/docs/batch-api)\n  * [Files API](https://ai.google.dev/gemini-api/docs/files)\n  * [Context caching](https://ai.google.dev/gemini-api/docs/caching)\n  * [Media resolution](https://ai.google.dev/gemini-api/docs/media-resolution)\n  * [Token counting](https://ai.google.dev/gemini-api/docs/tokens)\n  * [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n  * Logs and datasets\n    * [Get started with logs](https://ai.google.dev/gemini-api/docs/logs-datasets)\n    * [Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)\n  * Safety\n    * [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)\n    * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)\n  * Open-Source frameworks\n    * [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)\n    * [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)\n    * [LlamaIndex](https://ai.google.dev/gemini-api/docs/llama-index)\n    * [Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)\n  * Resources\n  * [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)\n  * [Release notes](https://ai.google.dev/gemini-api/docs/changelog)\n  * [Deprecations](https://ai.google.dev/gemini-api/docs/deprecations)\n  * [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)\n  * [Billing info](https://ai.google.dev/gemini-api/docs/billing)\n  * Google AI Studio\n    * [Google AI Studio quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)\n    * [LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)\n    * [AI Studio troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)\n    * [Google Workspace](https://ai.google.dev/gemini-api/docs/workspace)\n  * Google Cloud Platform\n    * [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)\n    * [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)\n  * Policies\n  * [Terms of service](https://ai.google.dev/gemini-api/terms)\n  * [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)\n  * [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)\n\n\nGemini 3 is here. Read the [developer guide](https://ai.google.dev/gemini-api/docs/gemini-3) to get started with our most advanced model yet. \n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Gemini API Docs ](https://ai.google.dev/gemini-api/docs)\n\n\nSend feedback \n#  Troubleshooting guide\nUse this guide to help you diagnose and resolve common issues that arise when you call the Gemini API. You may encounter issues from either the Gemini API backend service or the client SDKs. Our client SDKs are open sourced in the following repositories:\n  * [python-genai](https://github.com/googleapis/python-genai)\n  * [js-genai](https://github.com/googleapis/js-genai)\n  * [go-genai](https://github.com/googleapis/go-genai)\n\n\nIf you encounter API key issues, verify that you have set up your API key correctly per the [API key setup guide](https://ai.google.dev/gemini-api/docs/api-key).\n## Gemini API backend service error codes\nThe following table lists common backend error codes you may encounter, along with explanations for their causes and troubleshooting steps:\n**HTTP Code** |  **Status** |  **Description** |  **Example** |  **Solution**  \n---|---|---|---|---  \n400  | INVALID_ARGUMENT | The request body is malformed. | There is a typo, or a missing required field in your request. | Check the [API reference](https://ai.google.dev/api) for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors.  \n400  | FAILED_PRECONDITION | Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. | You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. | To use the Gemini API, you will need to setup a paid plan using [Google AI Studio](https://aistudio.google.com/app/apikey).  \n403  | PERMISSION_DENIED | Your API key doesn't have the required permissions. | You are using the wrong API key; you are trying to use a tuned model without going through [proper authentication](https://ai.google.dev/docs/model-tuning/tutorial?lang=python#set_up_authentication). | Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models.  \n404  | NOT_FOUND | The requested resource wasn't found. | An image, audio, or video file referenced in your request was not found. | Check if all [parameters in your request are valid](https://ai.google.dev/docs/troubleshooting#check-api) for your API version.  \n429  | RESOURCE_EXHAUSTED | You've exceeded the rate limit. | You are sending too many requests per minute with the free tier Gemini API. | Verify that you're within the model's [rate limit](https://ai.google.dev/gemini-api/docs/rate-limits). [Request a quota increase](https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase) if needed.  \n500  | INTERNAL | An unexpected error occurred on Google's side. | Your input context is too long. | Reduce your input context or temporarily switch to another model (e.g. from Gemini 2.5 Pro to Gemini 2.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.  \n503  | UNAVAILABLE | The service may be temporarily overloaded or down. | The service is temporarily running out of capacity. | Temporarily switch to another model (e.g. from Gemini 2.5 Pro to Gemini 2.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.  \n504  | DEADLINE_EXCEEDED | The service is unable to finish processing within the deadline. | Your prompt (or context) is too large to be processed in time. | Set a larger 'timeout' in your client request to avoid this error.  \n## Check your API calls for model parameter errors\nVerify that your model parameters are within the following values:\n**Model parameter** |  **Values (range)**  \n---|---  \nCandidate count  | 1-8 (integer)  \nTemperature  | 0.0-1.0  \nMax output tokens  |  Use `get_model` ([Python](https://ai.google.dev/api/python/google/generativeai/get_model)) to determine the maximum number of tokens for the model you are using.   \nTopP  | 0.0-1.0  \nIn addition to checking parameter values, make sure you're using the correct [API version](https://ai.google.dev/gemini-api/docs/api-versions) (e.g., `/v1` or `/v1beta`) and model that supports the features you need. For example, if a feature is in Beta release, it will only be available in the `/v1beta` API version.\n## Check if you have the right model\nVerify that you are using a supported model listed on our [models page](https://ai.google.dev/gemini-api/docs/models/gemini).\n## Higher latency or token usage with 2.5 models\nIf you're observing higher latency or token usage with the 2.5 Flash and Pro models, this can be because they come with **thinking is enabled by default** in order to enhance quality. If you are prioritizing speed or need to minimize costs, you can adjust or disable thinking.\nRefer to [thinking page](https://ai.google.dev/gemini-api/docs/thinking#set-budget) for guidance and sample code.\n## Safety issues\nIf you see a prompt was blocked because of a safety setting in your API call, review the prompt with respect to the filters you set in the API call.\nIf you see `BlockedReason.OTHER`, the query or response may violate the [terms of service](https://ai.google.dev/terms) or be otherwise unsupported.\n## Recitation issue\nIf you see the model stops generating output due to the RECITATION reason, this means the model output may resemble certain data. To fix this, try to make prompt / context as unique as possible and use a higher temperature.\nWhen using Gemini 3 models, we strongly recommend keeping the `temperature` at its default value of 1.0. Changing the temperature (setting it below 1.0) may lead to unexpected behavior, such as looping or degraded performance, particularly in complex mathematical or reasoning tasks. \n## Repetitive tokens issue\nIf you see repeated output tokens, try the following suggestions to help reduce or eliminate them.\nDescription | Cause | Suggested workaround  \n---|---|---  \nRepeated hyphens in Markdown tables |  This can occur when the contents of the table are long as the model tries to create a visually aligned Markdown table. However, the alignment in Markdown is not necessary for correct rendering.  |  Add instructions in your prompt to give the model specific guidelines for generating Markdown tables. Provide examples that follow those guidelines. You can also try adjusting the temperature. For generating code or very structured output like Markdown tables, high temperature have shown to work better (>= 0.8). The following is an example set of guidelines you can add to your prompt to prevent this issue:  ```\n          # Markdown Table Format\n          \n          * Separator line: Markdown tables must include a separator line below\n            the header row. The separator line must use only 3 hyphens per\n            column, for example: |---|---|---|. Using more hypens like\n            ----, -----, ------ can result in errors. Always\n            use |:---|, |---:|, or |---| in these separator strings.\n\n            For example:\n\n            | Date | Description | Attendees |\n            |---|---|---|\n            | 2024-10-26 | Annual Conference | 500 |\n            | 2025-01-15 | Q1 Planning Session | 25 |\n\n          * Alignment: Do not align columns. Always use |---|.\n            For three columns, use |---|---|---| as the separator line.\n            For four columns use |---|---|---|---| and so on.\n\n          * Conciseness: Keep cell content brief and to the point.\n\n          * Never pad column headers or other cells with lots of spaces to\n            match with width of other content. Only a single space on each side\n            is needed. For example, always do \"| column name |\" instead of\n            \"| column name                |\". Extra spaces are wasteful.\n            A markdown renderer will automatically take care displaying\n            the content in a visually appealing form.\n        \n```\n  \nRepeated tokens in Markdown tables  |  Similar to the repeated hyphens, this occurs when the model tries to visually align the contents of the table. The alignment in Markdown is not required for correct rendering.  | \n  * Try adding instructions like the following to your system prompt: ```\n            FOR TABLE HEADINGS, IMMEDIATELY ADD ' |' AFTER THE TABLE HEADING.\n          \n```\n\n  * Try adjusting the temperature. Higher temperatures (>= 0.8) generally helps to eliminate repetitions or duplication in the output. \n\n  \nRepeated newlines (`\\n`) in structured output  |  When the model input contains unicode or escape sequences like `\\u` or `\\t`, it can lead to repeated newlines.  | \n  * Check for and replace forbidden escape sequences with UTF-8 characters in your prompt. For example, `\\u` escape sequence in your JSON examples can cause the model to use them in its output too. \n  * Instruct the model on allowed escapes. Add a system instruction like this: ```\n            In quoted strings, the only allowed escape sequences are \\\\, \\n, and \\\". Instead of \\u escapes, use UTF-8.\n          \n```\n\n\n  \nRepeated text in using structured output  |  When the model output has a different order for the fields than the defined structured schema, this can lead to repeating text.  | \n  * Don't specify the order of fields in your prompt. \n  * Make all output fields required. \n\n  \nRepetitive tool calling  |  This can occur if the model loses the context of previous thoughts and/or call an unavailable endpoint that it's forced to.  |  Instruct the model to maintain state within its thought process. Add this to the end of your system instructions: ```\n        When thinking silently: ALWAYS start the thought with a brief\n        (one sentence) recap of the current progress on the task. In\n        particular, consider whether the task is already done.\n      \n```\n  \nRepetitive text that's not part of structured output  |  This can occur if the model gets stuck on a request that it can't resolve.  | \n  * If thinking is turned on, avoid giving explicit orders for how to think through a problem in the instructions. Just ask for the final output. \n  * Try a higher temperature >= 0.8. \n  * Add instructions like \"Be concise\", \"Don't repeat yourself\", or \"Provide the answer once\". \n\n  \n## Improve model output\nFor higher quality model outputs, explore writing more structured prompts. The [prompt engineering guide](https://ai.google.dev/gemini-api/docs/prompting-strategies) page introduces some basic concepts, strategies, and best practices to get you started.\n## Understand token limits\nRead through our [Token guide](https://ai.google.dev/gemini-api/docs/tokens) to better understand how to count tokens and their limits.\n## Known issues\n  * The API supports only a number of select languages. Submitting prompts in unsupported languages can produce unexpected or even blocked responses. See [available languages](https://ai.google.dev/gemini-api/docs/models#supported-languages) for updates.\n\n\n## File a bug\nJoin the discussion on the [Google AI developer forum](https://discuss.ai.google.dev) if you have questions.\nSend feedback \nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-27 UTC.\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-11-27 UTC.\"],[],[]] \n  * [ Terms ](https://policies.google.com/terms)\n  * [ Privacy ](https://policies.google.com/privacy)\n  * [ Manage cookies ](https://ai.google.dev/gemini-api/docs/troubleshooting)\n\n\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Polski\n  * Português – Brasil\n  * Shqip\n  * Tiếng Việt\n  * Türkçe\n  * Русский\n  * עברית\n  * العربيّة\n  * فارسی\n  * हिंदी\n  * বাংলা\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n\n\n\n\n# Content from https://ai.google.dev/gemini-api/docs/models/gemini\n\n[ Skip to main content ](https://ai.google.dev/gemini-api/docs/models#main-content)\n[ ![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg) ](https://ai.google.dev/)\n`/`\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Polski\n  * Português – Brasil\n  * Shqip\n  * Tiếng Việt\n  * Türkçe\n  * Русский\n  * עברית\n  * العربيّة\n  * فارسی\n  * हिंदी\n  * বাংলা\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n[ Get API key ](https://aistudio.google.com/apikey) [ Cookbook ](https://github.com/google-gemini/cookbook) [ Community ](https://discuss.ai.google.dev/c/gemini-api/) Sign in\n[ Gemini API Docs ](https://ai.google.dev/gemini-api/docs) [ API reference ](https://ai.google.dev/api)\n[ ![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg) ](https://ai.google.dev/)\n  * [ Gemini API  ](https://ai.google.dev/gemini-api/docs)\n    * [ Gemini API Docs  ](https://ai.google.dev/gemini-api/docs)\n    * [ API reference  ](https://ai.google.dev/api)\n  * [ Get API key  ](https://aistudio.google.com/apikey)\n  * [ Cookbook  ](https://github.com/google-gemini/cookbook)\n  * [ Community  ](https://discuss.ai.google.dev/c/gemini-api/)\n\n\n  * Get started\n  * [Overview](https://ai.google.dev/gemini-api/docs)\n  * [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n  * [API keys](https://ai.google.dev/gemini-api/docs/api-key)\n  * [Vibe code in Build mode](https://ai.google.dev/gemini-api/docs/aistudio-build-mode)\n  * [Libraries](https://ai.google.dev/gemini-api/docs/libraries)\n  * [OpenAI compatibility](https://ai.google.dev/gemini-api/docs/openai)\n  * Models\n  * [Gemini](https://ai.google.dev/gemini-api/docs/models)\n  * [Gemini 3](https://ai.google.dev/gemini-api/docs/gemini-3)\n  * [Imagen (image generation)](https://ai.google.dev/gemini-api/docs/imagen)\n  * [Veo (video generation)](https://ai.google.dev/gemini-api/docs/video)\n  * [Lyria (music generation)](https://ai.google.dev/gemini-api/docs/music-generation)\n  * [Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)\n  * [Robotics](https://ai.google.dev/gemini-api/docs/robotics-overview)\n  * [Pricing](https://ai.google.dev/gemini-api/docs/pricing)\n  * [Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n  * Core capabilities\n  * [Text](https://ai.google.dev/gemini-api/docs/text-generation)\n  * Image\n    * [Image generation 🍌](https://ai.google.dev/gemini-api/docs/image-generation)\n    * [Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)\n  * [Video](https://ai.google.dev/gemini-api/docs/video-understanding)\n  * [Documents](https://ai.google.dev/gemini-api/docs/document-processing)\n  * Speech and audio\n    * [Speech generation](https://ai.google.dev/gemini-api/docs/speech-generation)\n    * [Audio understanding](https://ai.google.dev/gemini-api/docs/audio)\n  * Thinking and thought signatures\n    * [Thinking](https://ai.google.dev/gemini-api/docs/thinking)\n    * [Thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)\n  * [Structured outputs](https://ai.google.dev/gemini-api/docs/structured-output)\n  * [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)\n  * [Long context](https://ai.google.dev/gemini-api/docs/long-context)\n  * Tools\n  * [Overview](https://ai.google.dev/gemini-api/docs/tools)\n  * [Google Search](https://ai.google.dev/gemini-api/docs/google-search)\n  * [Google Maps](https://ai.google.dev/gemini-api/docs/maps-grounding)\n  * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)\n  * [URL context](https://ai.google.dev/gemini-api/docs/url-context)\n  * [Computer Use](https://ai.google.dev/gemini-api/docs/computer-use)\n  * [File Search](https://ai.google.dev/gemini-api/docs/file-search)\n  * Live API\n  * [Get started](https://ai.google.dev/gemini-api/docs/live)\n  * [Capabilities](https://ai.google.dev/gemini-api/docs/live-guide)\n  * [Tool use](https://ai.google.dev/gemini-api/docs/live-tools)\n  * [Session management](https://ai.google.dev/gemini-api/docs/live-session)\n  * [Ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)\n  * Guides\n  * [Batch API](https://ai.google.dev/gemini-api/docs/batch-api)\n  * [Files API](https://ai.google.dev/gemini-api/docs/files)\n  * [Context caching](https://ai.google.dev/gemini-api/docs/caching)\n  * [Media resolution](https://ai.google.dev/gemini-api/docs/media-resolution)\n  * [Token counting](https://ai.google.dev/gemini-api/docs/tokens)\n  * [Prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-strategies)\n  * Logs and datasets\n    * [Get started with logs](https://ai.google.dev/gemini-api/docs/logs-datasets)\n    * [Data logging and sharing](https://ai.google.dev/gemini-api/docs/logs-policy)\n  * Safety\n    * [Safety settings](https://ai.google.dev/gemini-api/docs/safety-settings)\n    * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance)\n  * Open-Source frameworks\n    * [LangChain & LangGraph](https://ai.google.dev/gemini-api/docs/langgraph-example)\n    * [CrewAI](https://ai.google.dev/gemini-api/docs/crewai-example)\n    * [LlamaIndex](https://ai.google.dev/gemini-api/docs/llama-index)\n    * [Vercel AI SDK](https://ai.google.dev/gemini-api/docs/vercel-ai-sdk-example)\n  * Resources\n  * [Migrate to Gen AI SDK](https://ai.google.dev/gemini-api/docs/migrate)\n  * [Release notes](https://ai.google.dev/gemini-api/docs/changelog)\n  * [Deprecations](https://ai.google.dev/gemini-api/docs/deprecations)\n  * [API troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshooting)\n  * [Billing info](https://ai.google.dev/gemini-api/docs/billing)\n  * Google AI Studio\n    * [Google AI Studio quickstart](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart)\n    * [LearnLM](https://ai.google.dev/gemini-api/docs/learnlm)\n    * [AI Studio troubleshooting](https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio)\n    * [Google Workspace](https://ai.google.dev/gemini-api/docs/workspace)\n  * Google Cloud Platform\n    * [VertexAI Gemini API](https://ai.google.dev/gemini-api/docs/migrate-to-cloud)\n    * [OAuth authentication](https://ai.google.dev/gemini-api/docs/oauth)\n  * Policies\n  * [Terms of service](https://ai.google.dev/gemini-api/terms)\n  * [Available regions](https://ai.google.dev/gemini-api/docs/available-regions)\n  * [Additional usage polices](https://ai.google.dev/gemini-api/docs/usage-policies)\n\n\nGemini 3 is here. Read the [developer guide](https://ai.google.dev/gemini-api/docs/gemini-3) to get started with our most advanced model yet. \n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Gemini API Docs ](https://ai.google.dev/gemini-api/docs)\n\n\nSend feedback \n#  Gemini models\nOUR MOST INTELLIGENT MODEL\n## Gemini 3 Pro\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning. \n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-3-pro-preview)\n#### Model details\n### Gemini 3 Pro Preview\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-3-pro-preview`  \nsaveSupported data types |  **Inputs** Text, Image, Video, Audio, and PDF **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Supported **Function calling** Supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * `Preview: gemini-3-pro-preview`\n\n  \ncalendar_monthLatest update | November 2025  \ncognition_2Knowledge cutoff | January 2025  \n### Gemini 3 Pro Image Preview\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-3-pro-image-preview`  \nsaveSupported data types |  **Inputs** Image and Text **Output** Image and Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 65,536 **Output token limit** 32,768  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Not supported **Code execution** Not supported **File search** Not supported **Function calling** Not supported **Grounding with Google Maps** Not supported **Image generation** Supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * `Preview: gemini-3-pro-image-preview`\n\n  \ncalendar_monthLatest update | November 2025  \ncognition_2Knowledge cutoff | January 2025  \nOUR ADVANCED THINKING MODEL\n## Gemini 2.5 Pro\nOur state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro)\n#### Model details\n### Gemini 2.5 Pro\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, text, and PDF **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Supported **Function calling** Supported **Grounding with Google Maps** Supported **Image generation** Not supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * `Stable: gemini-2.5-pro`\n\n  \ncalendar_monthLatest update | June 2025  \ncognition_2Knowledge cutoff | January 2025  \n### Gemini 2.5 Pro TTS\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-pro-preview-tts`  \nsaveSupported data types |  **Inputs** Text **Output** Audio  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 8,192 **Output token limit** 16,384  \nhandymanCapabilities |  **Audio generation** Supported **Batch API** Not Supported **Caching** Not supported **Code execution** Not supported **File search** Not Supported **Function calling** Not supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Not supported **Search grounding** Not supported **Structured outputs** Not supported **Thinking** Not supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * `gemini-2.5-pro-preview-tts`\n\n  \ncalendar_monthLatest update | May 2025  \nFAST AND INTELLIGENT\n## Gemini 2.5 Flash\nOur best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. \n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash)\n#### Model details\n### Gemini 2.5 Flash\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-flash`  \nsaveSupported data types |  **Inputs** Text, images, video, audio **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Supported **Function calling** Supported **Grounding with Google Maps** Supported **Image generation** Not supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Stable: `gemini-2.5-flash`\n\n  \ncalendar_monthLatest update | June 2025  \ncognition_2Knowledge cutoff | January 2025  \n### Gemini 2.5 Flash Preview\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-flash-preview-09-2025`  \nsaveSupported data types |  **Inputs** Text, images, video, audio **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Supported **Function calling** Supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL Context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Preview: `gemini-2.5-flash-preview-09-2025`\n\n  \ncalendar_monthLatest update | September 2025  \ncognition_2Knowledge cutoff | January 2025  \n### Gemini 2.5 Flash Image\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-flash-image`  \nsaveSupported data types |  **Inputs** Images and text **Output** Images and text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 65,536 **Output token limit** 32,768  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Not Supported **File search** Not Supported **Function calling** Not supported **Grounding with Google Maps** Not supported **Image generation** Supported **Live API** Not Supported **Search grounding** Not Supported **Structured outputs** Supported **Thinking** Not Supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Stable: `gemini-2.5-flash-image`\n  * Preview: `gemini-2.5-flash-image-preview`\n\n  \ncalendar_monthLatest update | October 2025  \ncognition_2Knowledge cutoff | June 2025  \n### Gemini 2.5 Flash Live\nProperty | Description  \n---|---  \nid_cardModel code |  `gemini-2.5-flash-native-audio-preview-09-2025`  \nsaveSupported data types |  **Inputs** Audio, video, text **Output** Audio and text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 131,072 **Output token limit** 8,192  \nhandymanCapabilities |  **Audio generation** Supported **Batch API** Not supported **Caching** Not supported **Code execution** Not supported **File search** Not Supported **Function calling** Supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Supported **Search grounding** Supported **Structured outputs** Not supported **Thinking** Supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Preview: `gemini-2.5-flash-native-audio-preview-09-2025`\n  * Preview: `gemini-live-2.5-flash-preview`\ngemini-live-2.5-flash-preview will be deprecated on December 09, 2025 \n  \ncalendar_monthLatest update | September 2025  \ncognition_2Knowledge cutoff | January 2025  \n### Gemini 2.5 Flash TTS\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-flash-preview-tts`  \nsaveSupported data types |  **Inputs** Text **Output** Audio  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 8,192 **Output token limit** 16,384  \nhandymanCapabilities |  **Audio generation** Supported **Batch API** Supported **Caching** Not supported **Code execution** Not supported **File search** Not Supported **Function calling** Not supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Not supported **Search grounding** Not supported **Structured outputs** Not supported **Thinking** Not supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * `gemini-2.5-flash-preview-tts`\n\n  \ncalendar_monthLatest update | May 2025  \nULTRA FAST\n## Gemini 2.5 Flash-Lite\nOur fastest flash model optimized for cost-efficiency and high throughput. \n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite)\n#### Model details\n### Gemini 2.5 Flash-Lite\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-flash-lite`  \nsaveSupported data types |  **Inputs** Text, image, video, audio, PDF **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Supported **Function calling** Supported **Grounding with Google Maps** Supported **Image generation** Not supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Stable: `gemini-2.5-flash-lite`\n\n  \ncalendar_monthLatest update | July 2025  \ncognition_2Knowledge cutoff | January 2025  \n### Gemini 2.5 Flash-Lite Preview\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.5-flash-lite-preview-09-2025`  \nsaveSupported data types |  **Inputs** Text, image, video, audio, PDF **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Supported **Function calling** Supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Not supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Supported **URL context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Preview: `gemini-2.5-flash-lite-preview-09-2025`\n\n  \ncalendar_monthLatest update | September 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n\n## Previous Gemini models\nOUR SECOND GENERATION WORKHORSE MODEL\n## Gemini 2.0 Flash\nOur second generation workhorse model, with a 1 million token context window. \n### Expand to learn more\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. \n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash)\n#### Model details\n### Gemini 2.0 Flash\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Supported **File search** Not supported **Function calling** Supported **Grounding with Google Maps** Supported **Image generation** Not supported **Live API** Supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Experimental **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n### Gemini 2.0 Flash Image\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.0-flash-preview-image-generation`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text and images  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 32,768 **Output token limit** 8,192  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Not Supported **File search** Not supported **Function calling** Not supported **Grounding with Google Maps** Not supported **Image generation** Supported **Live API** Not Supported **Search grounding** Not Supported **Structured outputs** Supported **Thinking** Not Supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Preview: `gemini-2.0-flash-preview-image-generation`\ngemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa \n  \ncalendar_monthLatest update | May 2025  \ncognition_2Knowledge cutoff | August 2024  \n### Gemini 2.0 Flash Live\nProperty | Description  \n---|---  \nid_cardModel code |  `gemini-2.0-flash-live-001` gemini-2.0-flash-live-001 will be deprecated on December 09, 2025  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Audio generation** Supported **Batch API** Not supported **Caching** Not supported **Code execution** Supported **File search** Not supported **Function calling** Supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Supported **Search grounding** Supported **Structured outputs** Supported **Thinking** Not supported **URL context** Supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \nOUR SECOND GENERATION FAST MODEL\n## Gemini 2.0 Flash-Lite\nOur second generation small workhorse model, with a 1 million token context window. \n### Expand to learn more\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n#### Model details\nProperty | Description  \n---|---  \nid_cardModel code | `gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[[*]](https://ai.google.dev/gemini-api/docs/tokens) |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Audio generation** Not supported **Batch API** Supported **Caching** Supported **Code execution** Not supported **File search** Not supported **Function calling** Supported **Grounding with Google Maps** Not supported **Image generation** Not supported **Live API** Not supported **Search grounding** Not supported **Structured outputs** Supported **Thinking** Not Supported **URL context** Not supported  \n123Versions |  Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details.\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n\n## Model version name patterns\nGemini models are available in either _stable_ , _preview_ , _latest_ , or _experimental_ versions.\n**Note:** The following list refers to the model string naming convention as of September, 2025. Models released prior to that may have different naming conventions. Refer to the exact model string if you are using an older model.\n### Stable\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\nFor example: `gemini-2.5-flash`.\n### Preview\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\nFor example: `gemini-2.5-flash-preview-09-2025`.\n### Latest\nPoints to the latest release for a specific model variation. This can be a stable, preview or experimental release. This alias will get hot-swapped with every new release of a specific model variation. A **2-week notice** will be provided through email before the version behind latest is changed.\nFor example: `gemini-flash-latest`.\n### Experimental\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\nExperimental models are not stable and availability of model endpoints is subject to change.\n## Model deprecations\nFor information about model deprecations, visit the [Gemini deprecations](https://ai.google.dev/gemini-api/docs/deprecations) page.\nSend feedback \nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-22 UTC.\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-11-22 UTC.\"],[],[]] \n  * [ Terms ](https://policies.google.com/terms)\n  * [ Privacy ](https://policies.google.com/privacy)\n  * [ Manage cookies ](https://ai.google.dev/gemini-api/docs/models)\n\n\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Polski\n  * Português – Brasil\n  * Shqip\n  * Tiếng Việt\n  * Türkçe\n  * Русский\n  * עברית\n  * العربيّة\n  * فارسی\n  * हिंदी\n  * বাংলা\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n  * 日本語\n  * 한국어\n\n\n\n\n"
                        }
                      }
                    }
                  ],
                  "role": "user"
                }
              },
              {
                "author": "Code_Surgeon_Agent",
                "content": {
                  "parts": [
                    {
                      "function_call": {
                        "id": "call_25900051",
                        "args": {
                          "key": "packages"
                        },
                        "name": "retrieve_context"
                      }
                    },
                    {
                      "function_call": {
                        "id": "call_46726235",
                        "args": {
                          "key": "versions"
                        },
                        "name": "retrieve_context"
                      }
                    }
                  ],
                  "role": "model"
                }
              },
              {
                "author": "Code_Surgeon_Agent",
                "content": {
                  "parts": [
                    {
                      "function_response": {
                        "id": "call_25900051",
                        "name": "retrieve_context",
                        "response": {
                          "result": "Not found"
                        }
                      }
                    },
                    {
                      "function_response": {
                        "id": "call_46726235",
                        "name": "retrieve_context",
                        "response": {
                          "result": "Not found"
                        }
                      }
                    }
                  ],
                  "role": "user"
                }
              }
            ]
          },
          "creation_timestamp": 1764251702.76641
        }
      ],
      "session_input": {
        "app_name": "src",
        "user_id": "user"
      },
      "creation_timestamp": 1764252074.5245645
    },
    {
      "eval_id": "casec20b4a",
      "conversation": [
        {
          "invocation_id": "e-cbbd83ac-f98a-4122-9cfd-cf8f52d3f4d4",
          "user_content": {
            "parts": [
              {
                "text": "https://github.com/google/adk-python/issues/3726 i got the similar issue"
              }
            ],
            "role": "user"
          },
          "final_response": {
            "parts": [
              {
                "text": "Verification Passed"
              }
            ],
            "role": "model"
          },
          "intermediate_data": {
            "invocation_events": [
              {
                "author": "Web_Crawl_Agent",
                "content": {
                  "parts": [
                    {
                      "function_call": {
                        "id": "call_50080286",
                        "args": {
                          "urls": [
                            "https://github.com/google/adk-python/issues/3726"
                          ]
                        },
                        "name": "batch_crawl_tool"
                      }
                    },
                    {
                      "function_call": {
                        "id": "call_81397642",
                        "args": {
                          "urls": [
                            "https://github.com/google/adk-python"
                          ]
                        },
                        "name": "batch_crawl_tool"
                      }
                    },
                    {
                      "function_call": {
                        "id": "call_89754013",
                        "args": {
                          "url": "https://github.com/google/adk-python",
                          "query": "response_evaluator.py float item error"
                        },
                        "name": "adaptive_crawl_tool"
                      }
                    },
                    {
                      "function_call": {
                        "id": "call_59130861",
                        "args": {
                          "url": "https://github.com/google/adk-python/issues",
                          "query": "float int mismatch AttributeError item"
                        },
                        "name": "adaptive_crawl_tool"
                      }
                    }
                  ],
                  "role": "model"
                }
              },
              {
                "author": "Web_Crawl_Agent",
                "content": {
                  "parts": [
                    {
                      "function_response": {
                        "id": "call_50080286",
                        "name": "batch_crawl_tool",
                        "response": {
                          "result": "# Content from https://github.com/google/adk-python/issues/3726\n\n[Skip to content](https://github.com/google/adk-python/issues/3726#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues%2F3726)\nAppearance settings\n  * Platform\n    * AI CODE CREATION\n      * [ GitHub CopilotWrite better code with AI ](https://github.com/features/copilot)\n      * [ GitHub SparkBuild and deploy intelligent apps ](https://github.com/features/spark)\n      * [ GitHub ModelsManage and compare prompts ](https://github.com/features/models)\n      * [ MCP RegistryNewIntegrate external tools ](https://github.com/mcp)\n    * DEVELOPER WORKFLOWS\n      * [ ActionsAutomate any workflow ](https://github.com/features/actions)\n      * [ CodespacesInstant dev environments ](https://github.com/features/codespaces)\n      * [ IssuesPlan and track work ](https://github.com/features/issues)\n      * [ Code ReviewManage code changes ](https://github.com/features/code-review)\n    * APPLICATION SECURITY\n      * [ GitHub Advanced SecurityFind and fix vulnerabilities ](https://github.com/security/advanced-security)\n      * [ Code securitySecure your code as you build ](https://github.com/security/advanced-security/code-security)\n      * [ Secret protectionStop leaks before they start ](https://github.com/security/advanced-security/secret-protection)\n    * EXPLORE\n      * [Why GitHub](https://github.com/why-github)\n      * [Documentation](https://docs.github.com)\n      * [Blog](https://github.blog)\n      * [Changelog](https://github.blog/changelog)\n      * [Marketplace](https://github.com/marketplace)\n[View all features](https://github.com/features)\n  * Solutions\n    * BY COMPANY SIZE\n      * [Enterprises](https://github.com/enterprise)\n      * [Small and medium teams](https://github.com/team)\n      * [Startups](https://github.com/enterprise/startups)\n      * [Nonprofits](https://github.com/solutions/industry/nonprofits)\n    * BY USE CASE\n      * [App Modernization](https://github.com/solutions/use-case/app-modernization)\n      * [DevSecOps](https://github.com/solutions/use-case/devsecops)\n      * [DevOps](https://github.com/solutions/use-case/devops)\n      * [CI/CD](https://github.com/solutions/use-case/ci-cd)\n      * [View all use cases](https://github.com/solutions/use-case)\n    * BY INDUSTRY\n      * [Healthcare](https://github.com/solutions/industry/healthcare)\n      * [Financial services](https://github.com/solutions/industry/financial-services)\n      * [Manufacturing](https://github.com/solutions/industry/manufacturing)\n      * [Government](https://github.com/solutions/industry/government)\n      * [View all industries](https://github.com/solutions/industry)\n[View all solutions](https://github.com/solutions)\n  * Resources\n    * EXPLORE BY TOPIC\n      * [AI](https://github.com/resources/articles?topic=ai)\n      * [Software Development](https://github.com/resources/articles?topic=software-development)\n      * [DevOps](https://github.com/resources/articles?topic=devops)\n      * [Security](https://github.com/resources/articles?topic=security)\n      * [View all topics](https://github.com/resources/articles)\n    * EXPLORE BY TYPE\n      * [Customer stories](https://github.com/customer-stories)\n      * [Events & webinars](https://github.com/resources/events)\n      * [Ebooks & reports](https://github.com/resources/whitepapers)\n      * [Business insights](https://github.com/solutions/executive-insights)\n      * [GitHub Skills](https://skills.github.com)\n    * SUPPORT & SERVICES\n      * [Documentation](https://docs.github.com)\n      * [Customer support](https://support.github.com)\n      * [Community forum](https://github.com/orgs/community/discussions)\n      * [Trust center](https://github.com/trust-center)\n      * [Partners](https://github.com/partners)\n  * Open Source\n    * COMMUNITY\n      * [ GitHub SponsorsFund open source developers ](https://github.com/sponsors)\n    * PROGRAMS\n      * [Security Lab](https://securitylab.github.com)\n      * [Maintainer Community](https://maintainers.github.com)\n      * [Accelerator](https://github.com/accelerator)\n      * [Archive Program](https://archiveprogram.github.com)\n    * REPOSITORIES\n      * [Topics](https://github.com/topics)\n      * [Trending](https://github.com/trending)\n      * [Collections](https://github.com/collections)\n  * Enterprise\n    * ENTERPRISE SOLUTIONS\n      * [ Enterprise platformAI-powered developer platform ](https://github.com/enterprise)\n    * AVAILABLE ADD-ONS\n      * [ GitHub Advanced SecurityEnterprise-grade security features ](https://github.com/security/advanced-security)\n      * [ Copilot for BusinessEnterprise-grade AI features ](https://github.com/features/copilot/copilot-business)\n      * [ Premium SupportEnterprise-grade 24/7 support ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues%2F3726)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&source=header-repo&source_repo=google%2Fadk-python)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/google/adk-python/issues/3726) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/google/adk-python/issues/3726) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/google/adk-python/issues/3726) to refresh your session. Dismiss alert\n{{ message }}\n[ google ](https://github.com/google) / **[adk-python](https://github.com/google/adk-python) ** Public\n  * [ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ 2.5k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n  * [ Star  15.7k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n\n\n  * [ Code ](https://github.com/google/adk-python)\n  * [ Issues 354 ](https://github.com/google/adk-python/issues)\n  * [ Pull requests 102 ](https://github.com/google/adk-python/pulls)\n  * [ Discussions ](https://github.com/google/adk-python/discussions)\n  * [ Actions ](https://github.com/google/adk-python/actions)\n  * [ Projects 0 ](https://github.com/google/adk-python/projects)\n  * [ Security ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n### [ Uh oh!  ](https://github.com/google/adk-python/security)\n[There was an error while loading. ](https://github.com/google/adk-python/security)[Please reload this page](https://github.com/google/adk-python/issues/3726).\n  * [ Insights ](https://github.com/google/adk-python/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/google/adk-python)\n  * [ Issues  ](https://github.com/google/adk-python/issues)\n  * [ Pull requests  ](https://github.com/google/adk-python/pulls)\n  * [ Discussions  ](https://github.com/google/adk-python/discussions)\n  * [ Actions  ](https://github.com/google/adk-python/actions)\n  * [ Projects  ](https://github.com/google/adk-python/projects)\n  * [ Security  ](https://github.com/google/adk-python/security)\n  * [ Insights  ](https://github.com/google/adk-python/pulse)\n\n\n#  Ollama and Gemini Model Integration #3726\n[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues/3726)\nCopy link\n[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues/3726)\nCopy link\n[Bug](https://github.com/google/adk-python/issues?q=type:\"Bug\")\n[Ollama and Gemini Model Integration](https://github.com/google/adk-python/issues/3726#top)#3726\nBug\nCopy link\nAssignees\n[![wyf7107](https://avatars.githubusercontent.com/u/14008276?s=64&v=4)](https://github.com/wyf7107)[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)](https://github.com/surajksharma07)\nLabels\n[models[Component] Issues related to model support](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22models%22)[Component] Issues related to model support[need attention[Status] This issue is important but the maintainers haven't started working on it](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22need%20attention%22)[Status] This issue is important but the maintainers haven't started working on it\n[![@Yashwant00CR7](https://avatars.githubusercontent.com/u/146008508?u=bcaf596a8cff7207f0941ee4b2b7190c7e045ecd&v=4&size=80)](https://github.com/Yashwant00CR7)\n## Description\n[![@Yashwant00CR7](https://avatars.githubusercontent.com/u/146008508?u=bcaf596a8cff7207f0941ee4b2b7190c7e045ecd&v=4&size=48)](https://github.com/Yashwant00CR7)\n[Yashwant00CR7](https://github.com/Yashwant00CR7)\nopened [on Nov 26, 2025](https://github.com/google/adk-python/issues/3726#issue-3668587947)\nIssue body actions\n## While Integrating the _Gemini and Ollama_ model using the LiteLlm through a _**Sequential Agent**_ Igot an error like this\n`_APIConnectionError( 2271 message=\"{} - {}\".format(exception_provider, error_str), 2272 llm_provider=custom_llm_provider,APIConnectionError: litellm.APIConnectionError: Ollama_chatException - {\"error\":\"json: cannot unmarshal array into Go struct field ChatRequest.messages.content of type string\"}_`\n## But when I Used\n### `model=LiteLlm(model=\"openai/qwen2.5:7b\")`\n## Instead of\n#### `model=LiteLlm(model=\"ollama_chat/qwen2.5:7b\")`\n#### My error Got solved and my Sequential Agent was Working Fine\nJust Wanted to Help !!!!\n## Activity\n[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4&size=80)](https://github.com/surajksharma07)\n### surajksharma07 commented on Nov 27, 2025 \n[![@surajksharma07](https://avatars.githubusercontent.com/u/228440606?u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4&size=48)](https://github.com/surajksharma07)\n[surajksharma07](https://github.com/surajksharma07)\n[on Nov 27, 2025](https://github.com/google/adk-python/issues/3726#issuecomment-3582748147)\nCollaborator\nMore actions\n[@Yashwant00CR7](https://github.com/Yashwant00CR7) The issue is that Ollama's API expects message content to always be a string, but when using SequentialAgent or multi-part content, ADK sends an array format (which works for OpenAI but breaks Ollama).\nYour workaround using `openai/qwen2.5:7b` instead of `ollama_chat/qwen2.5:7b` is the correct solution - this makes LiteLLM use OpenAI-compatible formatting which Ollama supports.\nFor a permanent fix, we'd need to add provider-specific content formatting in ADK to automatically flatten arrays to strings when detecting Ollama.\nThanks for sharing this workaround.\n[![](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)surajksharma07](https://github.com/surajksharma07)\nassigned \n[surajksharma07](https://github.com/surajksharma07)\nand\n[wyf7107](https://github.com/wyf7107)\n[on Nov 27, 2025](https://github.com/google/adk-python/issues/3726#event-21203144503)\n[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4&size=80)](https://github.com/surajksharma07)\n### surajksharma07 commented on Nov 27, 2025 \n[![@surajksharma07](https://avatars.githubusercontent.com/u/228440606?u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4&size=48)](https://github.com/surajksharma07)\n[surajksharma07](https://github.com/surajksharma07)\n[on Nov 27, 2025](https://github.com/google/adk-python/issues/3726#issuecomment-3582751810)\nCollaborator\nMore actions\n[@wyf7107](https://github.com/wyf7107) Please have a look into it.\n[![](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)surajksharma07](https://github.com/surajksharma07)\nadded \n[models[Component] Issues related to model support](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22models%22)[Component] Issues related to model support\n[on Nov 27, 2025](https://github.com/google/adk-python/issues/3726#event-21203175242)\n[![](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)surajksharma07](https://github.com/surajksharma07)\nadded the\n[Bug](https://github.com/google/adk-python/issues?q=type:\"Bug\")\nissue type [on Nov 27, 2025](https://github.com/google/adk-python/issues/3726#event-21203176671)\n[![](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)surajksharma07](https://github.com/surajksharma07)\nadded \n[need attention[Status] This issue is important but the maintainers haven't started working on it](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22need%20attention%22)[Status] This issue is important but the maintainers haven't started working on it\n[on Nov 27, 2025](https://github.com/google/adk-python/issues/3726#event-21203183794)\n[Sign up for free](https://github.com/signup?return_to=https://github.com/google/adk-python/issues/3726)**to join this conversation on GitHub.** Already have an account? [Sign in to comment](https://github.com/login?return_to=https://github.com/google/adk-python/issues/3726)\n## Metadata\n## Metadata\n### Assignees\n  * [![@surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)surajksharma07](https://github.com/surajksharma07)\n  * [![@wyf7107](https://avatars.githubusercontent.com/u/14008276?s=64&v=4)wyf7107](https://github.com/wyf7107)\n\n\n### Labels\n[models[Component] Issues related to model support](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22models%22)[Component] Issues related to model support[need attention[Status] This issue is important but the maintainers haven't started working on it](https://github.com/google/adk-python/issues?q=state%3Aopen%20label%3A%22need%20attention%22)[Status] This issue is important but the maintainers haven't started working on it\n### Type\n[Bug](https://github.com/google/adk-python/issues?q=type:\"Bug\")\n### Projects\nNo projects\n### Milestone\nNo milestone\n### Relationships\nNone yet\n### Development\nCode with agent mode\nSelect code repository\nNo branches or pull requests\n### Participants\n[![@wyf7107](https://avatars.githubusercontent.com/u/14008276?s=64&v=4)](https://github.com/wyf7107)[![@Yashwant00CR7](https://avatars.githubusercontent.com/u/146008508?s=64&u=bcaf596a8cff7207f0941ee4b2b7190c7e045ecd&v=4)](https://github.com/Yashwant00CR7)[![@surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)](https://github.com/surajksharma07)\n## Issue actions\n## Footer\n[ ](https://github.com) © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Community](https://github.community/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nOllama and Gemini Model Integration · Issue #3726 · google/adk-python\n\n\n"
                        }
                      }
                    },
                    {
                      "function_response": {
                        "id": "call_81397642",
                        "name": "batch_crawl_tool",
                        "response": {
                          "result": "# Content from https://github.com/google/adk-python\n\n[Skip to content](https://github.com/google/adk-python#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python)\nAppearance settings\n  * Platform\n    * AI CODE CREATION\n      * [ GitHub CopilotWrite better code with AI ](https://github.com/features/copilot)\n      * [ GitHub SparkBuild and deploy intelligent apps ](https://github.com/features/spark)\n      * [ GitHub ModelsManage and compare prompts ](https://github.com/features/models)\n      * [ MCP RegistryNewIntegrate external tools ](https://github.com/mcp)\n    * DEVELOPER WORKFLOWS\n      * [ ActionsAutomate any workflow ](https://github.com/features/actions)\n      * [ CodespacesInstant dev environments ](https://github.com/features/codespaces)\n      * [ IssuesPlan and track work ](https://github.com/features/issues)\n      * [ Code ReviewManage code changes ](https://github.com/features/code-review)\n    * APPLICATION SECURITY\n      * [ GitHub Advanced SecurityFind and fix vulnerabilities ](https://github.com/security/advanced-security)\n      * [ Code securitySecure your code as you build ](https://github.com/security/advanced-security/code-security)\n      * [ Secret protectionStop leaks before they start ](https://github.com/security/advanced-security/secret-protection)\n    * EXPLORE\n      * [Why GitHub](https://github.com/why-github)\n      * [Documentation](https://docs.github.com)\n      * [Blog](https://github.blog)\n      * [Changelog](https://github.blog/changelog)\n      * [Marketplace](https://github.com/marketplace)\n[View all features](https://github.com/features)\n  * Solutions\n    * BY COMPANY SIZE\n      * [Enterprises](https://github.com/enterprise)\n      * [Small and medium teams](https://github.com/team)\n      * [Startups](https://github.com/enterprise/startups)\n      * [Nonprofits](https://github.com/solutions/industry/nonprofits)\n    * BY USE CASE\n      * [App Modernization](https://github.com/solutions/use-case/app-modernization)\n      * [DevSecOps](https://github.com/solutions/use-case/devsecops)\n      * [DevOps](https://github.com/solutions/use-case/devops)\n      * [CI/CD](https://github.com/solutions/use-case/ci-cd)\n      * [View all use cases](https://github.com/solutions/use-case)\n    * BY INDUSTRY\n      * [Healthcare](https://github.com/solutions/industry/healthcare)\n      * [Financial services](https://github.com/solutions/industry/financial-services)\n      * [Manufacturing](https://github.com/solutions/industry/manufacturing)\n      * [Government](https://github.com/solutions/industry/government)\n      * [View all industries](https://github.com/solutions/industry)\n[View all solutions](https://github.com/solutions)\n  * Resources\n    * EXPLORE BY TOPIC\n      * [AI](https://github.com/resources/articles?topic=ai)\n      * [Software Development](https://github.com/resources/articles?topic=software-development)\n      * [DevOps](https://github.com/resources/articles?topic=devops)\n      * [Security](https://github.com/resources/articles?topic=security)\n      * [View all topics](https://github.com/resources/articles)\n    * EXPLORE BY TYPE\n      * [Customer stories](https://github.com/customer-stories)\n      * [Events & webinars](https://github.com/resources/events)\n      * [Ebooks & reports](https://github.com/resources/whitepapers)\n      * [Business insights](https://github.com/solutions/executive-insights)\n      * [GitHub Skills](https://skills.github.com)\n    * SUPPORT & SERVICES\n      * [Documentation](https://docs.github.com)\n      * [Customer support](https://support.github.com)\n      * [Community forum](https://github.com/orgs/community/discussions)\n      * [Trust center](https://github.com/trust-center)\n      * [Partners](https://github.com/partners)\n  * Open Source\n    * COMMUNITY\n      * [ GitHub SponsorsFund open source developers ](https://github.com/sponsors)\n    * PROGRAMS\n      * [Security Lab](https://securitylab.github.com)\n      * [Maintainer Community](https://maintainers.github.com)\n      * [Accelerator](https://github.com/accelerator)\n      * [Archive Program](https://archiveprogram.github.com)\n    * REPOSITORIES\n      * [Topics](https://github.com/topics)\n      * [Trending](https://github.com/trending)\n      * [Collections](https://github.com/collections)\n  * Enterprise\n    * ENTERPRISE SOLUTIONS\n      * [ Enterprise platformAI-powered developer platform ](https://github.com/enterprise)\n    * AVAILABLE ADD-ONS\n      * [ GitHub Advanced SecurityEnterprise-grade security features ](https://github.com/security/advanced-security)\n      * [ Copilot for BusinessEnterprise-grade AI features ](https://github.com/features/copilot/copilot-business)\n      * [ Premium SupportEnterprise-grade 24/7 support ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=google%2Fadk-python)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/google/adk-python) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/google/adk-python) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/google/adk-python) to refresh your session. Dismiss alert\n{{ message }}\n[ google ](https://github.com/google) / **[adk-python](https://github.com/google/adk-python) ** Public\n  * [ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ 2.5k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n  * [ Star  15.7k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n\n\nAn open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. \n[google.github.io/adk-docs/](https://google.github.io/adk-docs/ \"https://google.github.io/adk-docs/\")\n### License\n[ ](https://github.com/google/adk-python/blob/main/LICENSE)\n[ 15.7k stars ](https://github.com/google/adk-python/stargazers) [ 2.5k forks ](https://github.com/google/adk-python/forks) [ Branches ](https://github.com/google/adk-python/branches) [ Tags ](https://github.com/google/adk-python/tags) [ Activity ](https://github.com/google/adk-python/activity)\n[ Star  ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n[ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ Code ](https://github.com/google/adk-python)\n  * [ Issues 354 ](https://github.com/google/adk-python/issues)\n  * [ Pull requests 102 ](https://github.com/google/adk-python/pulls)\n  * [ Discussions ](https://github.com/google/adk-python/discussions)\n  * [ Actions ](https://github.com/google/adk-python/actions)\n  * [ Projects 0 ](https://github.com/google/adk-python/projects)\n  * [ Security ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n### [ Uh oh!  ](https://github.com/google/adk-python/security)\n[There was an error while loading. ](https://github.com/google/adk-python/security)[Please reload this page](https://github.com/google/adk-python).\n  * [ Insights ](https://github.com/google/adk-python/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/google/adk-python)\n  * [ Issues  ](https://github.com/google/adk-python/issues)\n  * [ Pull requests  ](https://github.com/google/adk-python/pulls)\n  * [ Discussions  ](https://github.com/google/adk-python/discussions)\n  * [ Actions  ](https://github.com/google/adk-python/actions)\n  * [ Projects  ](https://github.com/google/adk-python/projects)\n  * [ Security  ](https://github.com/google/adk-python/security)\n  * [ Insights  ](https://github.com/google/adk-python/pulse)\n\n\n# google/adk-python\nmain\n[Branches](https://github.com/google/adk-python/branches)[Tags](https://github.com/google/adk-python/tags)\n[](https://github.com/google/adk-python/branches)[](https://github.com/google/adk-python/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\nName | Name | Last commit message | Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[1,754 Commits](https://github.com/google/adk-python/commits/main/)[](https://github.com/google/adk-python/commits/main/)  \n[.gemini](https://github.com/google/adk-python/tree/main/.gemini \".gemini\") | [.gemini](https://github.com/google/adk-python/tree/main/.gemini \".gemini\") |  |   \n[.github](https://github.com/google/adk-python/tree/main/.github \".github\") | [.github](https://github.com/google/adk-python/tree/main/.github \".github\") |  |   \n[assets](https://github.com/google/adk-python/tree/main/assets \"assets\") | [assets](https://github.com/google/adk-python/tree/main/assets \"assets\") |  |   \n[contributing](https://github.com/google/adk-python/tree/main/contributing \"contributing\") | [contributing](https://github.com/google/adk-python/tree/main/contributing \"contributing\") |  |   \n[scripts](https://github.com/google/adk-python/tree/main/scripts \"scripts\") | [scripts](https://github.com/google/adk-python/tree/main/scripts \"scripts\") |  |   \n[src/google/adk](https://github.com/google/adk-python/tree/main/src/google/adk \"This path skips through empty directories\") | [src/google/adk](https://github.com/google/adk-python/tree/main/src/google/adk \"This path skips through empty directories\") |  |   \n[tests](https://github.com/google/adk-python/tree/main/tests \"tests\") | [tests](https://github.com/google/adk-python/tree/main/tests \"tests\") |  |   \n[.gitignore](https://github.com/google/adk-python/blob/main/.gitignore \".gitignore\") | [.gitignore](https://github.com/google/adk-python/blob/main/.gitignore \".gitignore\") |  |   \n[AGENTS.md](https://github.com/google/adk-python/blob/main/AGENTS.md \"AGENTS.md\") | [AGENTS.md](https://github.com/google/adk-python/blob/main/AGENTS.md \"AGENTS.md\") |  |   \n[CHANGELOG.md](https://github.com/google/adk-python/blob/main/CHANGELOG.md \"CHANGELOG.md\") | [CHANGELOG.md](https://github.com/google/adk-python/blob/main/CHANGELOG.md \"CHANGELOG.md\") |  |   \n[CONTRIBUTING.md](https://github.com/google/adk-python/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\") | [CONTRIBUTING.md](https://github.com/google/adk-python/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\") |  |   \n[LICENSE](https://github.com/google/adk-python/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/google/adk-python/blob/main/LICENSE \"LICENSE\") |  |   \n[README.md](https://github.com/google/adk-python/blob/main/README.md \"README.md\") | [README.md](https://github.com/google/adk-python/blob/main/README.md \"README.md\") |  |   \n[autoformat.sh](https://github.com/google/adk-python/blob/main/autoformat.sh \"autoformat.sh\") | [autoformat.sh](https://github.com/google/adk-python/blob/main/autoformat.sh \"autoformat.sh\") |  |   \n[llms-full.txt](https://github.com/google/adk-python/blob/main/llms-full.txt \"llms-full.txt\") | [llms-full.txt](https://github.com/google/adk-python/blob/main/llms-full.txt \"llms-full.txt\") |  |   \n[llms.txt](https://github.com/google/adk-python/blob/main/llms.txt \"llms.txt\") | [llms.txt](https://github.com/google/adk-python/blob/main/llms.txt \"llms.txt\") |  |   \n[pylintrc](https://github.com/google/adk-python/blob/main/pylintrc \"pylintrc\") | [pylintrc](https://github.com/google/adk-python/blob/main/pylintrc \"pylintrc\") |  |   \n[pyproject.toml](https://github.com/google/adk-python/blob/main/pyproject.toml \"pyproject.toml\") | [pyproject.toml](https://github.com/google/adk-python/blob/main/pyproject.toml \"pyproject.toml\") |  |   \nView all files  \n## Repository files navigation\n  * [README](https://github.com/google/adk-python)\n  * [Code of conduct](https://github.com/google/adk-python)\n  * [Contributing](https://github.com/google/adk-python)\n  * [Apache-2.0 license](https://github.com/google/adk-python)\n  * [Security](https://github.com/google/adk-python)\n\n\n# Agent Development Kit (ADK)\n[](https://github.com/google/adk-python#agent-development-kit-adk)\n[![License](https://camo.githubusercontent.com/5b60841bea9e11d9d0b0950d690c9bc554e06385634056a7d5d62a15d1a4eabe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667)](https://github.com/google/adk-python/blob/main/LICENSE) [![PyPI](https://camo.githubusercontent.com/831aae963ba63f223d13c586fc6b3f2b620cec90a438f1a09c43a6be1e8d5f22/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f676f6f676c652d61646b)](https://pypi.org/project/google-adk/) [![Python Unit Tests](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg)](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml) [![r/agentdevelopmentkit](https://camo.githubusercontent.com/804c84103b652f4f9ffcceb4914840adca00dfeec97b4fea152a38aaccc16225/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5265646469742d722532466167656e74646576656c6f706d656e746b69742d4646343530303f7374796c653d666c6174266c6f676f3d726564646974266c6f676f436f6c6f723d7768697465)](https://www.reddit.com/r/agentdevelopmentkit/) [![Ask Code Wiki](https://camo.githubusercontent.com/3002e59856dd402678cfd2c64e5f8154749cb8bfe99df56ba8486253c269f3c2/68747470733a2f2f7777772e677374617469632e636f6d2f5f2f626f712d73646c632d6167656e74732d75692f5f2f722f4d766f7367346b6c4341342e737667)](https://codewiki.google/github.com/google/adk-python)\n##  [![](https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png)](https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png)\n[](https://github.com/google/adk-python#----------)\n###  An open-source, code-first Python framework for building, evaluating, and deploying sophisticated AI agents with flexibility and control. \n[](https://github.com/google/adk-python#------an-open-source-code-first-python-framework-for-building-evaluating-and-deploying-sophisticated-ai-agents-with-flexibility-and-control----)\n###  Important Links: [Docs](https://google.github.io/adk-docs/), [Samples](https://github.com/google/adk-samples), [Java ADK](https://github.com/google/adk-java), [Go ADK](https://github.com/google/adk-go) & [ADK Web](https://github.com/google/adk-web). \n[](https://github.com/google/adk-python#------important-links------docs------samples------java-adk------go-adk-------adk-web----)\nAgent Development Kit (ADK) is a flexible and modular framework that applies software development principles to AI agent creation. It is designed to simplify building, deploying, and orchestrating agent workflows, from simple tasks to complex systems. While optimized for Gemini, ADK is model-agnostic, deployment-agnostic, and compatible with other frameworks.\n* * *\n## 🔥 What's new\n[](https://github.com/google/adk-python#-whats-new)\n  * **Custom Service Registration** : Add a service registry to provide a generic way to register custom service implementations to be used in FastAPI server. See [short instruction](https://github.com/google/adk-python/discussions/3175#discussioncomment-14745120). ([391628f](https://github.com/google/adk-python/commit/391628fcdc7b950c6835f64ae3ccab197163c990))\n  * **Rewind** : Add the ability to rewind a session to before a previous invocation ([9dce06f](https://github.com/google/adk-python/commit/9dce06f9b00259ec42241df4f6638955e783a9d1)).\n  * **New CodeExecutor** : Introduces a new AgentEngineSandboxCodeExecutor class that supports executing agent-generated code using the Vertex AI Code Execution Sandbox API ([ee39a89](https://github.com/google/adk-python/commit/ee39a891106316b790621795b5cc529e89815a98))\n\n\n## ✨ Key Features\n[](https://github.com/google/adk-python#-key-features)\n  * **Rich Tool Ecosystem** : Utilize pre-built tools, custom functions, OpenAPI specs, MCP tools or integrate existing tools to give agents diverse capabilities, all for tight integration with the Google ecosystem.\n  * **Code-First Development** : Define agent logic, tools, and orchestration directly in Python for ultimate flexibility, testability, and versioning.\n  * **Agent Config** : Build agents without code. Check out the [Agent Config](https://google.github.io/adk-docs/agents/config/) feature.\n  * **Tool Confirmation** : A [tool confirmation flow(HITL)](https://google.github.io/adk-docs/tools/confirmation/) that can guard tool execution with explicit confirmation and custom input.\n  * **Modular Multi-Agent Systems** : Design scalable applications by composing multiple specialized agents into flexible hierarchies.\n  * **Deploy Anywhere** : Easily containerize and deploy agents on Cloud Run or scale seamlessly with Vertex AI Agent Engine.\n\n\n## 🚀 Installation\n[](https://github.com/google/adk-python#-installation)\n### Stable Release (Recommended)\n[](https://github.com/google/adk-python#stable-release-recommended)\nYou can install the latest stable version of ADK using `pip`:\n```\npip install google-adk\n```\n\nThe release cadence is roughly bi-weekly.\nThis version is recommended for most users as it represents the most recent official release.\n### Development Version\n[](https://github.com/google/adk-python#development-version)\nBug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven't been included in an official PyPI release yet, you can install directly from the main branch:\n```\npip install git+https://github.com/google/adk-python.git@main\n```\n\nNote: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.\n## 🤖 Agent2Agent (A2A) Protocol and ADK Integration\n[](https://github.com/google/adk-python#-agent2agent-a2a-protocol-and-adk-integration)\nFor remote agent-to-agent communication, ADK integrates with the [A2A protocol](https://github.com/google-a2a/A2A/). See this [example](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents) for how they can work together.\n## 📚 Documentation\n[](https://github.com/google/adk-python#-documentation)\nExplore the full documentation for detailed guides on building, evaluating, and deploying agents:\n  * **[Documentation](https://google.github.io/adk-docs)**\n\n\n## 🏁 Feature Highlight\n[](https://github.com/google/adk-python#-feature-highlight)\n### Define a single agent:\n[](https://github.com/google/adk-python#define-a-single-agent)\n```\nfrom google.adk.agents import Agent\nfrom google.adk.tools import google_search\n\nroot_agent = Agent(\n    name=\"search_assistant\",\n    model=\"gemini-2.5-flash\", # Or your preferred Gemini model\n    instruction=\"You are a helpful assistant. Answer user questions using Google Search when needed.\",\n    description=\"An assistant that can search the web.\",\n    tools=[google_search]\n)\n```\n\n### Define a multi-agent system:\n[](https://github.com/google/adk-python#define-a-multi-agent-system)\nDefine a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.\n```\nfrom google.adk.agents import LlmAgent, BaseAgent\n\n# Define individual agents\ngreeter = LlmAgent(name=\"greeter\", model=\"gemini-2.5-flash\", ...)\ntask_executor = LlmAgent(name=\"task_executor\", model=\"gemini-2.5-flash\", ...)\n\n# Create parent agent and assign children via sub_agents\ncoordinator = LlmAgent(\n    name=\"Coordinator\",\n    model=\"gemini-2.5-flash\",\n    description=\"I coordinate greetings and tasks.\",\n    sub_agents=[ # Assign sub_agents here\n        greeter,\n        task_executor\n    ]\n)\n```\n\n### Development UI\n[](https://github.com/google/adk-python#development-ui)\nA built-in development UI to help you test, evaluate, debug, and showcase your agent(s).\n[![](https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png)](https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png)\n### Evaluate Agents\n[](https://github.com/google/adk-python#evaluate-agents)\n```\nadk eval \\\n    samples_for_testing/hello_world \\\n    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json\n```\n\n## 🤝 Contributing\n[](https://github.com/google/adk-python#-contributing)\nWe welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our\n  * [General contribution guideline and flow](https://google.github.io/adk-docs/contributing-guide/).\n  * Then if you want to contribute code, please read [Code Contributing Guidelines](https://github.com/google/adk-python/blob/main/CONTRIBUTING.md) to get started.\n\n\n## Community Repo\n[](https://github.com/google/adk-python#community-repo)\nWe have [adk-python-community repo](https://github.com/google/adk-python-community)that is home to a growing ecosystem of community-contributed tools, third-party service integrations, and deployment scripts that extend the core capabilities of the ADK.\n## Vibe Coding\n[](https://github.com/google/adk-python#vibe-coding)\nIf you are to develop agent via vibe coding the [llms.txt](https://github.com/google/adk-python/blob/main/llms.txt) and the [llms-full.txt](https://github.com/google/adk-python/blob/main/llms-full.txt) can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.\n## Community Events\n[](https://github.com/google/adk-python#community-events)\n  * [Completed] ADK's 1st community meeting on Wednesday, October 15, 2025. Remember to [join our group](https://groups.google.com/g/adk-community) to get access to the [recording](https://drive.google.com/file/d/1rpXDq5NSH8-MyMeYI6_5pZ3Lhn0X9BQf/view), and [deck](https://docs.google.com/presentation/d/1_b8LG4xaiadbUUDzyNiapSFyxanc9ZgFdw7JQ6zmZ9Q/edit?slide=id.g384e60cdaca_0_658&resourcekey=0-tjFFv0VBQhpXBPCkZr0NOg#slide=id.g384e60cdaca_0_658).\n\n\n## 📄 License\n[](https://github.com/google/adk-python#-license)\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/google/adk-python/blob/main/LICENSE) file for details.\n* * *\n_Happy Agent Building!_\n## About\nAn open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. \n[google.github.io/adk-docs/](https://google.github.io/adk-docs/ \"https://google.github.io/adk-docs/\")\n### Topics\n[ agent ](https://github.com/topics/agent \"Topic: agent\") [ ai ](https://github.com/topics/ai \"Topic: ai\") [ multi-agent ](https://github.com/topics/multi-agent \"Topic: multi-agent\") [ agents ](https://github.com/topics/agents \"Topic: agents\") [ multi-agent-systems ](https://github.com/topics/multi-agent-systems \"Topic: multi-agent-systems\") [ ai-agents ](https://github.com/topics/ai-agents \"Topic: ai-agents\") [ multi-agents ](https://github.com/topics/multi-agents \"Topic: multi-agents\") [ llm ](https://github.com/topics/llm \"Topic: llm\") [ llms ](https://github.com/topics/llms \"Topic: llms\") [ genai ](https://github.com/topics/genai \"Topic: genai\") [ agentic ](https://github.com/topics/agentic \"Topic: agentic\") [ genai-chatbot ](https://github.com/topics/genai-chatbot \"Topic: genai-chatbot\") [ agentic-ai ](https://github.com/topics/agentic-ai \"Topic: agentic-ai\") [ multi-agents-collaboration ](https://github.com/topics/multi-agents-collaboration \"Topic: multi-agents-collaboration\") [ aiagentframework ](https://github.com/topics/aiagentframework \"Topic: aiagentframework\") [ agents-sdk ](https://github.com/topics/agents-sdk \"Topic: agents-sdk\")\n### Resources\n[ ](https://github.com/google/adk-python#readme-ov-file)\n### License\n[ ](https://github.com/google/adk-python#Apache-2.0-1-ov-file)\n### Code of conduct\n[ ](https://github.com/google/adk-python#coc-ov-file)\n### Contributing\n[ ](https://github.com/google/adk-python#contributing-ov-file)\n### Security policy\n[ ](https://github.com/google/adk-python#security-ov-file)\n###  Uh oh! \nThere was an error while loading. [Please reload this page](https://github.com/google/adk-python).\n[Activity](https://github.com/google/adk-python/activity)\n[Custom properties](https://github.com/google/adk-python/custom-properties)\n### Stars\n[**15.7k** stars](https://github.com/google/adk-python/stargazers)\n### Watchers\n[**137** watching](https://github.com/google/adk-python/watchers)\n### Forks\n[**2.5k** forks](https://github.com/google/adk-python/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python&report=google+%28user%29)\n##  [Releases 30](https://github.com/google/adk-python/releases)\n[ 1.19.0 Latest  Nov 20, 2025 ](https://github.com/google/adk-python/releases/tag/v1.19.0)\n[+ 29 releases](https://github.com/google/adk-python/releases)\n##  [Packages 0](https://github.com/orgs/google/packages?repo_name=adk-python)\nNo packages published   \n\n##  [Used by 2.8k](https://github.com/google/adk-python/network/dependents)\n[\n  * ![@WRLDInc](https://avatars.githubusercontent.com/u/56140573?s=64&v=4)\n  * ![@Klavis-AI](https://avatars.githubusercontent.com/u/205720652?s=64&v=4)\n  * ![@restatedev](https://avatars.githubusercontent.com/u/97341811?s=64&v=4)\n  * ![@Devvekariya711](https://avatars.githubusercontent.com/u/207506349?s=64&v=4)\n  * ![@Jineshkumar1](https://avatars.githubusercontent.com/u/85137150?s=64&v=4)\n  * ![@nextlinktechnology](https://avatars.githubusercontent.com/u/60595633?s=64&v=4)\n  * ![@abdulrahman305](https://avatars.githubusercontent.com/u/131770526?s=64&v=4)\n  * ![@Langdock](https://avatars.githubusercontent.com/u/136317085?s=64&v=4)\n\n+ 2,819  ](https://github.com/google/adk-python/network/dependents)\n##  [Contributors 206](https://github.com/google/adk-python/graphs/contributors)\n  * [ ![@seanzhougoogle](https://avatars.githubusercontent.com/u/69162095?s=64&v=4) ](https://github.com/seanzhougoogle)\n  * [ ![@google-genai-bot](https://avatars.githubusercontent.com/u/194307901?s=64&v=4) ](https://github.com/google-genai-bot)\n  * [ ![@hangfei](https://avatars.githubusercontent.com/u/5482153?s=64&v=4) ](https://github.com/hangfei)\n  * [ ![@Jacksunwei](https://avatars.githubusercontent.com/u/1281348?s=64&v=4) ](https://github.com/Jacksunwei)\n  * [ ![@DeanChensj](https://avatars.githubusercontent.com/u/9414763?s=64&v=4) ](https://github.com/DeanChensj)\n  * [ ![@xuanyang15](https://avatars.githubusercontent.com/u/13991296?s=64&v=4) ](https://github.com/xuanyang15)\n  * [ ![@GWeale](https://avatars.githubusercontent.com/u/51099682?s=64&v=4) ](https://github.com/GWeale)\n  * [ ![@ankursharmas](https://avatars.githubusercontent.com/u/5462261?s=64&v=4) ](https://github.com/ankursharmas)\n  * [ ![@wuliang229](https://avatars.githubusercontent.com/u/18244712?s=64&v=4) ](https://github.com/wuliang229)\n  * [ ![@wyf7107](https://avatars.githubusercontent.com/u/14008276?s=64&v=4) ](https://github.com/wyf7107)\n  * [ ![@selcukgun](https://avatars.githubusercontent.com/u/5055758?s=64&v=4) ](https://github.com/selcukgun)\n  * [ ![@wukath](https://avatars.githubusercontent.com/u/108756731?s=64&v=4) ](https://github.com/wukath)\n  * [ ![@copybara-github](https://avatars.githubusercontent.com/u/32201506?s=64&v=4) ](https://github.com/copybara-github)\n  * [ ![@XinranTang](https://avatars.githubusercontent.com/u/33537857?s=64&v=4) ](https://github.com/XinranTang)\n\n\n[+ 192 contributors](https://github.com/google/adk-python/graphs/contributors)\n## Languages\n  * [ Python 94.8% ](https://github.com/google/adk-python/search?l=python)\n  * [ Jupyter Notebook 4.3% ](https://github.com/google/adk-python/search?l=jupyter-notebook)\n  * Other 0.9%\n\n\n## Footer\n[ ](https://github.com) © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Community](https://github.community/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\n\n\n"
                        }
                      }
                    },
                    {
                      "function_response": {
                        "id": "call_89754013",
                        "name": "adaptive_crawl_tool",
                        "response": {
                          "result": "# Adaptive Result from https://github.com/google/adk-python\n\n[Skip to content](https://github.com/google/adk-python#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python)\nAppearance settings\n  * Platform\n    * AI CODE CREATION\n      * [ GitHub CopilotWrite better code with AI ](https://github.com/features/copilot)\n      * [ GitHub SparkBuild and deploy intelligent apps ](https://github.com/features/spark)\n      * [ GitHub ModelsManage and compare prompts ](https://github.com/features/models)\n      * [ MCP RegistryNewIntegrate external tools ](https://github.com/mcp)\n    * DEVELOPER WORKFLOWS\n      * [ ActionsAutomate any workflow ](https://github.com/features/actions)\n      * [ CodespacesInstant dev environments ](https://github.com/features/codespaces)\n      * [ IssuesPlan and track work ](https://github.com/features/issues)\n      * [ Code ReviewManage code changes ](https://github.com/features/code-review)\n    * APPLICATION SECURITY\n      * [ GitHub Advanced SecurityFind and fix vulnerabilities ](https://github.com/security/advanced-security)\n      * [ Code securitySecure your code as you build ](https://github.com/security/advanced-security/code-security)\n      * [ Secret protectionStop leaks before they start ](https://github.com/security/advanced-security/secret-protection)\n    * EXPLORE\n      * [Why GitHub](https://github.com/why-github)\n      * [Documentation](https://docs.github.com)\n      * [Blog](https://github.blog)\n      * [Changelog](https://github.blog/changelog)\n      * [Marketplace](https://github.com/marketplace)\n[View all features](https://github.com/features)\n  * Solutions\n    * BY COMPANY SIZE\n      * [Enterprises](https://github.com/enterprise)\n      * [Small and medium teams](https://github.com/team)\n      * [Startups](https://github.com/enterprise/startups)\n      * [Nonprofits](https://github.com/solutions/industry/nonprofits)\n    * BY USE CASE\n      * [App Modernization](https://github.com/solutions/use-case/app-modernization)\n      * [DevSecOps](https://github.com/solutions/use-case/devsecops)\n      * [DevOps](https://github.com/solutions/use-case/devops)\n      * [CI/CD](https://github.com/solutions/use-case/ci-cd)\n      * [View all use cases](https://github.com/solutions/use-case)\n    * BY INDUSTRY\n      * [Healthcare](https://github.com/solutions/industry/healthcare)\n      * [Financial services](https://github.com/solutions/industry/financial-services)\n      * [Manufacturing](https://github.com/solutions/industry/manufacturing)\n      * [Government](https://github.com/solutions/industry/government)\n      * [View all industries](https://github.com/solutions/industry)\n[View all solutions](https://github.com/solutions)\n  * Resources\n    * EXPLORE BY TOPIC\n      * [AI](https://github.com/resources/articles?topic=ai)\n      * [Software Development](https://github.com/resources/articles?topic=software-development)\n      * [DevOps](https://github.com/resources/articles?topic=devops)\n      * [Security](https://github.com/resources/articles?topic=security)\n      * [View all topics](https://github.com/resources/articles)\n    * EXPLORE BY TYPE\n      * [Customer stories](https://github.com/customer-stories)\n      * [Events & webinars](https://github.com/resources/events)\n      * [Ebooks & reports](https://github.com/resources/whitepapers)\n      * [Business insights](https://github.com/solutions/executive-insights)\n      * [GitHub Skills](https://skills.github.com)\n    * SUPPORT & SERVICES\n      * [Documentation](https://docs.github.com)\n      * [Customer support](https://support.github.com)\n      * [Community forum](https://github.com/orgs/community/discussions)\n      * [Trust center](https://github.com/trust-center)\n      * [Partners](https://github.com/partners)\n  * Open Source\n    * COMMUNITY\n      * [ GitHub SponsorsFund open source developers ](https://github.com/sponsors)\n    * PROGRAMS\n      * [Security Lab](https://securitylab.github.com)\n      * [Maintainer Community](https://maintainers.github.com)\n      * [Accelerator](https://github.com/accelerator)\n      * [Archive Program](https://archiveprogram.github.com)\n    * REPOSITORIES\n      * [Topics](https://github.com/topics)\n      * [Trending](https://github.com/trending)\n      * [Collections](https://github.com/collections)\n  * Enterprise\n    * ENTERPRISE SOLUTIONS\n      * [ Enterprise platformAI-powered developer platform ](https://github.com/enterprise)\n    * AVAILABLE ADD-ONS\n      * [ GitHub Advanced SecurityEnterprise-grade security features ](https://github.com/security/advanced-security)\n      * [ Copilot for BusinessEnterprise-grade AI features ](https://github.com/features/copilot/copilot-business)\n      * [ Premium SupportEnterprise-grade 24/7 support ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=google%2Fadk-python)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/google/adk-python) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/google/adk-python) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/google/adk-python) to refresh your session. Dismiss alert\n{{ message }}\n[ google ](https://github.com/google) / **[adk-python](https://github.com/google/adk-python) ** Public\n  * [ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ 2.5k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n  * [ Star  15.7k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n\n\nAn open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. \n[google.github.io/adk-docs/](https://google.github.io/adk-docs/ \"https://google.github.io/adk-docs/\")\n### License\n[ ](https://github.com/google/adk-python/blob/main/LICENSE)\n[ 15.7k stars ](https://github.com/google/adk-python/stargazers) [ 2.5k forks ](https://github.com/google/adk-python/forks) [ Branches ](https://github.com/google/adk-python/branches) [ Tags ](https://github.com/google/adk-python/tags) [ Activity ](https://github.com/google/adk-python/activity)\n[ Star  ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n[ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ Code ](https://github.com/google/adk-python)\n  * [ Issues 354 ](https://github.com/google/adk-python/issues)\n  * [ Pull requests 102 ](https://github.com/google/adk-python/pulls)\n  * [ Discussions ](https://github.com/google/adk-python/discussions)\n  * [ Actions ](https://github.com/google/adk-python/actions)\n  * [ Projects 0 ](https://github.com/google/adk-python/projects)\n  * [ Security ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n### [ Uh oh!  ](https://github.com/google/adk-python/security)\n[There was an error while loading. ](https://github.com/google/adk-python/security)[Please reload this page](https://github.com/google/adk-python).\n  * [ Insights ](https://github.com/google/adk-python/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/google/adk-python)\n  * [ Issues  ](https://github.com/google/adk-python/issues)\n  * [ Pull requests  ](https://github.com/google/adk-python/pulls)\n  * [ Discussions  ](https://github.com/google/adk-python/discussions)\n  * [ Actions  ](https://github.com/google/adk-python/actions)\n  * [ Projects  ](https://github.com/google/adk-python/projects)\n  * [ Security  ](https://github.com/google/adk-python/security)\n  * [ Insights  ](https://github.com/google/adk-python/pulse)\n\n\n# google/adk-python\nmain\n[Branches](https://github.com/google/adk-python/branches)[Tags](https://github.com/google/adk-python/tags)\n[](https://github.com/google/adk-python/branches)[](https://github.com/google/adk-python/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\nName | Name | Last commit message | Last commit date  \n---|---|---|---  \n## Latest commit\n## History\n[1,754 Commits](https://github.com/google/adk-python/commits/main/)[](https://github.com/google/adk-python/commits/main/)  \n[.gemini](https://github.com/google/adk-python/tree/main/.gemini \".gemini\") | [.gemini](https://github.com/google/adk-python/tree/main/.gemini \".gemini\") |  |   \n[.github](https://github.com/google/adk-python/tree/main/.github \".github\") | [.github](https://github.com/google/adk-python/tree/main/.github \".github\") |  |   \n[assets](https://github.com/google/adk-python/tree/main/assets \"assets\") | [assets](https://github.com/google/adk-python/tree/main/assets \"assets\") |  |   \n[contributing](https://github.com/google/adk-python/tree/main/contributing \"contributing\") | [contributing](https://github.com/google/adk-python/tree/main/contributing \"contributing\") |  |   \n[scripts](https://github.com/google/adk-python/tree/main/scripts \"scripts\") | [scripts](https://github.com/google/adk-python/tree/main/scripts \"scripts\") |  |   \n[src/google/adk](https://github.com/google/adk-python/tree/main/src/google/adk \"This path skips through empty directories\") | [src/google/adk](https://github.com/google/adk-python/tree/main/src/google/adk \"This path skips through empty directories\") |  |   \n[tests](https://github.com/google/adk-python/tree/main/tests \"tests\") | [tests](https://github.com/google/adk-python/tree/main/tests \"tests\") |  |   \n[.gitignore](https://github.com/google/adk-python/blob/main/.gitignore \".gitignore\") | [.gitignore](https://github.com/google/adk-python/blob/main/.gitignore \".gitignore\") |  |   \n[AGENTS.md](https://github.com/google/adk-python/blob/main/AGENTS.md \"AGENTS.md\") | [AGENTS.md](https://github.com/google/adk-python/blob/main/AGENTS.md \"AGENTS.md\") |  |   \n[CHANGELOG.md](https://github.com/google/adk-python/blob/main/CHANGELOG.md \"CHANGELOG.md\") | [CHANGELOG.md](https://github.com/google/adk-python/blob/main/CHANGELOG.md \"CHANGELOG.md\") |  |   \n[CONTRIBUTING.md](https://github.com/google/adk-python/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\") | [CONTRIBUTING.md](https://github.com/google/adk-python/blob/main/CONTRIBUTING.md \"CONTRIBUTING.md\") |  |   \n[LICENSE](https://github.com/google/adk-python/blob/main/LICENSE \"LICENSE\") | [LICENSE](https://github.com/google/adk-python/blob/main/LICENSE \"LICENSE\") |  |   \n[README.md](https://github.com/google/adk-python/blob/main/README.md \"README.md\") | [README.md](https://github.com/google/adk-python/blob/main/README.md \"README.md\") |  |   \n[autoformat.sh](https://github.com/google/adk-python/blob/main/autoformat.sh \"autoformat.sh\") | [autoformat.sh](https://github.com/google/adk-python/blob/main/autoformat.sh \"autoformat.sh\") |  |   \n[llms-full.txt](https://github.com/google/adk-python/blob/main/llms-full.txt \"llms-full.txt\") | [llms-full.txt](https://github.com/google/adk-python/blob/main/llms-full.txt \"llms-full.txt\") |  |   \n[llms.txt](https://github.com/google/adk-python/blob/main/llms.txt \"llms.txt\") | [llms.txt](https://github.com/google/adk-python/blob/main/llms.txt \"llms.txt\") |  |   \n[pylintrc](https://github.com/google/adk-python/blob/main/pylintrc \"pylintrc\") | [pylintrc](https://github.com/google/adk-python/blob/main/pylintrc \"pylintrc\") |  |   \n[pyproject.toml](https://github.com/google/adk-python/blob/main/pyproject.toml \"pyproject.toml\") | [pyproject.toml](https://github.com/google/adk-python/blob/main/pyproject.toml \"pyproject.toml\") |  |   \nView all files  \n## Repository files navigation\n  * [README](https://github.com/google/adk-python)\n  * [Code of conduct](https://github.com/google/adk-python)\n  * [Contributing](https://github.com/google/adk-python)\n  * [Apache-2.0 license](https://github.com/google/adk-python)\n  * [Security](https://github.com/google/adk-python)\n\n\n# Agent Development Kit (ADK)\n[](https://github.com/google/adk-python#agent-development-kit-adk)\n[![License](https://camo.githubusercontent.com/5b60841bea9e11d9d0b0950d690c9bc554e06385634056a7d5d62a15d1a4eabe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667)](https://github.com/google/adk-python/blob/main/LICENSE) [![PyPI](https://camo.githubusercontent.com/831aae963ba63f223d13c586fc6b3f2b620cec90a438f1a09c43a6be1e8d5f22/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f676f6f676c652d61646b)](https://pypi.org/project/google-adk/) [![Python Unit Tests](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg)](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml) [![r/agentdevelopmentkit](https://camo.githubusercontent.com/804c84103b652f4f9ffcceb4914840adca00dfeec97b4fea152a38aaccc16225/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5265646469742d722532466167656e74646576656c6f706d656e746b69742d4646343530303f7374796c653d666c6174266c6f676f3d726564646974266c6f676f436f6c6f723d7768697465)](https://www.reddit.com/r/agentdevelopmentkit/) [![Ask Code Wiki](https://camo.githubusercontent.com/3002e59856dd402678cfd2c64e5f8154749cb8bfe99df56ba8486253c269f3c2/68747470733a2f2f7777772e677374617469632e636f6d2f5f2f626f712d73646c632d6167656e74732d75692f5f2f722f4d766f7367346b6c4341342e737667)](https://codewiki.google/github.com/google/adk-python)\n##  [![](https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png)](https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png)\n[](https://github.com/google/adk-python#----------)\n###  An open-source, code-first Python framework for building, evaluating, and deploying sophisticated AI agents with flexibility and control. \n[](https://github.com/google/adk-python#------an-open-source-code-first-python-framework-for-building-evaluating-and-deploying-sophisticated-ai-agents-with-flexibility-and-control----)\n###  Important Links: [Docs](https://google.github.io/adk-docs/), [Samples](https://github.com/google/adk-samples), [Java ADK](https://github.com/google/adk-java), [Go ADK](https://github.com/google/adk-go) & [ADK Web](https://github.com/google/adk-web). \n[](https://github.com/google/adk-python#------important-links------docs------samples------java-adk------go-adk-------adk-web----)\nAgent Development Kit (ADK) is a flexible and modular framework that applies software development principles to AI agent creation. It is designed to simplify building, deploying, and orchestrating agent workflows, from simple tasks to complex systems. While optimized for Gemini, ADK is model-agnostic, deployment-agnostic, and compatible with other frameworks.\n* * *\n## 🔥 What's new\n[](https://github.com/google/adk-python#-whats-new)\n  * **Custom Service Registration** : Add a service registry to provide a generic way to register custom service implementations to be used in FastAPI server. See [short instruction](https://github.com/google/adk-python/discussions/3175#discussioncomment-14745120). ([391628f](https://github.com/google/adk-python/commit/391628fcdc7b950c6835f64ae3ccab197163c990))\n  * **Rewind** : Add the ability to rewind a session to before a previous invocation ([9dce06f](https://github.com/google/adk-python/commit/9dce06f9b00259ec42241df4f6638955e783a9d1)).\n  * **New CodeExecutor** : Introduces a new AgentEngineSandboxCodeExecutor class that supports executing agent-generated code using the Vertex AI Code Execution Sandbox API ([ee39a89](https://github.com/google/adk-python/commit/ee39a891106316b790621795b5cc529e89815a98))\n\n\n## ✨ Key Features\n[](https://github.com/google/adk-python#-key-features)\n  * **Rich Tool Ecosystem** : Utilize pre-built tools, custom functions, OpenAPI specs, MCP tools or integrate existing tools to give agents diverse capabilities, all for tight integration with the Google ecosystem.\n  * **Code-First Development** : Define agent logic, tools, and orchestration directly in Python for ultimate flexibility, testability, and versioning.\n  * **Agent Config** : Build agents without code. Check out the [Agent Config](https://google.github.io/adk-docs/agents/config/) feature.\n  * **Tool Confirmation** : A [tool confirmation flow(HITL)](https://google.github.io/adk-docs/tools/confirmation/) that can guard tool execution with explicit confirmation and custom input.\n  * **Modular Multi-Agent Systems** : Design scalable applications by composing multiple specialized agents into flexible hierarchies.\n  * **Deploy Anywhere** : Easily containerize and deploy agents on Cloud Run or scale seamlessly with Vertex AI Agent Engine.\n\n\n## 🚀 Installation\n[](https://github.com/google/adk-python#-installation)\n### Stable Release (Recommended)\n[](https://github.com/google/adk-python#stable-release-recommended)\nYou can install the latest stable version of ADK using `pip`:\n```\npip install google-adk\n```\n\nThe release cadence is roughly bi-weekly.\nThis version is recommended for most users as it represents the most recent official release.\n### Development Version\n[](https://github.com/google/adk-python#development-version)\nBug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven't been included in an official PyPI release yet, you can install directly from the main branch:\n```\npip install git+https://github.com/google/adk-python.git@main\n```\n\nNote: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.\n## 🤖 Agent2Agent (A2A) Protocol and ADK Integration\n[](https://github.com/google/adk-python#-agent2agent-a2a-protocol-and-adk-integration)\nFor remote agent-to-agent communication, ADK integrates with the [A2A protocol](https://github.com/google-a2a/A2A/). See this [example](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents) for how they can work together.\n## 📚 Documentation\n[](https://github.com/google/adk-python#-documentation)\nExplore the full documentation for detailed guides on building, evaluating, and deploying agents:\n  * **[Documentation](https://google.github.io/adk-docs)**\n\n\n## 🏁 Feature Highlight\n[](https://github.com/google/adk-python#-feature-highlight)\n### Define a single agent:\n[](https://github.com/google/adk-python#define-a-single-agent)\n```\nfrom google.adk.agents import Agent\nfrom google.adk.tools import google_search\n\nroot_agent = Agent(\n    name=\"search_assistant\",\n    model=\"gemini-2.5-flash\", # Or your preferred Gemini model\n    instruction=\"You are a helpful assistant. Answer user questions using Google Search when needed.\",\n    description=\"An assistant that can search the web.\",\n    tools=[google_search]\n)\n```\n\n### Define a multi-agent system:\n[](https://github.com/google/adk-python#define-a-multi-agent-system)\nDefine a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.\n```\nfrom google.adk.agents import LlmAgent, BaseAgent\n\n# Define individual agents\ngreeter = LlmAgent(name=\"greeter\", model=\"gemini-2.5-flash\", ...)\ntask_executor = LlmAgent(name=\"task_executor\", model=\"gemini-2.5-flash\", ...)\n\n# Create parent agent and assign children via sub_agents\ncoordinator = LlmAgent(\n    name=\"Coordinator\",\n    model=\"gemini-2.5-flash\",\n    description=\"I coordinate greetings and tasks.\",\n    sub_agents=[ # Assign sub_agents here\n        greeter,\n        task_executor\n    ]\n)\n```\n\n### Development UI\n[](https://github.com/google/adk-python#development-ui)\nA built-in development UI to help you test, evaluate, debug, and showcase your agent(s).\n[![](https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png)](https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png)\n### Evaluate Agents\n[](https://github.com/google/adk-python#evaluate-agents)\n```\nadk eval \\\n    samples_for_testing/hello_world \\\n    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json\n```\n\n## 🤝 Contributing\n[](https://github.com/google/adk-python#-contributing)\nWe welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our\n  * [General contribution guideline and flow](https://google.github.io/adk-docs/contributing-guide/).\n  * Then if you want to contribute code, please read [Code Contributing Guidelines](https://github.com/google/adk-python/blob/main/CONTRIBUTING.md) to get started.\n\n\n## Community Repo\n[](https://github.com/google/adk-python#community-repo)\nWe have [adk-python-community repo](https://github.com/google/adk-python-community)that is home to a growing ecosystem of community-contributed tools, third-party service integrations, and deployment scripts that extend the core capabilities of the ADK.\n## Vibe Coding\n[](https://github.com/google/adk-python#vibe-coding)\nIf you are to develop agent via vibe coding the [llms.txt](https://github.com/google/adk-python/blob/main/llms.txt) and the [llms-full.txt](https://github.com/google/adk-python/blob/main/llms-full.txt) can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.\n## Community Events\n[](https://github.com/google/adk-python#community-events)\n  * [Completed] ADK's 1st community meeting on Wednesday, October 15, 2025. Remember to [join our group](https://groups.google.com/g/adk-community) to get access to the [recording](https://drive.google.com/file/d/1rpXDq5NSH8-MyMeYI6_5pZ3Lhn0X9BQf/view), and [deck](https://docs.google.com/presentation/d/1_b8LG4xaiadbUUDzyNiapSFyxanc9ZgFdw7JQ6zmZ9Q/edit?slide=id.g384e60cdaca_0_658&resourcekey=0-tjFFv0VBQhpXBPCkZr0NOg#slide=id.g384e60cdaca_0_658).\n\n\n## 📄 License\n[](https://github.com/google/adk-python#-license)\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/google/adk-python/blob/main/LICENSE) file for details.\n* * *\n_Happy Agent Building!_\n## About\nAn open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. \n[google.github.io/adk-docs/](https://google.github.io/adk-docs/ \"https://google.github.io/adk-docs/\")\n### Topics\n[ agent ](https://github.com/topics/agent \"Topic: agent\") [ ai ](https://github.com/topics/ai \"Topic: ai\") [ multi-agent ](https://github.com/topics/multi-agent \"Topic: multi-agent\") [ agents ](https://github.com/topics/agents \"Topic: agents\") [ multi-agent-systems ](https://github.com/topics/multi-agent-systems \"Topic: multi-agent-systems\") [ ai-agents ](https://github.com/topics/ai-agents \"Topic: ai-agents\") [ multi-agents ](https://github.com/topics/multi-agents \"Topic: multi-agents\") [ llm ](https://github.com/topics/llm \"Topic: llm\") [ llms ](https://github.com/topics/llms \"Topic: llms\") [ genai ](https://github.com/topics/genai \"Topic: genai\") [ agentic ](https://github.com/topics/agentic \"Topic: agentic\") [ genai-chatbot ](https://github.com/topics/genai-chatbot \"Topic: genai-chatbot\") [ agentic-ai ](https://github.com/topics/agentic-ai \"Topic: agentic-ai\") [ multi-agents-collaboration ](https://github.com/topics/multi-agents-collaboration \"Topic: multi-agents-collaboration\") [ aiagentframework ](https://github.com/topics/aiagentframework \"Topic: aiagentframework\") [ agents-sdk ](https://github.com/topics/agents-sdk \"Topic: agents-sdk\")\n### Resources\n[ ](https://github.com/google/adk-python#readme-ov-file)\n### License\n[ ](https://github.com/google/adk-python#Apache-2.0-1-ov-file)\n### Code of conduct\n[ ](https://github.com/google/adk-python#coc-ov-file)\n### Contributing\n[ ](https://github.com/google/adk-python#contributing-ov-file)\n### Security policy\n[ ](https://github.com/google/adk-python#security-ov-file)\n###  Uh oh! \nThere was an error while loading. [Please reload this page](https://github.com/google/adk-python).\n[Activity](https://github.com/google/adk-python/activity)\n[Custom properties](https://github.com/google/adk-python/custom-properties)\n### Stars\n[**15.7k** stars](https://github.com/google/adk-python/stargazers)\n### Watchers\n[**137** watching](https://github.com/google/adk-python/watchers)\n### Forks\n[**2.5k** forks](https://github.com/google/adk-python/forks)\n[ Report repository ](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python&report=google+%28user%29)\n##  [Releases 30](https://github.com/google/adk-python/releases)\n[ 1.19.0 Latest  Nov 20, 2025 ](https://github.com/google/adk-python/releases/tag/v1.19.0)\n[+ 29 releases](https://github.com/google/adk-python/releases)\n##  [Packages 0](https://github.com/orgs/google/packages?repo_name=adk-python)\nNo packages published   \n\n##  [Used by 2.8k](https://github.com/google/adk-python/network/dependents)\n[\n  * ![@WRLDInc](https://avatars.githubusercontent.com/u/56140573?s=64&v=4)\n  * ![@Klavis-AI](https://avatars.githubusercontent.com/u/205720652?s=64&v=4)\n  * ![@restatedev](https://avatars.githubusercontent.com/u/97341811?s=64&v=4)\n  * ![@Devvekariya711](https://avatars.githubusercontent.com/u/207506349?s=64&v=4)\n  * ![@Jineshkumar1](https://avatars.githubusercontent.com/u/85137150?s=64&v=4)\n  * ![@nextlinktechnology](https://avatars.githubusercontent.com/u/60595633?s=64&v=4)\n  * ![@abdulrahman305](https://avatars.githubusercontent.com/u/131770526?s=64&v=4)\n  * ![@Langdock](https://avatars.githubusercontent.com/u/136317085?s=64&v=4)\n\n+ 2,819  ](https://github.com/google/adk-python/network/dependents)\n##  [Contributors 206](https://github.com/google/adk-python/graphs/contributors)\n  * [ ![@seanzhougoogle](https://avatars.githubusercontent.com/u/69162095?s=64&v=4) ](https://github.com/seanzhougoogle)\n  * [ ![@google-genai-bot](https://avatars.githubusercontent.com/u/194307901?s=64&v=4) ](https://github.com/google-genai-bot)\n  * [ ![@hangfei](https://avatars.githubusercontent.com/u/5482153?s=64&v=4) ](https://github.com/hangfei)\n  * [ ![@Jacksunwei](https://avatars.githubusercontent.com/u/1281348?s=64&v=4) ](https://github.com/Jacksunwei)\n  * [ ![@DeanChensj](https://avatars.githubusercontent.com/u/9414763?s=64&v=4) ](https://github.com/DeanChensj)\n  * [ ![@xuanyang15](https://avatars.githubusercontent.com/u/13991296?s=64&v=4) ](https://github.com/xuanyang15)\n  * [ ![@GWeale](https://avatars.githubusercontent.com/u/51099682?s=64&v=4) ](https://github.com/GWeale)\n  * [ ![@ankursharmas](https://avatars.githubusercontent.com/u/5462261?s=64&v=4) ](https://github.com/ankursharmas)\n  * [ ![@wuliang229](https://avatars.githubusercontent.com/u/18244712?s=64&v=4) ](https://github.com/wuliang229)\n  * [ ![@wyf7107](https://avatars.githubusercontent.com/u/14008276?s=64&v=4) ](https://github.com/wyf7107)\n  * [ ![@selcukgun](https://avatars.githubusercontent.com/u/5055758?s=64&v=4) ](https://github.com/selcukgun)\n  * [ ![@wukath](https://avatars.githubusercontent.com/u/108756731?s=64&v=4) ](https://github.com/wukath)\n  * [ ![@copybara-github](https://avatars.githubusercontent.com/u/32201506?s=64&v=4) ](https://github.com/copybara-github)\n  * [ ![@XinranTang](https://avatars.githubusercontent.com/u/33537857?s=64&v=4) ](https://github.com/XinranTang)\n\n\n[+ 192 contributors](https://github.com/google/adk-python/graphs/contributors)\n## Languages\n  * [ Python 94.8% ](https://github.com/google/adk-python/search?l=python)\n  * [ Jupyter Notebook 4.3% ](https://github.com/google/adk-python/search?l=jupyter-notebook)\n  * Other 0.9%\n\n\n## Footer\n[ ](https://github.com) © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Community](https://github.community/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\n"
                        }
                      }
                    },
                    {
                      "function_response": {
                        "id": "call_59130861",
                        "name": "adaptive_crawl_tool",
                        "response": {
                          "result": "# Adaptive Result from https://github.com/google/adk-python/issues\n\n[Skip to content](https://github.com/google/adk-python/issues#start-of-content)\n## Navigation Menu\nToggle navigation\n[ ](https://github.com/)\n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues)\nAppearance settings\n  * Platform\n    * AI CODE CREATION\n      * [ GitHub CopilotWrite better code with AI ](https://github.com/features/copilot)\n      * [ GitHub SparkBuild and deploy intelligent apps ](https://github.com/features/spark)\n      * [ GitHub ModelsManage and compare prompts ](https://github.com/features/models)\n      * [ MCP RegistryNewIntegrate external tools ](https://github.com/mcp)\n    * DEVELOPER WORKFLOWS\n      * [ ActionsAutomate any workflow ](https://github.com/features/actions)\n      * [ CodespacesInstant dev environments ](https://github.com/features/codespaces)\n      * [ IssuesPlan and track work ](https://github.com/features/issues)\n      * [ Code ReviewManage code changes ](https://github.com/features/code-review)\n    * APPLICATION SECURITY\n      * [ GitHub Advanced SecurityFind and fix vulnerabilities ](https://github.com/security/advanced-security)\n      * [ Code securitySecure your code as you build ](https://github.com/security/advanced-security/code-security)\n      * [ Secret protectionStop leaks before they start ](https://github.com/security/advanced-security/secret-protection)\n    * EXPLORE\n      * [Why GitHub](https://github.com/why-github)\n      * [Documentation](https://docs.github.com)\n      * [Blog](https://github.blog)\n      * [Changelog](https://github.blog/changelog)\n      * [Marketplace](https://github.com/marketplace)\n[View all features](https://github.com/features)\n  * Solutions\n    * BY COMPANY SIZE\n      * [Enterprises](https://github.com/enterprise)\n      * [Small and medium teams](https://github.com/team)\n      * [Startups](https://github.com/enterprise/startups)\n      * [Nonprofits](https://github.com/solutions/industry/nonprofits)\n    * BY USE CASE\n      * [App Modernization](https://github.com/solutions/use-case/app-modernization)\n      * [DevSecOps](https://github.com/solutions/use-case/devsecops)\n      * [DevOps](https://github.com/solutions/use-case/devops)\n      * [CI/CD](https://github.com/solutions/use-case/ci-cd)\n      * [View all use cases](https://github.com/solutions/use-case)\n    * BY INDUSTRY\n      * [Healthcare](https://github.com/solutions/industry/healthcare)\n      * [Financial services](https://github.com/solutions/industry/financial-services)\n      * [Manufacturing](https://github.com/solutions/industry/manufacturing)\n      * [Government](https://github.com/solutions/industry/government)\n      * [View all industries](https://github.com/solutions/industry)\n[View all solutions](https://github.com/solutions)\n  * Resources\n    * EXPLORE BY TOPIC\n      * [AI](https://github.com/resources/articles?topic=ai)\n      * [Software Development](https://github.com/resources/articles?topic=software-development)\n      * [DevOps](https://github.com/resources/articles?topic=devops)\n      * [Security](https://github.com/resources/articles?topic=security)\n      * [View all topics](https://github.com/resources/articles)\n    * EXPLORE BY TYPE\n      * [Customer stories](https://github.com/customer-stories)\n      * [Events & webinars](https://github.com/resources/events)\n      * [Ebooks & reports](https://github.com/resources/whitepapers)\n      * [Business insights](https://github.com/solutions/executive-insights)\n      * [GitHub Skills](https://skills.github.com)\n    * SUPPORT & SERVICES\n      * [Documentation](https://docs.github.com)\n      * [Customer support](https://support.github.com)\n      * [Community forum](https://github.com/orgs/community/discussions)\n      * [Trust center](https://github.com/trust-center)\n      * [Partners](https://github.com/partners)\n  * Open Source\n    * COMMUNITY\n      * [ GitHub SponsorsFund open source developers ](https://github.com/sponsors)\n    * PROGRAMS\n      * [Security Lab](https://securitylab.github.com)\n      * [Maintainer Community](https://maintainers.github.com)\n      * [Accelerator](https://github.com/accelerator)\n      * [Archive Program](https://archiveprogram.github.com)\n    * REPOSITORIES\n      * [Topics](https://github.com/topics)\n      * [Trending](https://github.com/trending)\n      * [Collections](https://github.com/collections)\n  * Enterprise\n    * ENTERPRISE SOLUTIONS\n      * [ Enterprise platformAI-powered developer platform ](https://github.com/enterprise)\n    * AVAILABLE ADD-ONS\n      * [ GitHub Advanced SecurityEnterprise-grade security features ](https://github.com/security/advanced-security)\n      * [ Copilot for BusinessEnterprise-grade AI features ](https://github.com/features/copilot/copilot-business)\n      * [ Premium SupportEnterprise-grade 24/7 support ](https://github.com/premium-support)\n  * [Pricing](https://github.com/pricing)\n\n\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\nSearch \nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n#  Provide feedback \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel  Submit feedback \n#  Saved searches \n## Use saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax). \nCancel  Create saved search \n[ Sign in ](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fadk-python%2Fissues)\n[ Sign up ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fissues%2Findex&source=header-repo&source_repo=google%2Fadk-python)\nAppearance settings\nResetting focus\nYou signed in with another tab or window. [Reload](https://github.com/google/adk-python/issues) to refresh your session. You signed out in another tab or window. [Reload](https://github.com/google/adk-python/issues) to refresh your session. You switched accounts on another tab or window. [Reload](https://github.com/google/adk-python/issues) to refresh your session. Dismiss alert\n{{ message }}\n[ google ](https://github.com/google) / **[adk-python](https://github.com/google/adk-python) ** Public\n  * [ ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python) You must be signed in to change notification settings\n  * [ 2.5k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n  * [ Star  15.7k ](https://github.com/login?return_to=%2Fgoogle%2Fadk-python)\n\n\n  * [ Code ](https://github.com/google/adk-python)\n  * [ Issues 354 ](https://github.com/google/adk-python/issues)\n  * [ Pull requests 102 ](https://github.com/google/adk-python/pulls)\n  * [ Discussions ](https://github.com/google/adk-python/discussions)\n  * [ Actions ](https://github.com/google/adk-python/actions)\n  * [ Projects 0 ](https://github.com/google/adk-python/projects)\n  * [ Security ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n[ ](https://github.com/google/adk-python/security)\n### [ Uh oh!  ](https://github.com/google/adk-python/security)\n[There was an error while loading. ](https://github.com/google/adk-python/security)[Please reload this page](https://github.com/google/adk-python/issues).\n  * [ Insights ](https://github.com/google/adk-python/pulse)\n\n\nAdditional navigation options\n  * [ Code  ](https://github.com/google/adk-python)\n  * [ Issues  ](https://github.com/google/adk-python/issues)\n  * [ Pull requests  ](https://github.com/google/adk-python/pulls)\n  * [ Discussions  ](https://github.com/google/adk-python/discussions)\n  * [ Actions  ](https://github.com/google/adk-python/actions)\n  * [ Projects  ](https://github.com/google/adk-python/projects)\n  * [ Security  ](https://github.com/google/adk-python/security)\n  * [ Insights  ](https://github.com/google/adk-python/pulse)\n\n\n# Issues\nSearch Issues\nis:issuestate:open\nis:issue state:open \nSearch\n[Labels](https://github.com/google/adk-python/labels)[Milestones](https://github.com/google/adk-python/milestones)[New issue](https://github.com/login?return_to=https://github.com/google/adk-python/issues)\n## Search results\n  * [Open 354 (354)](https://github.com/google/adk-python/issues)\n  * [Closed 1,430 (1,430)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aclosed)\n\n\nAuthor\nLabels\nProjects\nMilestones\nAssignees\nTypes\nSort by Newest, descending\n  * ### [Swagger/OpenAPI Docs Fail to Load with google-adk >= 1.19.0 (Internal Server Error on /openapi.json)](https://github.com/google/adk-python/issues/3750)\nStatus: Open.\n#3750 In google/adk-python;\n· [acloudpotato](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aacloudpotato) opened on Nov 28, 2025\n  * ### [Support stopping response after tool call (similar feature supported in Agno)](https://github.com/google/adk-python/issues/3748)\nStatus: Open.\n#3748 In google/adk-python;\n· [jamie0725](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Ajamie0725) opened on Nov 28, 2025\n  * ### [getting asyncio.exceptions.CancelledError when running pytest evalation for agent which uses mcptoolset](https://github.com/google/adk-python/issues/3746)\nStatus: Open.\n#3746 In google/adk-python;\n· [VandanaJn](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3AVandanaJn) opened on Nov 28, 2025\n  * ### [[Live] Transcription History is not sent in agent transfer with ADK 1.19.0](https://github.com/google/adk-python/issues/3742)\nStatus: Open.\n#3742 In google/adk-python;\n· [simone-viozzi](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Asimone-viozzi) opened on Nov 27, 2025\n  * ### [list_sessions of InMemorySessionService never includes events](https://github.com/google/adk-python/issues/3741)\nStatus: Open.\n#3741 In google/adk-python;\n· [connor-qua](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aconnor-qua) opened on Nov 27, 2025\n  * ### [Guidance on Mocking MCP Toolbox Request and Responses for Testing (Agent Evaluation)](https://github.com/google/adk-python/issues/3740)\n[answered[Status] This issue has been answered by the maintainer](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aanswered)[Status] This issue has been answered by the maintainer\n[eval[Component] This issue is related to evaluation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aeval)[Component] This issue is related to evaluation\n[question[Component] This issue is asking a question or clarification](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aquestion)[Component] This issue is asking a question or clarification\nStatus: Open.\n[Task](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ATask)\n#3740 In google/adk-python;\n· [sivakumar-d3v](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Asivakumar-d3v) opened on Nov 27, 2025\n3 comments\n[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20assignee%3Asurajksharma07)\n  * ### [Global Instructions not getting applied using the GlobalInstructionPlugin](https://github.com/google/adk-python/issues/3739)\nStatus: Open.\n#3739 In google/adk-python;\n· [iampushkar](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aiampushkar) opened on Nov 27, 2025\n  * ### [ValueError: \"Both invocation_id and new_message are None.\" during second turn when frontend (copilotkit) interacts with ag_ui_adk on Python 3.12](https://github.com/google/adk-python/issues/3735)\nStatus: Open.\n#3735 In google/adk-python;\n· [ywan614](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aywan614) opened on Nov 27, 2025\n  * ### [Is it possible to create a dynamic configurable agent?](https://github.com/google/adk-python/issues/3732)\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Acore)[Component] This issue is related to the core interface and implementation\n[needs-review[Status] The PR is awaiting review from the maintainer](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aneeds-review)[Status] The PR is awaiting review from the maintainer\nStatus: Open.\n[Feature](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3AFeature)\n#3732 In google/adk-python;\n· [Morriaty-The-Murderer](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3AMorriaty-The-Murderer) opened on Nov 27, 2025\n1 comment\n[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20assignee%3Asurajksharma07)\n  * ### [Critical Runtime Crash: RuntimeError: Attempted to exit cancel scope... when using MCPToolset on Vertex AI Agent Engine](https://github.com/google/adk-python/issues/3731)\n[mcp[Component] Issues about MCP support](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Amcp)[Component] Issues about MCP support\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3731 In google/adk-python;\n· [priyayarrabolu-boop](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Apriyayarrabolu-boop) opened on Nov 27, 2025\n  * ### [Ollama_chatException - {\"error\":\"json: cannot unmarshal array into Go struct field ChatRequest.messages.content of type string\"}](https://github.com/google/adk-python/issues/3727)\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Acore)[Component] This issue is related to the core interface and implementation\n[models[Component] Issues related to model support](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Amodels)[Component] Issues related to model support\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3727 In google/adk-python;\n· [Yashwant00CR7](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3AYashwant00CR7) opened on Nov 26, 2025\n  * ### [Ollama and Gemini Model Integration](https://github.com/google/adk-python/issues/3726)\n[models[Component] Issues related to model support](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Amodels)[Component] Issues related to model support\n[need attention[Status] This issue is important but the maintainers haven't started working on it](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22need%20attention%22)[Status] This issue is important but the maintainers haven't started working on it\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3726 In google/adk-python;\n· [Yashwant00CR7](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3AYashwant00CR7) opened on Nov 26, 2025\n2 comments\n[![wyf7107](https://avatars.githubusercontent.com/u/14008276?s=64&v=4)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20assignee%3Awyf7107)[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20assignee%3Asurajksharma07)\n  * ### [BigQuery Toolset OAuth Token Key Mismatch with Gemini Enterprise](https://github.com/google/adk-python/issues/3725)\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Acore)[Component] This issue is related to the core interface and implementation\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3725 In google/adk-python;\n· [svelezdevilla](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Asvelezdevilla) opened on Nov 26, 2025\n  * ### [Refactor BigQuery Analytics Plugin to use Structured JSON](https://github.com/google/adk-python/issues/3724)\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Acore)[Component] This issue is related to the core interface and implementation\nStatus: Open.\n#3724 In google/adk-python;\n· [amenegola](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aamenegola) opened on Nov 26, 2025\n1\n  * ### [Send message to the ADK API and display in the chat/session](https://github.com/google/adk-python/issues/3722)\n[services[Component] This issue is related to runtime services, e.g. sessions, memory, artifacts, etc](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aservices)[Component] This issue is related to runtime services, e.g. sessions, memory, artifacts, etc\nStatus: Open.\n#3722 In google/adk-python;\n· [zdenulo](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Azdenulo) opened on Nov 26, 2025\n5 comments\n  * ### [A2A Request Metadata Failure](https://github.com/google/adk-python/issues/3721)\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Acore)[Component] This issue is related to the core interface and implementation\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3721 In google/adk-python;\n· [xbill9](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Axbill9) opened on Nov 26, 2025\n  * ### [OpenAPIToolset does not provide a way to use a different CA (eg. Enterprise TLS proxy)](https://github.com/google/adk-python/issues/3720)\n[tools[Component] This issue is related to tools](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools)[Component] This issue is related to tools\nStatus: Open.\n[Feature](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3AFeature)\n#3720 In google/adk-python;\n· [asnowfix](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aasnowfix) opened on Nov 26, 2025\n  * ### [Error during async stream generation: 404 NOT_FOUND](https://github.com/google/adk-python/issues/3719)\n[agent engine[Component] This issue is related to Agent Engine deployment](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22agent%20engine%22)[Component] This issue is related to Agent Engine deployment\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3719 In google/adk-python;\n· [ahmed-othm](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aahmed-othm) opened on Nov 26, 2025\n2 comments\n  * ### [`adk web` does not render Eval tab when more than one agent folder exists](https://github.com/google/adk-python/issues/3718)\n[web[Component] This issue will be transferred to adk-web](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aweb)[Component] This issue will be transferred to adk-web\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3718 In google/adk-python;\n· [olliefr](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aolliefr) opened on Nov 26, 2025\n2 comments\n  * ### [Title: DatabaseSessionService.append_event raises stale session error with LongRunningFunctionTool ADK Version](https://github.com/google/adk-python/issues/3717)\n[core[Component] This issue is related to the core interface and implementation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Acore)[Component] This issue is related to the core interface and implementation\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3717 In google/adk-python;\n· [hungpq-ai](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Ahungpq-ai) opened on Nov 26, 2025\n  * ### [When using HITL inside a sequential / loop workflow, the subagent after the HITL agent never gets executed unless manually triggered from adk web](https://github.com/google/adk-python/issues/3716)\n[google support[Status] This issue is hand offed to google support team](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22google%20support%22)[Status] This issue is hand offed to google support team\n[tools[Component] This issue is related to tools](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools)[Component] This issue is related to tools\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3716 In google/adk-python;\n· [ashwinjo](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aashwinjo) opened on Nov 26, 2025\n1 comment\n[![surajksharma07](https://avatars.githubusercontent.com/u/228440606?s=64&u=fab6bf43670a6af8dc415d2d03c07555c77adb65&v=4)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20assignee%3Asurajksharma07)\n  * ### [Bug: Unable to create session for Agent Engine deployment when deploying with event compaction/context caching](https://github.com/google/adk-python/issues/3715)\n[agent engine[Component] This issue is related to Agent Engine deployment](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22agent%20engine%22)[Component] This issue is related to Agent Engine deployment\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3715 In google/adk-python;\n· [juszzz](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Ajuszzz) opened on Nov 26, 2025\n1\n5 comments\n  * ### [Bug: formatted response only works with some model providers](https://github.com/google/adk-python/issues/3713)\n[models[Component] Issues related to model support](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Amodels)[Component] Issues related to model support\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3713 In google/adk-python;\n· [david-velasquez-v](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Adavid-velasquez-v) opened on Nov 26, 2025\n2 comments\n[![GWeale](https://avatars.githubusercontent.com/u/51099682?s=64&u=a2b7d2e577160a7c2dd023b3dbf5a8fff08ee9a8&v=4)](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20assignee%3AGWeale)\n  * ### [tool_trajectory_avg_score class doesn't accept string values for `match_type`](https://github.com/google/adk-python/issues/3711)\n[eval[Component] This issue is related to evaluation](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aeval)[Component] This issue is related to evaluation\nStatus: Open.\n[Bug](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3ABug)\n#3711 In google/adk-python;\n· [alexhermida](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Aalexhermida) opened on Nov 25, 2025\n  * ### [feat: Add web API endpoints to retrieve artifact metadata](https://github.com/google/adk-python/issues/3710)\n[web[Component] This issue will be transferred to adk-web](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20label%3Aweb)[Component] This issue will be transferred to adk-web\nStatus: Open.\n[Feature](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20type%3AFeature)\n#3710 In google/adk-python;\n· [snbaker](https://github.com/google/adk-python/issues?q=is%3Aissue%20state%3Aopen%20author%3Asnbaker) opened on Nov 25, 2025\n1\n\n\n[1](https://github.com/google/adk-python/issues?page=1)[2](https://github.com/google/adk-python/issues?page=2)[3](https://github.com/google/adk-python/issues?page=3)[4](https://github.com/google/adk-python/issues?page=4)[5](https://github.com/google/adk-python/issues?page=5)[6](https://github.com/google/adk-python/issues?page=6)[7](https://github.com/google/adk-python/issues?page=7)[8](https://github.com/google/adk-python/issues?page=8)…[14](https://github.com/google/adk-python/issues?page=14)[15](https://github.com/google/adk-python/issues?page=15)[Next](https://github.com/google/adk-python/issues?page=2)\n## Footer\n[ ](https://github.com) © 2025 GitHub, Inc. \n### Footer navigation\n  * [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)\n  * [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)\n  * [Security](https://github.com/security)\n  * [Status](https://www.githubstatus.com/)\n  * [Community](https://github.community/)\n  * [Docs](https://docs.github.com/)\n  * [Contact](https://support.github.com?tags=dotcom-footer)\n  * Manage cookies \n  * Do not share my personal information \n\n\nIssues · google/adk-python · GitHub\n354 results\n"
                        }
                      }
                    }
                  ],
                  "role": "user"
                }
              },
              {
                "author": "Code_Surgeon_Agent",
                "content": {
                  "parts": [
                    {
                      "function_call": {
                        "id": "call_27564279",
                        "args": {
                          "key": "packages"
                        },
                        "name": "retrieve_context"
                      }
                    },
                    {
                      "function_call": {
                        "id": "call_81991433",
                        "args": {
                          "key": "versions"
                        },
                        "name": "retrieve_context"
                      }
                    }
                  ],
                  "role": "model"
                }
              },
              {
                "author": "Code_Surgeon_Agent",
                "content": {
                  "parts": [
                    {
                      "function_response": {
                        "id": "call_27564279",
                        "name": "retrieve_context",
                        "response": {
                          "result": "Not found"
                        }
                      }
                    },
                    {
                      "function_response": {
                        "id": "call_81991433",
                        "name": "retrieve_context",
                        "response": {
                          "result": "Not found"
                        }
                      }
                    }
                  ],
                  "role": "user"
                }
              },
              {
                "author": "Verification_Agent",
                "content": {
                  "parts": [
                    {
                      "function_call": {
                        "id": "call_30455985",
                        "args": {
                          "requirements_content": "google-adk==1.19.0\n# Optional: Explicit pins for common eval deps if local issues persist\n# numpy>=1.24.0  # Handles large int/float precision\n# pandas>=2.0.0  # Nullable Int64 for NaN\n# torch>=2.0.0   # .item() on tensors\nlitellm>=1.44.0  # For LiteLlm + Ollama workaround"
                        },
                        "name": "validate_requirements"
                      }
                    }
                  ],
                  "role": "model"
                }
              },
              {
                "author": "Verification_Agent",
                "content": {
                  "parts": [
                    {
                      "function_response": {
                        "id": "call_30455985",
                        "name": "validate_requirements",
                        "response": {
                          "result": "SUCCESS"
                        }
                      }
                    }
                  ],
                  "role": "user"
                }
              },
              {
                "author": "Verification_Agent",
                "content": {
                  "parts": [
                    {
                      "text": "Verification Passed"
                    },
                    {
                      "function_call": {
                        "id": "call_03517618",
                        "args": {
                          "key": "verification_status",
                          "value": "SUCCESS"
                        },
                        "name": "save_context"
                      }
                    }
                  ],
                  "role": "model"
                }
              },
              {
                "author": "Verification_Agent",
                "content": {
                  "parts": [
                    {
                      "function_response": {
                        "id": "call_03517618",
                        "name": "save_context",
                        "response": {
                          "result": "Saved verification_status to state."
                        }
                      }
                    }
                  ],
                  "role": "user"
                }
              }
            ]
          },
          "creation_timestamp": 1764336166.841116
        }
      ],
      "session_input": {
        "app_name": "src",
        "user_id": "user"
      },
      "creation_timestamp": 1764336446.3407524
    }
  ],
  "creation_timestamp": 1764252070.091962
}